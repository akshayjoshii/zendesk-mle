2025-04-26 11:52:13,902 - INFO - Data Utils -   label_column_name: atis_labels
2025-04-26 11:52:13,902 - INFO - Data Utils -   label_delimiter: +
2025-04-26 11:52:13,902 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:52:13,908 - INFO - Data Utils - Loaded initial Pandas df with 4634 rows and 2 columns.
2025-04-26 11:52:13,909 - INFO - Data Utils - Unpacking multi-labels in column 'atis_labels' using delimiter '+'.
2025-04-26 11:52:13,915 - INFO - Data Utils - Unpacking complete. Initial rows: 4634, Final rows: 4657.
2025-04-26 11:52:13,919 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:52:13,920 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:52:13,924 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:52:13,926 - INFO - Data Utils - Resetting index after cleanup. New shape: (4657, 2)
2025-04-26 11:52:13,927 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (4657, 2)
2025-04-26 11:52:13,930 - INFO - DataProcessor - Preparing labels for task type: multilabel
2025-04-26 11:52:13,930 - INFO - DataProcessor - Grouping unpacked labels by text to prepare for multi-hot encoding...
2025-04-26 11:52:14,013 - INFO - DataProcessor - Calculated 17 unique individual labels for multilabel task.
2025-04-26 11:52:14,034 - INFO - DataProcessor - Reconstructed DataFrame shape for multilabel: (4634, 2)
2025-04-26 11:52:14,040 - INFO - DataProcessor - Splitting data into train/validation (0.8/0.2)
2025-04-26 11:52:14,041 - INFO - DataProcessor - Train size: 3707, Validation size: 927
2025-04-26 11:52:14,041 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:52:14,086 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3707/3707 [00:00<00:00, 19688.45 examples/s]
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 927/927 [00:00<00:00, 25226.24 examples/s]
2025-04-26 11:52:14,393 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:52:14,426 - INFO - TrainMain - Data processing complete. Number of labels: 17
2025-04-26 11:52:14,427 - INFO - TrainMain - Label mapping (id2label): {0: 'abbreviation', 1: 'aircraft', 2: 'airfare', 3: 'airline', 4: 'airport', 5: 'capacity', 6: 'cheapest', 7: 'city', 8: 'distance', 9: 'flight', 10: 'flight_no', 11: 'flight_time', 12: 'ground_fare', 13: 'ground_service', 14: 'meal', 15: 'quantity', 16: 'restriction'}
2025-04-26 11:52:14,427 - INFO - TrainMain - Label mappings saved to ./results/atis_multilabel_xlmr_lora
2025-04-26 11:52:14,428 - INFO - TrainMain - Loading model for training...
2025-04-26 11:52:14,428 - INFO - ModelLoader - Loading base model: xlm-roberta-base
2025-04-26 11:52:14,428 - INFO - ModelLoader - Configuring model for 'multi_label_classification' (num_labels=17).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-04-26 11:52:17,036 - INFO - ModelLoader - Freezing base model parameters.
2025-04-26 11:52:17,038 - INFO - ModelLoader - Applying PEFT method: lora
2025-04-26 11:52:17,038 - INFO - ModelLoader - lora_target_modules not specified, attempting auto-detection by PEFT library.
2025-04-26 11:52:17,038 - INFO - ModelLoader - LoRA Config: r=64, alpha=16, dropout=0.1, target_modules=auto
2025-04-26 11:52:17,321 - INFO - ModelLoader - PEFT model created successfully.
trainable params: 2,962,961 || all params: 281,019,682 || trainable%: 1.0544
2025-04-26 11:52:17,323 - INFO - TrainMain - Model loading complete.
2025-04-26 11:52:17,323 - INFO - TrainMain - Setting up Trainer...
2025-04-26 11:52:17,323 - INFO - TrainerSetup - Configuring HuggingFace Trainer...
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-04-26 11:52:17,436 - INFO - TrainerSetup - Adding EarlyStoppingCallback with patience=3 based on 'eval_f1_micro'.
/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/train/trainer.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-04-26 11:52:18,046] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
You are adding a <class 'transformers.trainer_callback.ProgressCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
TensorBoardCallback
PrinterCallback
ProgressCallback
EarlyStoppingCallback
2025-04-26 11:52:21,142 - INFO - TrainerSetup - Trainer configured.
2025-04-26 11:52:21,142 - INFO - TrainMain - Trainer setup complete.
2025-04-26 11:52:21,142 - INFO - TrainMain - *** Starting Training ***
  1%|█▋                                                                                                                                                        | 25/2320 [00:03<04:23,  8.72it/s{'loss': 0.6573, 'grad_norm': 1.1604199409484863, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.10775862068965517}                                         | 25/2320 [00:03<04:23,  8.72it/s]
{'loss': 0.6573, 'grad_norm': 1.1604199409484863, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.11}                                                                                         
{'loss': 0.6573, 'grad_norm': 1.1604199409484863, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.11}                                                                                         
  2%|███▎                                                                                                                                                      | 50/2320 [00:06<04:13,  8.95it/s{'loss': 0.5814, 'grad_norm': 1.1866159439086914, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.21551724137931033}                                         | 50/2320 [00:06<04:13,  8.95it/s]
{'loss': 0.5814, 'grad_norm': 1.1866159439086914, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.22}                                                                                         
{'loss': 0.5814, 'grad_norm': 1.1866159439086914, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.22}                                                                                         
  3%|████▉                                                                                                                                                     | 75/2320 [00:08<04:08,  9.04it/s{'loss': 0.4207, 'grad_norm': 0.6516017317771912, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.3232758620689655}                                         | 75/2320 [00:08<04:08,  9.04it/s]
{'loss': 0.4207, 'grad_norm': 0.6516017317771912, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.32}                                                                                        
{'loss': 0.4207, 'grad_norm': 0.6516017317771912, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.32}                                                                                        
  4%|██████▌                                                                                                                                                  | 100/2320 [00:11<04:01,  9.20it/s{'loss': 0.1926, 'grad_norm': 0.37643492221832275, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43103448275862066}                                       | 100/2320 [00:11<04:01,  9.20it/s]
{'loss': 0.1926, 'grad_norm': 0.37643492221832275, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43}                                                                                        
{'loss': 0.1926, 'grad_norm': 0.37643492221832275, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43}                                                                                        
  5%|████████▏                                                                                                                                                | 125/2320 [00:14<04:05,  8.95it/s{'loss': 0.1292, 'grad_norm': 0.2678804397583008, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.5387931034482759}                                          | 125/2320 [00:14<04:05,  8.95it/s]
{'loss': 0.1292, 'grad_norm': 0.2678804397583008, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.54}                                                                                          
{'loss': 0.1292, 'grad_norm': 0.2678804397583008, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.54}                                                                                          
  6%|█████████▉                                                                                                                                               | 150/2320 [00:17<04:01,  9.00it/s{'loss': 0.1189, 'grad_norm': 0.2343885898590088, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.646551724137931}                                          | 150/2320 [00:17<04:01,  9.00it/s]
{'loss': 0.1189, 'grad_norm': 0.2343885898590088, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.65}                                                                                         
{'loss': 0.1189, 'grad_norm': 0.2343885898590088, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.65}                                                                                         
  8%|███████████▌                                                                                                                                             | 175/2320 [00:20<03:56,  9.05it/s{'loss': 0.1064, 'grad_norm': 0.2209407389163971, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.7543103448275862}                                         | 175/2320 [00:20<03:56,  9.05it/s]
{'loss': 0.1064, 'grad_norm': 0.2209407389163971, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.75}                                                                                         
{'loss': 0.1064, 'grad_norm': 0.2209407389163971, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.75}                                                                                         
  9%|█████████████▏                                                                                                                                           | 200/2320 [00:22<03:58,  8.88it/s{'loss': 0.1069, 'grad_norm': 0.1964622288942337, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.8620689655172413}                                         | 200/2320 [00:22<03:58,  8.88it/s]
{'loss': 0.1069, 'grad_norm': 0.1964622288942337, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.86}                                                                                         
{'loss': 0.1069, 'grad_norm': 0.1964622288942337, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.86}                                                                                         
 10%|██████████████▊                                                                                                                                          | 225/2320 [00:25<03:55,  8.90it/s{'loss': 0.103, 'grad_norm': 0.23840835690498352, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.9698275862068966}                                         | 225/2320 [00:25<03:55,  8.90it/s]
{'loss': 0.103, 'grad_norm': 0.23840835690498352, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.97}                                                                                         
{'loss': 0.103, 'grad_norm': 0.23840835690498352, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.97}                                                                                         
 10%|███████████████▎                                                                                                                                         | 232/2320 [00:26<03:52,  8.97it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.██████████████████████████████████████████████████████████████▏          | 27/29 [00:00<00:00, 38.19it/s]
  _warn_prf(average, modifier, msg_start, len(result))████████████████████████████████████████████████████████████████████████████████████████████████▏          | 27/29 [00:00<00:00, 38.19it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.10146497189998627, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.7879096570768638, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7321727870174966, 'eval_avg_precision': 0.7879096570768638, 'eval_runtime': 0.9051, 'eval_samples_per_second': 1024.194, 'eval_steps_per_second': 32.041, 'epoch': 1.0}
{'eval_loss': 0.10146497189998627, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.7879096570768638, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7321727870174966, 'eval_avg_precision': 0.7879096570768638, 'eval_runtime': 0.9051, 'eval_samples_per_second': 1024.194, 'eval_steps_per_second': 32.041, 'epoch': 1.0}
{'eval_loss': 0.10146497189998627, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.7879096570768638, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7321727870174966, 'eval_avg_precision': 0.7879096570768638, 'eval_runtime': 0.9051, 'eval_samples_per_second': 1024.194, 'eval_steps_per_second': 32.041, 'epoch': 1.0}
 11%|████████████████▍                                                                                                                                        | 250/2320 [00:29<03:55,  8.79it/s{'loss': 0.0922, 'grad_norm': 0.282998651266098, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.0775862068965518}                                          | 250/2320 [00:29<03:55,  8.79it/s]
{'loss': 0.0922, 'grad_norm': 0.282998651266098, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.08}                                                                                          
{'loss': 0.0922, 'grad_norm': 0.282998651266098, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.08}                                                                                          
 12%|██████████████████▏                                                                                                                                      | 275/2320 [00:32<03:53,  8.78it/s{'loss': 0.1042, 'grad_norm': 0.18015369772911072, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.1853448275862069}                                        | 275/2320 [00:32<03:53,  8.78it/s]
{'loss': 0.1042, 'grad_norm': 0.18015369772911072, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.19}                                                                                        
{'loss': 0.1042, 'grad_norm': 0.18015369772911072, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.19}                                                                                        
 13%|███████████████████▊                                                                                                                                     | 300/2320 [00:35<03:44,  9.00it/s{'loss': 0.1019, 'grad_norm': 0.16672830283641815, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.293103448275862}                                         | 300/2320 [00:35<03:44,  9.00it/s]
{'loss': 0.1019, 'grad_norm': 0.16672830283641815, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.29}                                                                                        
{'loss': 0.1019, 'grad_norm': 0.16672830283641815, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.29}                                                                                        
 14%|█████████████████████▍                                                                                                                                   | 325/2320 [00:38<03:32,  9.40it/s{'loss': 0.099, 'grad_norm': 0.1748085618019104, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4008620689655173}                                          | 325/2320 [00:38<03:32,  9.40it/s]
{'loss': 0.099, 'grad_norm': 0.1748085618019104, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4}                                                                                           
{'loss': 0.099, 'grad_norm': 0.1748085618019104, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4}                                                                                           
 15%|███████████████████████                                                                                                                                  | 350/2320 [00:41<03:40,  8.95it/s{'loss': 0.0907, 'grad_norm': 0.27595940232276917, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.5086206896551724}                                        | 350/2320 [00:41<03:39,  8.95it/s]
{'loss': 0.0907, 'grad_norm': 0.27595940232276917, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.51}                                                                                        
{'loss': 0.0907, 'grad_norm': 0.27595940232276917, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.51}                                                                                        
 16%|████████████████████████▋                                                                                                                                | 375/2320 [00:43<03:39,  8.86it/s{'loss': 0.0884, 'grad_norm': 0.15860186517238617, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.6163793103448276}                                        | 375/2320 [00:43<03:39,  8.86it/s]
{'loss': 0.0884, 'grad_norm': 0.15860186517238617, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.62}                                                                                        
{'loss': 0.0884, 'grad_norm': 0.15860186517238617, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.62}                                                                                        
 17%|██████████████████████████▍                                                                                                                              | 400/2320 [00:46<03:31,  9.06it/s{'loss': 0.0863, 'grad_norm': 0.34145107865333557, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.7241379310344827}                                        | 400/2320 [00:46<03:31,  9.06it/s]
{'loss': 0.0863, 'grad_norm': 0.34145107865333557, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.72}                                                                                        
{'loss': 0.0863, 'grad_norm': 0.34145107865333557, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.72}                                                                                        
 18%|████████████████████████████                                                                                                                             | 425/2320 [00:49<03:35,  8.79it/s{'loss': 0.0808, 'grad_norm': 0.18001538515090942, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.831896551724138}                                         | 425/2320 [00:49<03:35,  8.79it/s]
{'loss': 0.0808, 'grad_norm': 0.18001538515090942, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.83}                                                                                        
{'loss': 0.0808, 'grad_norm': 0.18001538515090942, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.83}                                                                                        
 19%|█████████████████████████████▋                                                                                                                           | 450/2320 [00:52<03:29,  8.94it/s{'loss': 0.081, 'grad_norm': 0.1794935017824173, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.9396551724137931}                                          | 450/2320 [00:52<03:29,  8.94it/s]
{'loss': 0.081, 'grad_norm': 0.1794935017824173, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.94}                                                                                          
{'loss': 0.081, 'grad_norm': 0.1794935017824173, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.94}                                                                                          
 20%|██████████████████████████████▌                                                                                                                          | 464/2320 [00:53<03:24,  9.06it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.67it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.67it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.07953780144453049, 'eval_precision_micro': 0.8798342541436464, 'eval_recall_micro': 0.6827438370846731, 'eval_f1_micro': 0.7688593844296923, 'eval_precision_macro': 0.051754956126096846, 'eval_recall_macro': 0.05617779345621307, 'eval_f1_macro': 0.05387575590984057, 'eval_precision_weighted': 0.6289919051594985, 'eval_recall_weighted': 0.6827438370846731, 'eval_f1_weighted': 0.6547665554787591, 'eval_precision_samples': 0.6871628910463862, 'eval_recall_samples': 0.6853649766271125, 'eval_f1_samples': 0.6859043509528946, 'eval_f1': 0.7688593844296923, 'eval_subset_accuracy': 0.6839266450916937, 'eval_hamming_loss': 0.024303572561710767, 'eval_jaccard_micro': 0.6245098039215686, 'eval_jaccard_macro': 0.04969574036511156, 'eval_jaccard_weighted': 0.6039657020364416, 'eval_jaccard_samples': 0.6853649766271125, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8342706060423467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7715003032683364, 'eval_avg_precision': 0.8342706060423467, 'eval_runtime': 0.8904, 'eval_samples_per_second': 1041.154, 'eval_steps_per_second': 32.571, 'epoch': 2.0}
{'eval_loss': 0.07953780144453049, 'eval_precision_micro': 0.8798342541436464, 'eval_recall_micro': 0.6827438370846731, 'eval_f1_micro': 0.7688593844296923, 'eval_precision_macro': 0.051754956126096846, 'eval_recall_macro': 0.05617779345621307, 'eval_f1_macro': 0.05387575590984057, 'eval_precision_weighted': 0.6289919051594985, 'eval_recall_weighted': 0.6827438370846731, 'eval_f1_weighted': 0.6547665554787591, 'eval_precision_samples': 0.6871628910463862, 'eval_recall_samples': 0.6853649766271125, 'eval_f1_samples': 0.6859043509528946, 'eval_f1': 0.7688593844296923, 'eval_subset_accuracy': 0.6839266450916937, 'eval_hamming_loss': 0.024303572561710767, 'eval_jaccard_micro': 0.6245098039215686, 'eval_jaccard_macro': 0.04969574036511156, 'eval_jaccard_weighted': 0.6039657020364416, 'eval_jaccard_samples': 0.6853649766271125, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8342706060423467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7715003032683364, 'eval_avg_precision': 0.8342706060423467, 'eval_runtime': 0.8904, 'eval_samples_per_second': 1041.154, 'eval_steps_per_second': 32.571, 'epoch': 2.0}
{'eval_loss': 0.07953780144453049, 'eval_precision_micro': 0.8798342541436464, 'eval_recall_micro': 0.6827438370846731, 'eval_f1_micro': 0.7688593844296923, 'eval_precision_macro': 0.051754956126096846, 'eval_recall_macro': 0.05617779345621307, 'eval_f1_macro': 0.05387575590984057, 'eval_precision_weighted': 0.6289919051594985, 'eval_recall_weighted': 0.6827438370846731, 'eval_f1_weighted': 0.6547665554787591, 'eval_precision_samples': 0.6871628910463862, 'eval_recall_samples': 0.6853649766271125, 'eval_f1_samples': 0.6859043509528946, 'eval_f1': 0.7688593844296923, 'eval_subset_accuracy': 0.6839266450916937, 'eval_hamming_loss': 0.024303572561710767, 'eval_jaccard_micro': 0.6245098039215686, 'eval_jaccard_macro': 0.04969574036511156, 'eval_jaccard_weighted': 0.6039657020364416, 'eval_jaccard_samples': 0.6853649766271125, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8342706060423467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7715003032683364, 'eval_avg_precision': 0.8342706060423467, 'eval_runtime': 0.8904, 'eval_samples_per_second': 1041.154, 'eval_steps_per_second': 32.571, 'epoch': 2.0}
 20%|███████████████████████████████▎                                                                                                                         | 475/2320 [00:56<03:43,  8.25it/s{'loss': 0.0862, 'grad_norm': 0.18798132240772247, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.0474137931034484}                                        | 475/2320 [00:56<03:43,  8.25it/s]
{'loss': 0.0862, 'grad_norm': 0.18798132240772247, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.05}                                                                                        
{'loss': 0.0862, 'grad_norm': 0.18798132240772247, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.05}                                                                                        
 22%|████████████████████████████████▉                                                                                                                        | 500/2320 [00:59<03:25,  8.86it/s{'loss': 0.0841, 'grad_norm': 0.30765944719314575, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.1551724137931036}                                         | 500/2320 [00:59<03:25,  8.87it/s]
{'loss': 0.0841, 'grad_norm': 0.30765944719314575, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.16}                                                                                         
{'loss': 0.0841, 'grad_norm': 0.30765944719314575, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.16}                                                                                         
 23%|██████████████████████████████████▌                                                                                                                      | 525/2320 [01:01<03:18,  9.05it/s{'loss': 0.0693, 'grad_norm': 0.18706612288951874, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.2629310344827585}                                         | 525/2320 [01:01<03:18,  9.05it/s]
{'loss': 0.0693, 'grad_norm': 0.18706612288951874, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.26}                                                                                         
{'loss': 0.0693, 'grad_norm': 0.18706612288951874, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.26}                                                                                         
 24%|████████████████████████████████████▎                                                                                                                    | 550/2320 [01:04<03:15,  9.06it/s{'loss': 0.0697, 'grad_norm': 0.16042888164520264, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.3706896551724137}                                        | 550/2320 [01:04<03:15,  9.05it/s]
{'loss': 0.0697, 'grad_norm': 0.16042888164520264, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.37}                                                                                        
{'loss': 0.0697, 'grad_norm': 0.16042888164520264, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.37}                                                                                        
 25%|█████████████████████████████████████▉                                                                                                                   | 575/2320 [01:07<03:15,  8.92it/s{'loss': 0.076, 'grad_norm': 0.2368347942829132, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.478448275862069}                                           | 575/2320 [01:07<03:15,  8.92it/s]
{'loss': 0.076, 'grad_norm': 0.2368347942829132, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.48}                                                                                          
{'loss': 0.076, 'grad_norm': 0.2368347942829132, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.48}                                                                                          
 26%|███████████████████████████████████████▌                                                                                                                 | 600/2320 [01:10<03:09,  9.07it/s{'loss': 0.0692, 'grad_norm': 0.22593611478805542, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.586206896551724}                                         | 600/2320 [01:10<03:09,  9.07it/s]
{'loss': 0.0692, 'grad_norm': 0.22593611478805542, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.59}                                                                                        
{'loss': 0.0692, 'grad_norm': 0.22593611478805542, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.59}                                                                                        
 27%|█████████████████████████████████████████▏                                                                                                               | 625/2320 [01:12<03:06,  9.09it/s{'loss': 0.0624, 'grad_norm': 0.12947288155555725, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.6939655172413794}                                        | 625/2320 [01:12<03:06,  9.09it/s]
{'loss': 0.0624, 'grad_norm': 0.12947288155555725, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.69}                                                                                        
{'loss': 0.0624, 'grad_norm': 0.12947288155555725, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.69}                                                                                        
 28%|██████████████████████████████████████████▊                                                                                                              | 650/2320 [01:15<03:05,  9.00it/s{'loss': 0.0684, 'grad_norm': 0.13072402775287628, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8017241379310347}                                        | 650/2320 [01:15<03:05,  9.00it/s]
{'loss': 0.0684, 'grad_norm': 0.13072402775287628, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8}                                                                                         
{'loss': 0.0684, 'grad_norm': 0.13072402775287628, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8}                                                                                         
 29%|████████████████████████████████████████████▌                                                                                                            | 675/2320 [01:18<02:59,  9.14it/s{'loss': 0.0603, 'grad_norm': 0.17011740803718567, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.9094827586206895}                                         | 675/2320 [01:18<02:59,  9.14it/s]
{'loss': 0.0603, 'grad_norm': 0.17011740803718567, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.91}                                                                                         
{'loss': 0.0603, 'grad_norm': 0.17011740803718567, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.91}                                                                                         
 30%|█████████████████████████████████████████████▉                                                                                                           | 696/2320 [01:20<02:58,  9.10it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.68it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.68it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.057460710406303406, 'eval_precision_micro': 0.9320987654320988, 'eval_recall_micro': 0.8092175777063236, 'eval_f1_micro': 0.8663224325874928, 'eval_precision_macro': 0.17097132970835394, 'eval_recall_macro': 0.13815399017082305, 'eval_f1_macro': 0.15084373428425818, 'eval_precision_weighted': 0.8134293910490241, 'eval_recall_weighted': 0.8092175777063236, 'eval_f1_weighted': 0.8051711166936609, 'eval_precision_samples': 0.81445523193096, 'eval_recall_samples': 0.8126573175116864, 'eval_f1_samples': 0.8131966918374685, 'eval_f1': 0.8663224325874928, 'eval_subset_accuracy': 0.8112189859762675, 'eval_hamming_loss': 0.01478520210673266, 'eval_jaccard_micro': 0.7641700404858299, 'eval_jaccard_macro': 0.1334081383774071, 'eval_jaccard_weighted': 0.7555957433648104, 'eval_jaccard_samples': 0.8126573175116864, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.903855735025579, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8855917692992464, 'eval_avg_precision': 0.903855735025579, 'eval_runtime': 0.8878, 'eval_samples_per_second': 1044.098, 'eval_steps_per_second': 32.663, 'epoch': 3.0}
{'eval_loss': 0.057460710406303406, 'eval_precision_micro': 0.9320987654320988, 'eval_recall_micro': 0.8092175777063236, 'eval_f1_micro': 0.8663224325874928, 'eval_precision_macro': 0.17097132970835394, 'eval_recall_macro': 0.13815399017082305, 'eval_f1_macro': 0.15084373428425818, 'eval_precision_weighted': 0.8134293910490241, 'eval_recall_weighted': 0.8092175777063236, 'eval_f1_weighted': 0.8051711166936609, 'eval_precision_samples': 0.81445523193096, 'eval_recall_samples': 0.8126573175116864, 'eval_f1_samples': 0.8131966918374685, 'eval_f1': 0.8663224325874928, 'eval_subset_accuracy': 0.8112189859762675, 'eval_hamming_loss': 0.01478520210673266, 'eval_jaccard_micro': 0.7641700404858299, 'eval_jaccard_macro': 0.1334081383774071, 'eval_jaccard_weighted': 0.7555957433648104, 'eval_jaccard_samples': 0.8126573175116864, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.903855735025579, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8855917692992464, 'eval_avg_precision': 0.903855735025579, 'eval_runtime': 0.8878, 'eval_samples_per_second': 1044.098, 'eval_steps_per_second': 32.663, 'epoch': 3.0}
{'eval_loss': 0.057460710406303406, 'eval_precision_micro': 0.9320987654320988, 'eval_recall_micro': 0.8092175777063236, 'eval_f1_micro': 0.8663224325874928, 'eval_precision_macro': 0.17097132970835394, 'eval_recall_macro': 0.13815399017082305, 'eval_f1_macro': 0.15084373428425818, 'eval_precision_weighted': 0.8134293910490241, 'eval_recall_weighted': 0.8092175777063236, 'eval_f1_weighted': 0.8051711166936609, 'eval_precision_samples': 0.81445523193096, 'eval_recall_samples': 0.8126573175116864, 'eval_f1_samples': 0.8131966918374685, 'eval_f1': 0.8663224325874928, 'eval_subset_accuracy': 0.8112189859762675, 'eval_hamming_loss': 0.01478520210673266, 'eval_jaccard_micro': 0.7641700404858299, 'eval_jaccard_macro': 0.1334081383774071, 'eval_jaccard_weighted': 0.7555957433648104, 'eval_jaccard_samples': 0.8126573175116864, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.903855735025579, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8855917692992464, 'eval_avg_precision': 0.903855735025579, 'eval_runtime': 0.8878, 'eval_samples_per_second': 1044.098, 'eval_steps_per_second': 32.663, 'epoch': 3.0}
 30%|██████████████████████████████████████████████▏                                                                                                          | 700/2320 [01:22<06:40,  4.04it/s{'loss': 0.051, 'grad_norm': 0.2787420153617859, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.0172413793103448}                                          | 700/2320 [01:22<06:40,  4.04it/s]
{'loss': 0.051, 'grad_norm': 0.2787420153617859, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.02}                                                                                          
{'loss': 0.051, 'grad_norm': 0.2787420153617859, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.02}                                                                                          
 31%|███████████████████████████████████████████████▊                                                                                                         | 725/2320 [01:25<02:57,  9.00it/s{'loss': 0.0606, 'grad_norm': 0.1292707920074463, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.125}                                                      | 725/2320 [01:25<02:57,  9.00it/s]
{'loss': 0.0606, 'grad_norm': 0.1292707920074463, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.12}                                                                                         
{'loss': 0.0606, 'grad_norm': 0.1292707920074463, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.12}                                                                                         
 32%|█████████████████████████████████████████████████▍                                                                                                       | 750/2320 [01:28<02:54,  8.97it/s{'loss': 0.0525, 'grad_norm': 0.22885940968990326, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.2327586206896552}                                        | 750/2320 [01:28<02:54,  8.97it/s]
{'loss': 0.0525, 'grad_norm': 0.22885940968990326, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.23}                                                                                        
{'loss': 0.0525, 'grad_norm': 0.22885940968990326, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.23}                                                                                        
 33%|███████████████████████████████████████████████████                                                                                                      | 775/2320 [01:30<02:52,  8.97it/s{'loss': 0.042, 'grad_norm': 0.1556222289800644, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.3405172413793105}                                          | 775/2320 [01:30<02:52,  8.97it/s]
{'loss': 0.042, 'grad_norm': 0.1556222289800644, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.34}                                                                                          
{'loss': 0.042, 'grad_norm': 0.1556222289800644, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.34}                                                                                          
 34%|████████████████████████████████████████████████████▊                                                                                                    | 800/2320 [01:33<02:49,  8.97it/s{'loss': 0.0453, 'grad_norm': 0.15756957232952118, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.4482758620689653}                                        | 800/2320 [01:33<02:49,  8.97it/s]
{'loss': 0.0453, 'grad_norm': 0.15756957232952118, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.45}                                                                                        
{'loss': 0.0453, 'grad_norm': 0.15756957232952118, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.45}                                                                                        
 36%|██████████████████████████████████████████████████████▍                                                                                                  | 825/2320 [01:36<02:47,  8.93it/s{'loss': 0.0415, 'grad_norm': 0.10483244806528091, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.5560344827586206}                                        | 825/2320 [01:36<02:47,  8.93it/s]
{'loss': 0.0415, 'grad_norm': 0.10483244806528091, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.56}                                                                                        
{'loss': 0.0415, 'grad_norm': 0.10483244806528091, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.56}                                                                                        
 37%|████████████████████████████████████████████████████████                                                                                                 | 850/2320 [01:39<02:42,  9.04it/s{'loss': 0.0444, 'grad_norm': 0.17385517060756683, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.663793103448276}                                          | 850/2320 [01:39<02:42,  9.04it/s]
{'loss': 0.0444, 'grad_norm': 0.17385517060756683, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.66}                                                                                         
{'loss': 0.0444, 'grad_norm': 0.17385517060756683, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.66}                                                                                         
 38%|█████████████████████████████████████████████████████████▋                                                                                               | 875/2320 [01:42<02:38,  9.09it/s{'loss': 0.0526, 'grad_norm': 0.21605098247528076, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.771551724137931}                                          | 875/2320 [01:42<02:38,  9.09it/s]
{'loss': 0.0526, 'grad_norm': 0.21605098247528076, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.77}                                                                                         
{'loss': 0.0526, 'grad_norm': 0.21605098247528076, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.77}                                                                                         
 39%|███████████████████████████████████████████████████████████▎                                                                                             | 900/2320 [01:44<02:37,  9.00it/s{'loss': 0.0361, 'grad_norm': 0.1701488345861435, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.8793103448275863}                                          | 900/2320 [01:44<02:37,  9.00it/s]
{'loss': 0.0361, 'grad_norm': 0.1701488345861435, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.88}                                                                                          
{'loss': 0.0361, 'grad_norm': 0.1701488345861435, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.88}                                                                                          
 40%|█████████████████████████████████████████████████████████████                                                                                            | 925/2320 [01:47<02:34,  9.02it/s{'loss': 0.044, 'grad_norm': 0.19092929363250732, 'learning_rate': 6.403669724770642e-05, 'epoch': 3.987068965517241}                                          | 925/2320 [01:47<02:34,  9.02it/s]
{'loss': 0.044, 'grad_norm': 0.19092929363250732, 'learning_rate': 6.403669724770642e-05, 'epoch': 3.99}                                                                                         
{'loss': 0.044, 'grad_norm': 0.19092929363250732, 'learning_rate': 6.403669724770642e-05, 'epoch': 3.99}                                                                                         
 40%|█████████████████████████████████████████████████████████████▏                                                                                           | 928/2320 [01:47<02:35,  8.95it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.84it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.84it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.03535613417625427, 'eval_precision_micro': 0.9544392523364486, 'eval_recall_micro': 0.8756698821007503, 'eval_f1_micro': 0.9133594186696479, 'eval_precision_macro': 0.3408521396425102, 'eval_recall_macro': 0.22482212870220866, 'eval_f1_macro': 0.253327716665276, 'eval_precision_weighted': 0.907479212206006, 'eval_recall_weighted': 0.8756698821007503, 'eval_f1_weighted': 0.8797119359160617, 'eval_precision_samples': 0.8797195253505933, 'eval_recall_samples': 0.8795397339086659, 'eval_f1_samples': 0.8790003595828839, 'eval_f1': 0.9133594186696479, 'eval_subset_accuracy': 0.8748651564185544, 'eval_hamming_loss': 0.009835649470144045, 'eval_jaccard_micro': 0.8405349794238683, 'eval_jaccard_macro': 0.21708404708335893, 'eval_jaccard_weighted': 0.8375930356372466, 'eval_jaccard_samples': 0.8779216109313196, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9532069403949488, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9399424222685188, 'eval_avg_precision': 0.9532069403949488, 'eval_runtime': 0.886, 'eval_samples_per_second': 1046.227, 'eval_steps_per_second': 32.73, 'epoch': 4.0}
{'eval_loss': 0.03535613417625427, 'eval_precision_micro': 0.9544392523364486, 'eval_recall_micro': 0.8756698821007503, 'eval_f1_micro': 0.9133594186696479, 'eval_precision_macro': 0.3408521396425102, 'eval_recall_macro': 0.22482212870220866, 'eval_f1_macro': 0.253327716665276, 'eval_precision_weighted': 0.907479212206006, 'eval_recall_weighted': 0.8756698821007503, 'eval_f1_weighted': 0.8797119359160617, 'eval_precision_samples': 0.8797195253505933, 'eval_recall_samples': 0.8795397339086659, 'eval_f1_samples': 0.8790003595828839, 'eval_f1': 0.9133594186696479, 'eval_subset_accuracy': 0.8748651564185544, 'eval_hamming_loss': 0.009835649470144045, 'eval_jaccard_micro': 0.8405349794238683, 'eval_jaccard_macro': 0.21708404708335893, 'eval_jaccard_weighted': 0.8375930356372466, 'eval_jaccard_samples': 0.8779216109313196, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9532069403949488, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9399424222685188, 'eval_avg_precision': 0.9532069403949488, 'eval_runtime': 0.886, 'eval_samples_per_second': 1046.227, 'eval_steps_per_second': 32.73, 'epoch': 4.0}
{'eval_loss': 0.03535613417625427, 'eval_precision_micro': 0.9544392523364486, 'eval_recall_micro': 0.8756698821007503, 'eval_f1_micro': 0.9133594186696479, 'eval_precision_macro': 0.3408521396425102, 'eval_recall_macro': 0.22482212870220866, 'eval_f1_macro': 0.253327716665276, 'eval_precision_weighted': 0.907479212206006, 'eval_recall_weighted': 0.8756698821007503, 'eval_f1_weighted': 0.8797119359160617, 'eval_precision_samples': 0.8797195253505933, 'eval_recall_samples': 0.8795397339086659, 'eval_f1_samples': 0.8790003595828839, 'eval_f1': 0.9133594186696479, 'eval_subset_accuracy': 0.8748651564185544, 'eval_hamming_loss': 0.009835649470144045, 'eval_jaccard_micro': 0.8405349794238683, 'eval_jaccard_macro': 0.21708404708335893, 'eval_jaccard_weighted': 0.8375930356372466, 'eval_jaccard_samples': 0.8779216109313196, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9532069403949488, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9399424222685188, 'eval_avg_precision': 0.9532069403949488, 'eval_runtime': 0.886, 'eval_samples_per_second': 1046.227, 'eval_steps_per_second': 32.73, 'epoch': 4.0}
 41%|██████████████████████████████████████████████████████████████▋                                                                                          | 950/2320 [01:51<02:32,  9.00it/s{'loss': 0.0376, 'grad_norm': 0.1841895878314972, 'learning_rate': 6.288990825688073e-05, 'epoch': 4.094827586206897}                                          | 950/2320 [01:51<02:32,  9.00it/s]
{'loss': 0.0376, 'grad_norm': 0.1841895878314972, 'learning_rate': 6.288990825688073e-05, 'epoch': 4.09}                                                                                         
{'loss': 0.0376, 'grad_norm': 0.1841895878314972, 'learning_rate': 6.288990825688073e-05, 'epoch': 4.09}                                                                                         
 42%|████████████████████████████████████████████████████████████████▎                                                                                        | 975/2320 [01:54<02:26,  9.16it/s{'loss': 0.0376, 'grad_norm': 0.07538238167762756, 'learning_rate': 6.174311926605505e-05, 'epoch': 4.202586206896552}                                         | 975/2320 [01:54<02:26,  9.16it/s]
{'loss': 0.0376, 'grad_norm': 0.07538238167762756, 'learning_rate': 6.174311926605505e-05, 'epoch': 4.2}                                                                                         
{'loss': 0.0376, 'grad_norm': 0.07538238167762756, 'learning_rate': 6.174311926605505e-05, 'epoch': 4.2}                                                                                         
 43%|█████████████████████████████████████████████████████████████████▌                                                                                      | 1000/2320 [01:57<02:25,  9.05it/s{'loss': 0.0328, 'grad_norm': 0.0353945717215538, 'learning_rate': 6.059633027522936e-05, 'epoch': 4.310344827586207}                                         | 1000/2320 [01:57<02:25,  9.05it/s]
{'loss': 0.0328, 'grad_norm': 0.0353945717215538, 'learning_rate': 6.059633027522936e-05, 'epoch': 4.31}                                                                                         
{'loss': 0.0328, 'grad_norm': 0.0353945717215538, 'learning_rate': 6.059633027522936e-05, 'epoch': 4.31}                                                                                         
 44%|███████████████████████████████████████████████████████████████████▏                                                                                    | 1025/2320 [01:59<02:22,  9.06it/s{'loss': 0.034, 'grad_norm': 0.22192154824733734, 'learning_rate': 5.944954128440368e-05, 'epoch': 4.418103448275862}                                         | 1025/2320 [01:59<02:22,  9.06it/s]
{'loss': 0.034, 'grad_norm': 0.22192154824733734, 'learning_rate': 5.944954128440368e-05, 'epoch': 4.42}                                                                                         
{'loss': 0.034, 'grad_norm': 0.22192154824733734, 'learning_rate': 5.944954128440368e-05, 'epoch': 4.42}                                                                                         
 45%|████████████████████████████████████████████████████████████████████▊                                                                                   | 1050/2320 [02:02<02:20,  9.01it/s{'loss': 0.0397, 'grad_norm': 0.14297692477703094, 'learning_rate': 5.830275229357799e-05, 'epoch': 4.525862068965517}                                        | 1050/2320 [02:02<02:20,  9.01it/s]
{'loss': 0.0397, 'grad_norm': 0.14297692477703094, 'learning_rate': 5.830275229357799e-05, 'epoch': 4.53}                                                                                        
{'loss': 0.0397, 'grad_norm': 0.14297692477703094, 'learning_rate': 5.830275229357799e-05, 'epoch': 4.53}                                                                                        
 46%|██████████████████████████████████████████████████████████████████████▍                                                                                 | 1075/2320 [02:05<02:17,  9.05it/s{'loss': 0.03, 'grad_norm': 0.11986649036407471, 'learning_rate': 5.7155963302752295e-05, 'epoch': 4.633620689655173}                                         | 1075/2320 [02:05<02:17,  9.05it/s]
{'loss': 0.03, 'grad_norm': 0.11986649036407471, 'learning_rate': 5.7155963302752295e-05, 'epoch': 4.63}                                                                                         
{'loss': 0.03, 'grad_norm': 0.11986649036407471, 'learning_rate': 5.7155963302752295e-05, 'epoch': 4.63}                                                                                         
 47%|████████████████████████████████████████████████████████████████████████                                                                                | 1100/2320 [02:08<02:15,  8.99it/s{'loss': 0.0293, 'grad_norm': 0.1852777749300003, 'learning_rate': 5.6009174311926606e-05, 'epoch': 4.741379310344827}                                        | 1100/2320 [02:08<02:15,  8.99it/s]
{'loss': 0.0293, 'grad_norm': 0.1852777749300003, 'learning_rate': 5.6009174311926606e-05, 'epoch': 4.74}                                                                                        
{'loss': 0.0293, 'grad_norm': 0.1852777749300003, 'learning_rate': 5.6009174311926606e-05, 'epoch': 4.74}                                                                                        
 48%|█████████████████████████████████████████████████████████████████████████▋                                                                              | 1125/2320 [02:11<02:13,  8.96it/s{'loss': 0.0338, 'grad_norm': 0.1486990749835968, 'learning_rate': 5.486238532110092e-05, 'epoch': 4.849137931034483}                                         | 1125/2320 [02:11<02:13,  8.96it/s]
{'loss': 0.0338, 'grad_norm': 0.1486990749835968, 'learning_rate': 5.486238532110092e-05, 'epoch': 4.85}                                                                                         
{'loss': 0.0338, 'grad_norm': 0.1486990749835968, 'learning_rate': 5.486238532110092e-05, 'epoch': 4.85}                                                                                         
 50%|███████████████████████████████████████████████████████████████████████████▎                                                                            | 1150/2320 [02:13<02:11,  8.90it/s{'loss': 0.0289, 'grad_norm': 0.28092092275619507, 'learning_rate': 5.371559633027523e-05, 'epoch': 4.956896551724138}                                        | 1150/2320 [02:13<02:11,  8.90it/s]
{'loss': 0.0289, 'grad_norm': 0.28092092275619507, 'learning_rate': 5.371559633027523e-05, 'epoch': 4.96}                                                                                        
{'loss': 0.0289, 'grad_norm': 0.28092092275619507, 'learning_rate': 5.371559633027523e-05, 'epoch': 4.96}                                                                                        
 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 1160/2320 [02:14<02:08,  9.04it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.77it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.77it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.02577229216694832, 'eval_precision_micro': 0.9727582292849035, 'eval_recall_micro': 0.9185423365487674, 'eval_f1_micro': 0.9448732083792724, 'eval_precision_macro': 0.34272656578042193, 'eval_recall_macro': 0.31048446890762277, 'eval_f1_macro': 0.3243943328651665, 'eval_precision_weighted': 0.9242445687405981, 'eval_recall_weighted': 0.9185423365487674, 'eval_f1_weighted': 0.9203845990681039, 'eval_precision_samples': 0.9234088457389428, 'eval_recall_samples': 0.9216109313196691, 'eval_f1_samples': 0.9221503056454512, 'eval_f1': 0.9448732083792724, 'eval_subset_accuracy': 0.9201725997842503, 'eval_hamming_loss': 0.006345580303318738, 'eval_jaccard_micro': 0.8955067920585162, 'eval_jaccard_macro': 0.3015343892585857, 'eval_jaccard_weighted': 0.8944837535288439, 'eval_jaccard_samples': 0.9216109313196691, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.971134121853662, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.961764201406275, 'eval_avg_precision': 0.971134121853662, 'eval_runtime': 0.888, 'eval_samples_per_second': 1043.883, 'eval_steps_per_second': 32.657, 'epoch': 5.0}
{'eval_loss': 0.02577229216694832, 'eval_precision_micro': 0.9727582292849035, 'eval_recall_micro': 0.9185423365487674, 'eval_f1_micro': 0.9448732083792724, 'eval_precision_macro': 0.34272656578042193, 'eval_recall_macro': 0.31048446890762277, 'eval_f1_macro': 0.3243943328651665, 'eval_precision_weighted': 0.9242445687405981, 'eval_recall_weighted': 0.9185423365487674, 'eval_f1_weighted': 0.9203845990681039, 'eval_precision_samples': 0.9234088457389428, 'eval_recall_samples': 0.9216109313196691, 'eval_f1_samples': 0.9221503056454512, 'eval_f1': 0.9448732083792724, 'eval_subset_accuracy': 0.9201725997842503, 'eval_hamming_loss': 0.006345580303318738, 'eval_jaccard_micro': 0.8955067920585162, 'eval_jaccard_macro': 0.3015343892585857, 'eval_jaccard_weighted': 0.8944837535288439, 'eval_jaccard_samples': 0.9216109313196691, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.971134121853662, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.961764201406275, 'eval_avg_precision': 0.971134121853662, 'eval_runtime': 0.888, 'eval_samples_per_second': 1043.883, 'eval_steps_per_second': 32.657, 'epoch': 5.0}
{'eval_loss': 0.02577229216694832, 'eval_precision_micro': 0.9727582292849035, 'eval_recall_micro': 0.9185423365487674, 'eval_f1_micro': 0.9448732083792724, 'eval_precision_macro': 0.34272656578042193, 'eval_recall_macro': 0.31048446890762277, 'eval_f1_macro': 0.3243943328651665, 'eval_precision_weighted': 0.9242445687405981, 'eval_recall_weighted': 0.9185423365487674, 'eval_f1_weighted': 0.9203845990681039, 'eval_precision_samples': 0.9234088457389428, 'eval_recall_samples': 0.9216109313196691, 'eval_f1_samples': 0.9221503056454512, 'eval_f1': 0.9448732083792724, 'eval_subset_accuracy': 0.9201725997842503, 'eval_hamming_loss': 0.006345580303318738, 'eval_jaccard_micro': 0.8955067920585162, 'eval_jaccard_macro': 0.3015343892585857, 'eval_jaccard_weighted': 0.8944837535288439, 'eval_jaccard_samples': 0.9216109313196691, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.971134121853662, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.961764201406275, 'eval_avg_precision': 0.971134121853662, 'eval_runtime': 0.888, 'eval_samples_per_second': 1043.883, 'eval_steps_per_second': 32.657, 'epoch': 5.0}
 51%|████████████████████████████████████████████████████████████████████████████▉                                                                           | 1175/2320 [02:17<02:09,  8.87it/s{'loss': 0.0266, 'grad_norm': 0.07619545608758926, 'learning_rate': 5.256880733944954e-05, 'epoch': 5.064655172413793}                                        | 1175/2320 [02:17<02:09,  8.87it/s]
{'loss': 0.0266, 'grad_norm': 0.07619545608758926, 'learning_rate': 5.256880733944954e-05, 'epoch': 5.06}                                                                                        
{'loss': 0.0266, 'grad_norm': 0.07619545608758926, 'learning_rate': 5.256880733944954e-05, 'epoch': 5.06}                                                                                        
 52%|██████████████████████████████████████████████████████████████████████████████▌                                                                         | 1200/2320 [02:20<02:02,  9.11it/s{'loss': 0.0249, 'grad_norm': 0.10899079591035843, 'learning_rate': 5.142201834862386e-05, 'epoch': 5.172413793103448}                                        | 1200/2320 [02:20<02:02,  9.11it/s]
{'loss': 0.0249, 'grad_norm': 0.10899079591035843, 'learning_rate': 5.142201834862386e-05, 'epoch': 5.17}                                                                                        
{'loss': 0.0249, 'grad_norm': 0.10899079591035843, 'learning_rate': 5.142201834862386e-05, 'epoch': 5.17}                                                                                        
 53%|████████████████████████████████████████████████████████████████████████████████▎                                                                       | 1225/2320 [02:23<01:59,  9.19it/s{'loss': 0.028, 'grad_norm': 0.19045862555503845, 'learning_rate': 5.027522935779817e-05, 'epoch': 5.280172413793103}                                         | 1225/2320 [02:23<01:59,  9.19it/s]
{'loss': 0.028, 'grad_norm': 0.19045862555503845, 'learning_rate': 5.027522935779817e-05, 'epoch': 5.28}                                                                                         
{'loss': 0.028, 'grad_norm': 0.19045862555503845, 'learning_rate': 5.027522935779817e-05, 'epoch': 5.28}                                                                                         
 54%|█████████████████████████████████████████████████████████████████████████████████▉                                                                      | 1250/2320 [02:26<01:55,  9.27it/s{'loss': 0.0301, 'grad_norm': 0.20345230400562286, 'learning_rate': 4.9128440366972476e-05, 'epoch': 5.387931034482759}                                       | 1250/2320 [02:26<01:55,  9.27it/s]
{'loss': 0.0301, 'grad_norm': 0.20345230400562286, 'learning_rate': 4.9128440366972476e-05, 'epoch': 5.39}                                                                                       
{'loss': 0.0301, 'grad_norm': 0.20345230400562286, 'learning_rate': 4.9128440366972476e-05, 'epoch': 5.39}                                                                                       
 55%|███████████████████████████████████████████████████████████████████████████████████▌                                                                    | 1275/2320 [02:29<01:54,  9.11it/s{'loss': 0.0295, 'grad_norm': 0.2706785798072815, 'learning_rate': 4.7981651376146794e-05, 'epoch': 5.495689655172414}                                        | 1275/2320 [02:29<01:54,  9.11it/s]
{'loss': 0.0295, 'grad_norm': 0.2706785798072815, 'learning_rate': 4.7981651376146794e-05, 'epoch': 5.5}                                                                                         
{'loss': 0.0295, 'grad_norm': 0.2706785798072815, 'learning_rate': 4.7981651376146794e-05, 'epoch': 5.5}                                                                                         
 56%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 1300/2320 [02:31<01:53,  8.99it/s{'loss': 0.0213, 'grad_norm': 0.05864567309617996, 'learning_rate': 4.6834862385321106e-05, 'epoch': 5.603448275862069}                                       | 1300/2320 [02:31<01:53,  8.98it/s]
{'loss': 0.0213, 'grad_norm': 0.05864567309617996, 'learning_rate': 4.6834862385321106e-05, 'epoch': 5.6}                                                                                        
{'loss': 0.0213, 'grad_norm': 0.05864567309617996, 'learning_rate': 4.6834862385321106e-05, 'epoch': 5.6}                                                                                        
 57%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 1325/2320 [02:34<01:51,  8.93it/s{'loss': 0.0218, 'grad_norm': 0.04596620425581932, 'learning_rate': 4.568807339449542e-05, 'epoch': 5.711206896551724}                                        | 1325/2320 [02:34<01:51,  8.93it/s]
{'loss': 0.0218, 'grad_norm': 0.04596620425581932, 'learning_rate': 4.568807339449542e-05, 'epoch': 5.71}                                                                                        
{'loss': 0.0218, 'grad_norm': 0.04596620425581932, 'learning_rate': 4.568807339449542e-05, 'epoch': 5.71}                                                                                        
 58%|████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 1350/2320 [02:37<01:47,  9.05it/s{'loss': 0.0306, 'grad_norm': 0.24250152707099915, 'learning_rate': 4.454128440366972e-05, 'epoch': 5.818965517241379}                                        | 1350/2320 [02:37<01:47,  9.05it/s]
{'loss': 0.0306, 'grad_norm': 0.24250152707099915, 'learning_rate': 4.454128440366972e-05, 'epoch': 5.82}                                                                                        
{'loss': 0.0306, 'grad_norm': 0.24250152707099915, 'learning_rate': 4.454128440366972e-05, 'epoch': 5.82}                                                                                        
 59%|██████████████████████████████████████████████████████████████████████████████████████████                                                              | 1375/2320 [02:40<01:45,  8.97it/s{'loss': 0.0234, 'grad_norm': 0.15216302871704102, 'learning_rate': 4.339449541284404e-05, 'epoch': 5.926724137931035}                                        | 1375/2320 [02:40<01:45,  8.97it/s]
{'loss': 0.0234, 'grad_norm': 0.15216302871704102, 'learning_rate': 4.339449541284404e-05, 'epoch': 5.93}                                                                                        
{'loss': 0.0234, 'grad_norm': 0.15216302871704102, 'learning_rate': 4.339449541284404e-05, 'epoch': 5.93}                                                                                        
 60%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 1392/2320 [02:42<01:41,  9.13it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.72it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.72it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.02101965807378292, 'eval_precision_micro': 0.9807037457434733, 'eval_recall_micro': 0.9260450160771704, 'eval_f1_micro': 0.9525909592061741, 'eval_precision_macro': 0.33522489783354603, 'eval_recall_macro': 0.3311741470131615, 'eval_f1_macro': 0.3331065983401638, 'eval_precision_weighted': 0.9314417442711255, 'eval_recall_weighted': 0.9260450160771704, 'eval_f1_weighted': 0.9286862379901695, 'eval_precision_samples': 0.9320388349514563, 'eval_recall_samples': 0.9291621718806184, 'eval_f1_samples': 0.9300611290902554, 'eval_f1': 0.9525909592061741, 'eval_subset_accuracy': 0.9266450916936354, 'eval_hamming_loss': 0.005457199060854115, 'eval_jaccard_micro': 0.9094736842105263, 'eval_jaccard_macro': 0.31578921007407557, 'eval_jaccard_weighted': 0.9094362690968639, 'eval_jaccard_samples': 0.9291621718806184, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9805531595033384, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9736822085202769, 'eval_avg_precision': 0.9805531595033384, 'eval_runtime': 0.8835, 'eval_samples_per_second': 1049.264, 'eval_steps_per_second': 32.825, 'epoch': 6.0}
{'eval_loss': 0.02101965807378292, 'eval_precision_micro': 0.9807037457434733, 'eval_recall_micro': 0.9260450160771704, 'eval_f1_micro': 0.9525909592061741, 'eval_precision_macro': 0.33522489783354603, 'eval_recall_macro': 0.3311741470131615, 'eval_f1_macro': 0.3331065983401638, 'eval_precision_weighted': 0.9314417442711255, 'eval_recall_weighted': 0.9260450160771704, 'eval_f1_weighted': 0.9286862379901695, 'eval_precision_samples': 0.9320388349514563, 'eval_recall_samples': 0.9291621718806184, 'eval_f1_samples': 0.9300611290902554, 'eval_f1': 0.9525909592061741, 'eval_subset_accuracy': 0.9266450916936354, 'eval_hamming_loss': 0.005457199060854115, 'eval_jaccard_micro': 0.9094736842105263, 'eval_jaccard_macro': 0.31578921007407557, 'eval_jaccard_weighted': 0.9094362690968639, 'eval_jaccard_samples': 0.9291621718806184, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9805531595033384, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9736822085202769, 'eval_avg_precision': 0.9805531595033384, 'eval_runtime': 0.8835, 'eval_samples_per_second': 1049.264, 'eval_steps_per_second': 32.825, 'epoch': 6.0}
{'eval_loss': 0.02101965807378292, 'eval_precision_micro': 0.9807037457434733, 'eval_recall_micro': 0.9260450160771704, 'eval_f1_micro': 0.9525909592061741, 'eval_precision_macro': 0.33522489783354603, 'eval_recall_macro': 0.3311741470131615, 'eval_f1_macro': 0.3331065983401638, 'eval_precision_weighted': 0.9314417442711255, 'eval_recall_weighted': 0.9260450160771704, 'eval_f1_weighted': 0.9286862379901695, 'eval_precision_samples': 0.9320388349514563, 'eval_recall_samples': 0.9291621718806184, 'eval_f1_samples': 0.9300611290902554, 'eval_f1': 0.9525909592061741, 'eval_subset_accuracy': 0.9266450916936354, 'eval_hamming_loss': 0.005457199060854115, 'eval_jaccard_micro': 0.9094736842105263, 'eval_jaccard_macro': 0.31578921007407557, 'eval_jaccard_weighted': 0.9094362690968639, 'eval_jaccard_samples': 0.9291621718806184, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9805531595033384, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9736822085202769, 'eval_avg_precision': 0.9805531595033384, 'eval_runtime': 0.8835, 'eval_samples_per_second': 1049.264, 'eval_steps_per_second': 32.825, 'epoch': 6.0}
 60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 1400/2320 [02:44<02:22,  6.47it/s{'loss': 0.0241, 'grad_norm': 0.2077215015888214, 'learning_rate': 4.224770642201835e-05, 'epoch': 6.0344827586206895}                                        | 1400/2320 [02:44<02:22,  6.47it/s]
{'loss': 0.0241, 'grad_norm': 0.2077215015888214, 'learning_rate': 4.224770642201835e-05, 'epoch': 6.03}                                                                                         
{'loss': 0.0241, 'grad_norm': 0.2077215015888214, 'learning_rate': 4.224770642201835e-05, 'epoch': 6.03}                                                                                         
 61%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 1425/2320 [02:47<01:39,  8.98it/s{'loss': 0.0268, 'grad_norm': 0.10178040713071823, 'learning_rate': 4.1100917431192664e-05, 'epoch': 6.142241379310345}                                       | 1425/2320 [02:47<01:39,  8.98it/s]
{'loss': 0.0268, 'grad_norm': 0.10178040713071823, 'learning_rate': 4.1100917431192664e-05, 'epoch': 6.14}                                                                                       
{'loss': 0.0268, 'grad_norm': 0.10178040713071823, 'learning_rate': 4.1100917431192664e-05, 'epoch': 6.14}                                                                                       
 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 1450/2320 [02:49<01:36,  9.00it/s{'loss': 0.0246, 'grad_norm': 0.23419763147830963, 'learning_rate': 3.9954128440366975e-05, 'epoch': 6.25}                                                    | 1450/2320 [02:49<01:36,  9.00it/s]
{'loss': 0.0246, 'grad_norm': 0.23419763147830963, 'learning_rate': 3.9954128440366975e-05, 'epoch': 6.25}                                                                                       
{'loss': 0.0246, 'grad_norm': 0.23419763147830963, 'learning_rate': 3.9954128440366975e-05, 'epoch': 6.25}                                                                                       
 64%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 1475/2320 [02:52<01:33,  9.04it/s{'loss': 0.0257, 'grad_norm': 0.14567141234874725, 'learning_rate': 3.880733944954129e-05, 'epoch': 6.357758620689655}                                        | 1475/2320 [02:52<01:33,  9.04it/s]
{'loss': 0.0257, 'grad_norm': 0.14567141234874725, 'learning_rate': 3.880733944954129e-05, 'epoch': 6.36}                                                                                        
{'loss': 0.0257, 'grad_norm': 0.14567141234874725, 'learning_rate': 3.880733944954129e-05, 'epoch': 6.36}                                                                                        
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 1500/2320 [02:55<01:30,  9.03it/s{'loss': 0.0185, 'grad_norm': 0.10168421268463135, 'learning_rate': 3.76605504587156e-05, 'epoch': 6.4655172413793105}                                        | 1500/2320 [02:55<01:30,  9.03it/s]
{'loss': 0.0185, 'grad_norm': 0.10168421268463135, 'learning_rate': 3.76605504587156e-05, 'epoch': 6.47}                                                                                         
{'loss': 0.0185, 'grad_norm': 0.10168421268463135, 'learning_rate': 3.76605504587156e-05, 'epoch': 6.47}                                                                                         
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                    | 1525/2320 [02:58<01:28,  8.98it/s{'loss': 0.0238, 'grad_norm': 0.18851631879806519, 'learning_rate': 3.651376146788991e-05, 'epoch': 6.573275862068965}                                        | 1525/2320 [02:58<01:28,  8.98it/s]
{'loss': 0.0238, 'grad_norm': 0.18851631879806519, 'learning_rate': 3.651376146788991e-05, 'epoch': 6.57}                                                                                        
{'loss': 0.0238, 'grad_norm': 0.18851631879806519, 'learning_rate': 3.651376146788991e-05, 'epoch': 6.57}                                                                                        
 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 1550/2320 [03:00<01:25,  8.99it/s{'loss': 0.0236, 'grad_norm': 0.11254716664552689, 'learning_rate': 3.536697247706422e-05, 'epoch': 6.681034482758621}                                        | 1550/2320 [03:00<01:25,  8.99it/s]
{'loss': 0.0236, 'grad_norm': 0.11254716664552689, 'learning_rate': 3.536697247706422e-05, 'epoch': 6.68}                                                                                        
{'loss': 0.0236, 'grad_norm': 0.11254716664552689, 'learning_rate': 3.536697247706422e-05, 'epoch': 6.68}                                                                                        
 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 1575/2320 [03:03<01:23,  8.93it/s{'loss': 0.0238, 'grad_norm': 0.19487415254116058, 'learning_rate': 3.422018348623853e-05, 'epoch': 6.788793103448276}                                        | 1575/2320 [03:03<01:23,  8.93it/s]
{'loss': 0.0238, 'grad_norm': 0.19487415254116058, 'learning_rate': 3.422018348623853e-05, 'epoch': 6.79}                                                                                        
{'loss': 0.0238, 'grad_norm': 0.19487415254116058, 'learning_rate': 3.422018348623853e-05, 'epoch': 6.79}                                                                                        
 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 1600/2320 [03:06<01:19,  9.07it/s{'loss': 0.0157, 'grad_norm': 0.10383403301239014, 'learning_rate': 3.3073394495412845e-05, 'epoch': 6.896551724137931}                                       | 1600/2320 [03:06<01:19,  9.07it/s]
{'loss': 0.0157, 'grad_norm': 0.10383403301239014, 'learning_rate': 3.3073394495412845e-05, 'epoch': 6.9}                                                                                        
{'loss': 0.0157, 'grad_norm': 0.10383403301239014, 'learning_rate': 3.3073394495412845e-05, 'epoch': 6.9}                                                                                        
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1624/2320 [03:09<01:17,  9.03it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.80it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.80it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.018360724672675133, 'eval_precision_micro': 0.9841986455981941, 'eval_recall_micro': 0.9346195069667739, 'eval_f1_micro': 0.9587685541506322, 'eval_precision_macro': 0.3985257607858425, 'eval_recall_macro': 0.3562279460670928, 'eval_f1_macro': 0.36890126059714823, 'eval_precision_weighted': 0.9432982867052618, 'eval_recall_weighted': 0.9346195069667739, 'eval_f1_weighted': 0.937634459164003, 'eval_precision_samples': 0.9390507011866235, 'eval_recall_samples': 0.9372527867673498, 'eval_f1_samples': 0.9376123696512046, 'eval_f1': 0.9587685541506322, 'eval_subset_accuracy': 0.9341963322545846, 'eval_hamming_loss': 0.004759185227489054, 'eval_jaccard_micro': 0.9208025343189018, 'eval_jaccard_macro': 0.3443994352664667, 'eval_jaccard_weighted': 0.9206616185674837, 'eval_jaccard_samples': 0.9367134124415677, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9847161626721634, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9785431684224511, 'eval_avg_precision': 0.9847161626721634, 'eval_runtime': 0.8869, 'eval_samples_per_second': 1045.158, 'eval_steps_per_second': 32.696, 'epoch': 7.0}
{'eval_loss': 0.018360724672675133, 'eval_precision_micro': 0.9841986455981941, 'eval_recall_micro': 0.9346195069667739, 'eval_f1_micro': 0.9587685541506322, 'eval_precision_macro': 0.3985257607858425, 'eval_recall_macro': 0.3562279460670928, 'eval_f1_macro': 0.36890126059714823, 'eval_precision_weighted': 0.9432982867052618, 'eval_recall_weighted': 0.9346195069667739, 'eval_f1_weighted': 0.937634459164003, 'eval_precision_samples': 0.9390507011866235, 'eval_recall_samples': 0.9372527867673498, 'eval_f1_samples': 0.9376123696512046, 'eval_f1': 0.9587685541506322, 'eval_subset_accuracy': 0.9341963322545846, 'eval_hamming_loss': 0.004759185227489054, 'eval_jaccard_micro': 0.9208025343189018, 'eval_jaccard_macro': 0.3443994352664667, 'eval_jaccard_weighted': 0.9206616185674837, 'eval_jaccard_samples': 0.9367134124415677, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9847161626721634, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9785431684224511, 'eval_avg_precision': 0.9847161626721634, 'eval_runtime': 0.8869, 'eval_samples_per_second': 1045.158, 'eval_steps_per_second': 32.696, 'epoch': 7.0}
{'eval_loss': 0.018360724672675133, 'eval_precision_micro': 0.9841986455981941, 'eval_recall_micro': 0.9346195069667739, 'eval_f1_micro': 0.9587685541506322, 'eval_precision_macro': 0.3985257607858425, 'eval_recall_macro': 0.3562279460670928, 'eval_f1_macro': 0.36890126059714823, 'eval_precision_weighted': 0.9432982867052618, 'eval_recall_weighted': 0.9346195069667739, 'eval_f1_weighted': 0.937634459164003, 'eval_precision_samples': 0.9390507011866235, 'eval_recall_samples': 0.9372527867673498, 'eval_f1_samples': 0.9376123696512046, 'eval_f1': 0.9587685541506322, 'eval_subset_accuracy': 0.9341963322545846, 'eval_hamming_loss': 0.004759185227489054, 'eval_jaccard_micro': 0.9208025343189018, 'eval_jaccard_macro': 0.3443994352664667, 'eval_jaccard_weighted': 0.9206616185674837, 'eval_jaccard_samples': 0.9367134124415677, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9847161626721634, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9785431684224511, 'eval_avg_precision': 0.9847161626721634, 'eval_runtime': 0.8869, 'eval_samples_per_second': 1045.158, 'eval_steps_per_second': 32.696, 'epoch': 7.0}
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1625/2320 [03:10<05:56,  1.95it/s{'loss': 0.0152, 'grad_norm': 0.15962998569011688, 'learning_rate': 3.1926605504587156e-05, 'epoch': 7.004310344827586}                                       | 1625/2320 [03:10<05:56,  1.95it/s]
{'loss': 0.0152, 'grad_norm': 0.15962998569011688, 'learning_rate': 3.1926605504587156e-05, 'epoch': 7.0}                                                                                        
{'loss': 0.0152, 'grad_norm': 0.15962998569011688, 'learning_rate': 3.1926605504587156e-05, 'epoch': 7.0}                                                                                        
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 1650/2320 [03:13<01:14,  9.01it/s{'loss': 0.0284, 'grad_norm': 0.1327258050441742, 'learning_rate': 3.077981651376147e-05, 'epoch': 7.112068965517241}                                         | 1650/2320 [03:13<01:14,  9.01it/s]
{'loss': 0.0284, 'grad_norm': 0.1327258050441742, 'learning_rate': 3.077981651376147e-05, 'epoch': 7.11}                                                                                         
{'loss': 0.0284, 'grad_norm': 0.1327258050441742, 'learning_rate': 3.077981651376147e-05, 'epoch': 7.11}                                                                                         
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 1675/2320 [03:16<01:11,  9.06it/s{'loss': 0.0163, 'grad_norm': 0.08849132061004639, 'learning_rate': 2.963302752293578e-05, 'epoch': 7.219827586206897}                                        | 1675/2320 [03:16<01:11,  9.06it/s]
{'loss': 0.0163, 'grad_norm': 0.08849132061004639, 'learning_rate': 2.963302752293578e-05, 'epoch': 7.22}                                                                                        
{'loss': 0.0163, 'grad_norm': 0.08849132061004639, 'learning_rate': 2.963302752293578e-05, 'epoch': 7.22}                                                                                        
 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 1700/2320 [03:18<01:08,  9.04it/s{'loss': 0.0225, 'grad_norm': 0.02771965228021145, 'learning_rate': 2.8486238532110095e-05, 'epoch': 7.327586206896552}                                       | 1700/2320 [03:18<01:08,  9.04it/s]
{'loss': 0.0225, 'grad_norm': 0.02771965228021145, 'learning_rate': 2.8486238532110095e-05, 'epoch': 7.33}                                                                                       
{'loss': 0.0225, 'grad_norm': 0.02771965228021145, 'learning_rate': 2.8486238532110095e-05, 'epoch': 7.33}                                                                                       
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 1725/2320 [03:21<01:06,  8.95it/s{'loss': 0.0165, 'grad_norm': 0.07395478338003159, 'learning_rate': 2.7339449541284406e-05, 'epoch': 7.435344827586207}                                       | 1725/2320 [03:21<01:06,  8.95it/s]
{'loss': 0.0165, 'grad_norm': 0.07395478338003159, 'learning_rate': 2.7339449541284406e-05, 'epoch': 7.44}                                                                                       
{'loss': 0.0165, 'grad_norm': 0.07395478338003159, 'learning_rate': 2.7339449541284406e-05, 'epoch': 7.44}                                                                                       
 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 1750/2320 [03:24<01:03,  9.03it/s{'loss': 0.0206, 'grad_norm': 0.07781102508306503, 'learning_rate': 2.6192660550458714e-05, 'epoch': 7.543103448275862}█▋                                     | 1750/2320 [03:24<01:03,  9.03it/s]
{'loss': 0.0206, 'grad_norm': 0.07781102508306503, 'learning_rate': 2.6192660550458714e-05, 'epoch': 7.54}                                                                                       
{'loss': 0.0206, 'grad_norm': 0.07781102508306503, 'learning_rate': 2.6192660550458714e-05, 'epoch': 7.54}                                                                                       
 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 1775/2320 [03:27<01:01,  8.84it/s{'loss': 0.0237, 'grad_norm': 0.1651109755039215, 'learning_rate': 2.5045871559633026e-05, 'epoch': 7.650862068965517}████▎                                   | 1775/2320 [03:27<01:01,  8.84it/s]
{'loss': 0.0237, 'grad_norm': 0.1651109755039215, 'learning_rate': 2.5045871559633026e-05, 'epoch': 7.65}                                                                                        
{'loss': 0.0237, 'grad_norm': 0.1651109755039215, 'learning_rate': 2.5045871559633026e-05, 'epoch': 7.65}                                                                                        
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 1800/2320 [03:30<00:58,  8.87it/s{'loss': 0.0157, 'grad_norm': 0.0592854805290699, 'learning_rate': 2.389908256880734e-05, 'epoch': 7.758620689655173}██████▉                                  | 1800/2320 [03:30<00:58,  8.87it/s]
{'loss': 0.0157, 'grad_norm': 0.0592854805290699, 'learning_rate': 2.389908256880734e-05, 'epoch': 7.76}                                                                                         
{'loss': 0.0157, 'grad_norm': 0.0592854805290699, 'learning_rate': 2.389908256880734e-05, 'epoch': 7.76}                                                                                         
 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                | 1825/2320 [03:32<00:56,  8.78it/s{'loss': 0.0174, 'grad_norm': 0.022606827318668365, 'learning_rate': 2.2752293577981652e-05, 'epoch': 7.866379310344827}█████▌                                | 1825/2320 [03:32<00:56,  8.78it/s]
{'loss': 0.0174, 'grad_norm': 0.022606827318668365, 'learning_rate': 2.2752293577981652e-05, 'epoch': 7.87}                                                                                      
{'loss': 0.0174, 'grad_norm': 0.022606827318668365, 'learning_rate': 2.2752293577981652e-05, 'epoch': 7.87}                                                                                      
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 1850/2320 [03:35<00:52,  8.93it/s{'loss': 0.0201, 'grad_norm': 0.020930252969264984, 'learning_rate': 2.1605504587155964e-05, 'epoch': 7.974137931034483}███████▏                              | 1850/2320 [03:35<00:52,  8.93it/s]
{'loss': 0.0201, 'grad_norm': 0.020930252969264984, 'learning_rate': 2.1605504587155964e-05, 'epoch': 7.97}                                                                                      
{'loss': 0.0201, 'grad_norm': 0.020930252969264984, 'learning_rate': 2.1605504587155964e-05, 'epoch': 7.97}                                                                                      
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 1856/2320 [03:36<00:50,  9.24it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.81it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.81it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.016724003478884697, 'eval_precision_micro': 0.9810479375696767, 'eval_recall_micro': 0.9431939978563773, 'eval_f1_micro': 0.9617486338797814, 'eval_precision_macro': 0.4395467615627419, 'eval_recall_macro': 0.3893013334199581, 'eval_f1_macro': 0.3943033858828295, 'eval_precision_weighted': 0.9573769120798329, 'eval_recall_weighted': 0.9431939978563773, 'eval_f1_weighted': 0.9443774596227311, 'eval_precision_samples': 0.9471413160733549, 'eval_recall_samples': 0.9458827759798633, 'eval_f1_samples': 0.9458827759798633, 'eval_f1': 0.9617486338797814, 'eval_subset_accuracy': 0.941747572815534, 'eval_hamming_loss': 0.004441906212323117, 'eval_jaccard_micro': 0.9263157894736842, 'eval_jaccard_macro': 0.3656997212449488, 'eval_jaccard_weighted': 0.9270237733139663, 'eval_jaccard_samples': 0.944804027328299, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9870387009074735, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9811225020269938, 'eval_avg_precision': 0.9870387009074735, 'eval_runtime': 0.8795, 'eval_samples_per_second': 1054.035, 'eval_steps_per_second': 32.974, 'epoch': 8.0}
{'eval_loss': 0.016724003478884697, 'eval_precision_micro': 0.9810479375696767, 'eval_recall_micro': 0.9431939978563773, 'eval_f1_micro': 0.9617486338797814, 'eval_precision_macro': 0.4395467615627419, 'eval_recall_macro': 0.3893013334199581, 'eval_f1_macro': 0.3943033858828295, 'eval_precision_weighted': 0.9573769120798329, 'eval_recall_weighted': 0.9431939978563773, 'eval_f1_weighted': 0.9443774596227311, 'eval_precision_samples': 0.9471413160733549, 'eval_recall_samples': 0.9458827759798633, 'eval_f1_samples': 0.9458827759798633, 'eval_f1': 0.9617486338797814, 'eval_subset_accuracy': 0.941747572815534, 'eval_hamming_loss': 0.004441906212323117, 'eval_jaccard_micro': 0.9263157894736842, 'eval_jaccard_macro': 0.3656997212449488, 'eval_jaccard_weighted': 0.9270237733139663, 'eval_jaccard_samples': 0.944804027328299, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9870387009074735, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9811225020269938, 'eval_avg_precision': 0.9870387009074735, 'eval_runtime': 0.8795, 'eval_samples_per_second': 1054.035, 'eval_steps_per_second': 32.974, 'epoch': 8.0}
{'eval_loss': 0.016724003478884697, 'eval_precision_micro': 0.9810479375696767, 'eval_recall_micro': 0.9431939978563773, 'eval_f1_micro': 0.9617486338797814, 'eval_precision_macro': 0.4395467615627419, 'eval_recall_macro': 0.3893013334199581, 'eval_f1_macro': 0.3943033858828295, 'eval_precision_weighted': 0.9573769120798329, 'eval_recall_weighted': 0.9431939978563773, 'eval_f1_weighted': 0.9443774596227311, 'eval_precision_samples': 0.9471413160733549, 'eval_recall_samples': 0.9458827759798633, 'eval_f1_samples': 0.9458827759798633, 'eval_f1': 0.9617486338797814, 'eval_subset_accuracy': 0.941747572815534, 'eval_hamming_loss': 0.004441906212323117, 'eval_jaccard_micro': 0.9263157894736842, 'eval_jaccard_macro': 0.3656997212449488, 'eval_jaccard_weighted': 0.9270237733139663, 'eval_jaccard_samples': 0.944804027328299, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9870387009074735, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9811225020269938, 'eval_avg_precision': 0.9870387009074735, 'eval_runtime': 0.8795, 'eval_samples_per_second': 1054.035, 'eval_steps_per_second': 32.974, 'epoch': 8.0}
 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 1875/2320 [03:39<00:49,  8.93it/s{'loss': 0.0123, 'grad_norm': 0.08531025797128677, 'learning_rate': 2.045871559633028e-05, 'epoch': 8.081896551724139}██████████▊                             | 1875/2320 [03:39<00:49,  8.93it/s]
{'loss': 0.0123, 'grad_norm': 0.08531025797128677, 'learning_rate': 2.045871559633028e-05, 'epoch': 8.08}                                                                                        
{'loss': 0.0123, 'grad_norm': 0.08531025797128677, 'learning_rate': 2.045871559633028e-05, 'epoch': 8.08}                                                                                        
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 1900/2320 [03:42<00:46,  8.98it/s{'loss': 0.0198, 'grad_norm': 0.13688765466213226, 'learning_rate': 1.9311926605504587e-05, 'epoch': 8.189655172413794}███████████▍                           | 1900/2320 [03:42<00:46,  8.98it/s]
{'loss': 0.0198, 'grad_norm': 0.13688765466213226, 'learning_rate': 1.9311926605504587e-05, 'epoch': 8.19}                                                                                       
{'loss': 0.0198, 'grad_norm': 0.13688765466213226, 'learning_rate': 1.9311926605504587e-05, 'epoch': 8.19}                                                                                       
 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 1925/2320 [03:45<00:43,  9.19it/s{'loss': 0.0165, 'grad_norm': 0.06687736511230469, 'learning_rate': 1.8165137614678902e-05, 'epoch': 8.297413793103448}█████████████                          | 1925/2320 [03:45<00:42,  9.19it/s]
{'loss': 0.0165, 'grad_norm': 0.06687736511230469, 'learning_rate': 1.8165137614678902e-05, 'epoch': 8.3}                                                                                        
{'loss': 0.0165, 'grad_norm': 0.06687736511230469, 'learning_rate': 1.8165137614678902e-05, 'epoch': 8.3}                                                                                        
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 1950/2320 [03:48<00:40,  9.04it/s{'loss': 0.0191, 'grad_norm': 0.11366534233093262, 'learning_rate': 1.701834862385321e-05, 'epoch': 8.405172413793103}███████████████▊                        | 1950/2320 [03:48<00:40,  9.04it/s]
{'loss': 0.0191, 'grad_norm': 0.11366534233093262, 'learning_rate': 1.701834862385321e-05, 'epoch': 8.41}                                                                                        
{'loss': 0.0191, 'grad_norm': 0.11366534233093262, 'learning_rate': 1.701834862385321e-05, 'epoch': 8.41}                                                                                        
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 1975/2320 [03:50<00:38,  8.97it/s{'loss': 0.0203, 'grad_norm': 0.06615179777145386, 'learning_rate': 1.5871559633027525e-05, 'epoch': 8.512931034482758}████████████████▍                      | 1975/2320 [03:50<00:38,  8.97it/s]
{'loss': 0.0203, 'grad_norm': 0.06615179777145386, 'learning_rate': 1.5871559633027525e-05, 'epoch': 8.51}                                                                                       
{'loss': 0.0203, 'grad_norm': 0.06615179777145386, 'learning_rate': 1.5871559633027525e-05, 'epoch': 8.51}                                                                                       
 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 2000/2320 [03:53<00:35,  9.08it/s{'loss': 0.0171, 'grad_norm': 0.04863600432872772, 'learning_rate': 1.4724770642201835e-05, 'epoch': 8.620689655172415}██████████████████                     | 2000/2320 [03:53<00:35,  9.08it/s]
{'loss': 0.0171, 'grad_norm': 0.04863600432872772, 'learning_rate': 1.4724770642201835e-05, 'epoch': 8.62}                                                                                       
{'loss': 0.0171, 'grad_norm': 0.04863600432872772, 'learning_rate': 1.4724770642201835e-05, 'epoch': 8.62}                                                                                       
 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 2025/2320 [03:56<00:32,  9.08it/s{'loss': 0.019, 'grad_norm': 0.12082919478416443, 'learning_rate': 1.3577981651376149e-05, 'epoch': 8.72844827586207}█████████████████████▋                   | 2025/2320 [03:56<00:32,  9.08it/s]
{'loss': 0.019, 'grad_norm': 0.12082919478416443, 'learning_rate': 1.3577981651376149e-05, 'epoch': 8.73}                                                                                        
{'loss': 0.019, 'grad_norm': 0.12082919478416443, 'learning_rate': 1.3577981651376149e-05, 'epoch': 8.73}                                                                                        
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 2050/2320 [03:59<00:29,  9.01it/s{'loss': 0.016, 'grad_norm': 0.010959053412079811, 'learning_rate': 1.243119266055046e-05, 'epoch': 8.836206896551724}██████████████████████▎                 | 2050/2320 [03:59<00:29,  9.01it/s]
{'loss': 0.016, 'grad_norm': 0.010959053412079811, 'learning_rate': 1.243119266055046e-05, 'epoch': 8.84}                                                                                        
{'loss': 0.016, 'grad_norm': 0.010959053412079811, 'learning_rate': 1.243119266055046e-05, 'epoch': 8.84}                                                                                        
 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 2075/2320 [04:01<00:26,  9.13it/s{'loss': 0.0192, 'grad_norm': 0.20213066041469574, 'learning_rate': 1.1284403669724772e-05, 'epoch': 8.943965517241379}██████████████████████▉                | 2075/2320 [04:01<00:26,  9.13it/s]
{'loss': 0.0192, 'grad_norm': 0.20213066041469574, 'learning_rate': 1.1284403669724772e-05, 'epoch': 8.94}                                                                                       
{'loss': 0.0192, 'grad_norm': 0.20213066041469574, 'learning_rate': 1.1284403669724772e-05, 'epoch': 8.94}                                                                                       
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 2088/2320 [04:03<00:25,  9.19it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.78it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.78it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.015931295230984688, 'eval_precision_micro': 0.9834070796460177, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9678824169842134, 'eval_precision_macro': 0.44854941348014926, 'eval_recall_macro': 0.4219069655571161, 'eval_f1_macro': 0.43133784928100327, 'eval_precision_weighted': 0.959555048185654, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9550573025755453, 'eval_precision_samples': 0.9563106796116505, 'eval_recall_samples': 0.9550521395181588, 'eval_f1_samples': 0.9552319309600863, 'eval_f1': 0.9678824169842134, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003743892378958056, 'eval_jaccard_micro': 0.9377637130801688, 'eval_jaccard_macro': 0.4034173669467787, 'eval_jaccard_weighted': 0.9381809830041342, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9881377441075258, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9829640781243583, 'eval_avg_precision': 0.9881377441075258, 'eval_runtime': 0.8809, 'eval_samples_per_second': 1052.327, 'eval_steps_per_second': 32.921, 'epoch': 9.0}
{'eval_loss': 0.015931295230984688, 'eval_precision_micro': 0.9834070796460177, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9678824169842134, 'eval_precision_macro': 0.44854941348014926, 'eval_recall_macro': 0.4219069655571161, 'eval_f1_macro': 0.43133784928100327, 'eval_precision_weighted': 0.959555048185654, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9550573025755453, 'eval_precision_samples': 0.9563106796116505, 'eval_recall_samples': 0.9550521395181588, 'eval_f1_samples': 0.9552319309600863, 'eval_f1': 0.9678824169842134, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003743892378958056, 'eval_jaccard_micro': 0.9377637130801688, 'eval_jaccard_macro': 0.4034173669467787, 'eval_jaccard_weighted': 0.9381809830041342, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9881377441075258, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9829640781243583, 'eval_avg_precision': 0.9881377441075258, 'eval_runtime': 0.8809, 'eval_samples_per_second': 1052.327, 'eval_steps_per_second': 32.921, 'epoch': 9.0}
{'eval_loss': 0.015931295230984688, 'eval_precision_micro': 0.9834070796460177, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9678824169842134, 'eval_precision_macro': 0.44854941348014926, 'eval_recall_macro': 0.4219069655571161, 'eval_f1_macro': 0.43133784928100327, 'eval_precision_weighted': 0.959555048185654, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9550573025755453, 'eval_precision_samples': 0.9563106796116505, 'eval_recall_samples': 0.9550521395181588, 'eval_f1_samples': 0.9552319309600863, 'eval_f1': 0.9678824169842134, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003743892378958056, 'eval_jaccard_micro': 0.9377637130801688, 'eval_jaccard_macro': 0.4034173669467787, 'eval_jaccard_weighted': 0.9381809830041342, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9881377441075258, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9829640781243583, 'eval_avg_precision': 0.9881377441075258, 'eval_runtime': 0.8809, 'eval_samples_per_second': 1052.327, 'eval_steps_per_second': 32.921, 'epoch': 9.0}
 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 2100/2320 [04:05<00:27,  7.92it/s{'loss': 0.018, 'grad_norm': 0.15830574929714203, 'learning_rate': 1.0137614678899083e-05, 'epoch': 9.051724137931034}█████████████████████████▌              | 2100/2320 [04:05<00:27,  7.92it/s]
{'loss': 0.018, 'grad_norm': 0.15830574929714203, 'learning_rate': 1.0137614678899083e-05, 'epoch': 9.05}                                                                                        
{'loss': 0.018, 'grad_norm': 0.15830574929714203, 'learning_rate': 1.0137614678899083e-05, 'epoch': 9.05}                                                                                        
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 2125/2320 [04:08<00:21,  9.06it/s{'loss': 0.0181, 'grad_norm': 0.32579541206359863, 'learning_rate': 8.990825688073395e-06, 'epoch': 9.15948275862069}████████████████████████████▏            | 2125/2320 [04:08<00:21,  9.06it/s]
{'loss': 0.0181, 'grad_norm': 0.32579541206359863, 'learning_rate': 8.990825688073395e-06, 'epoch': 9.16}                                                                                        
{'loss': 0.0181, 'grad_norm': 0.32579541206359863, 'learning_rate': 8.990825688073395e-06, 'epoch': 9.16}                                                                                        
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 2150/2320 [04:11<00:19,  8.80it/s{'loss': 0.0157, 'grad_norm': 0.1342673897743225, 'learning_rate': 7.844036697247707e-06, 'epoch': 9.267241379310345}█████████████████████████████▊           | 2150/2320 [04:11<00:19,  8.80it/s]
{'loss': 0.0157, 'grad_norm': 0.1342673897743225, 'learning_rate': 7.844036697247707e-06, 'epoch': 9.27}                                                                                         
{'loss': 0.0157, 'grad_norm': 0.1342673897743225, 'learning_rate': 7.844036697247707e-06, 'epoch': 9.27}                                                                                         
 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 2175/2320 [04:14<00:16,  9.05it/s{'loss': 0.0154, 'grad_norm': 0.15610016882419586, 'learning_rate': 6.697247706422019e-06, 'epoch': 9.375}██████████████████████████████████████████▌         | 2175/2320 [04:14<00:16,  9.06it/s]
{'loss': 0.0154, 'grad_norm': 0.15610016882419586, 'learning_rate': 6.697247706422019e-06, 'epoch': 9.38}                                                                                        
{'loss': 0.0154, 'grad_norm': 0.15610016882419586, 'learning_rate': 6.697247706422019e-06, 'epoch': 9.38}                                                                                        
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 2200/2320 [04:17<00:13,  8.99it/s{'loss': 0.0211, 'grad_norm': 0.19655324518680573, 'learning_rate': 5.5504587155963306e-06, 'epoch': 9.482758620689655}███████████████████████████████▏       | 2200/2320 [04:17<00:13,  8.99it/s]
{'loss': 0.0211, 'grad_norm': 0.19655324518680573, 'learning_rate': 5.5504587155963306e-06, 'epoch': 9.48}                                                                                       
{'loss': 0.0211, 'grad_norm': 0.19655324518680573, 'learning_rate': 5.5504587155963306e-06, 'epoch': 9.48}                                                                                       
 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2225/2320 [04:20<00:10,  8.90it/s{'loss': 0.0204, 'grad_norm': 0.08821550011634827, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59051724137931}██████████████████████████████████▊      | 2225/2320 [04:20<00:10,  8.90it/s]
{'loss': 0.0204, 'grad_norm': 0.08821550011634827, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59}                                                                                        
{'loss': 0.0204, 'grad_norm': 0.08821550011634827, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59}                                                                                        
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 2250/2320 [04:22<00:07,  8.97it/s{'loss': 0.0156, 'grad_norm': 0.033170051872730255, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.698275862068966}█████████████████████████████████▍    | 2250/2320 [04:22<00:07,  8.97it/s]
{'loss': 0.0156, 'grad_norm': 0.033170051872730255, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.7}                                                                                       
{'loss': 0.0156, 'grad_norm': 0.033170051872730255, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.7}                                                                                       
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 2275/2320 [04:25<00:05,  8.90it/s{'loss': 0.0184, 'grad_norm': 0.13204126060009003, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.806034482758621}█████████████████████████████████████   | 2275/2320 [04:25<00:05,  8.90it/s]
{'loss': 0.0184, 'grad_norm': 0.13204126060009003, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.81}                                                                                        
{'loss': 0.0184, 'grad_norm': 0.13204126060009003, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.81}                                                                                        
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 2300/2320 [04:28<00:02,  9.11it/s{'loss': 0.0163, 'grad_norm': 0.09435597062110901, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.913793103448276}███████████████████████████████████████▋ | 2300/2320 [04:28<00:02,  9.11it/s]
{'loss': 0.0163, 'grad_norm': 0.09435597062110901, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.91}                                                                                         
{'loss': 0.0163, 'grad_norm': 0.09435597062110901, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.91}                                                                                         
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:30<00:00,  9.45it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.50it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.50it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.015811562538146973, 'eval_precision_micro': 0.9834437086092715, 'eval_recall_micro': 0.954983922829582, 'eval_f1_micro': 0.9690048939641109, 'eval_precision_macro': 0.4486354850740117, 'eval_recall_macro': 0.42423405023714195, 'eval_f1_macro': 0.4325723017265284, 'eval_precision_weighted': 0.959644267780793, 'eval_recall_weighted': 0.954983922829582, 'eval_f1_weighted': 0.9561830934596441, 'eval_precision_samples': 0.9579288025889967, 'eval_recall_samples': 0.9572096368212872, 'eval_f1_samples': 0.9570298453793599, 'eval_f1': 0.9690048939641109, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003616980772891681, 'eval_jaccard_micro': 0.939873417721519, 'eval_jaccard_macro': 0.40566409897292255, 'eval_jaccard_weighted': 0.940239009680328, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9882894951570467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9838124106347597, 'eval_avg_precision': 0.9882894951570467, 'eval_runtime': 0.8583, 'eval_samples_per_second': 1080.027, 'eval_steps_per_second': 33.787, 'epoch': 10.0}
{'eval_loss': 0.015811562538146973, 'eval_precision_micro': 0.9834437086092715, 'eval_recall_micro': 0.954983922829582, 'eval_f1_micro': 0.9690048939641109, 'eval_precision_macro': 0.4486354850740117, 'eval_recall_macro': 0.42423405023714195, 'eval_f1_macro': 0.4325723017265284, 'eval_precision_weighted': 0.959644267780793, 'eval_recall_weighted': 0.954983922829582, 'eval_f1_weighted': 0.9561830934596441, 'eval_precision_samples': 0.9579288025889967, 'eval_recall_samples': 0.9572096368212872, 'eval_f1_samples': 0.9570298453793599, 'eval_f1': 0.9690048939641109, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003616980772891681, 'eval_jaccard_micro': 0.939873417721519, 'eval_jaccard_macro': 0.40566409897292255, 'eval_jaccard_weighted': 0.940239009680328, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9882894951570467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9838124106347597, 'eval_avg_precision': 0.9882894951570467, 'eval_runtime': 0.8583, 'eval_samples_per_second': 1080.027, 'eval_steps_per_second': 33.787, 'epoch': 10.0}
{'eval_loss': 0.015811562538146973, 'eval_precision_micro': 0.9834437086092715, 'eval_recall_micro': 0.954983922829582, 'eval_f1_micro': 0.9690048939641109, 'eval_precision_macro': 0.4486354850740117, 'eval_recall_macro': 0.42423405023714195, 'eval_f1_macro': 0.4325723017265284, 'eval_precision_weighted': 0.959644267780793, 'eval_recall_weighted': 0.954983922829582, 'eval_f1_weighted': 0.9561830934596441, 'eval_precision_samples': 0.9579288025889967, 'eval_recall_samples': 0.9572096368212872, 'eval_f1_samples': 0.9570298453793599, 'eval_f1': 0.9690048939641109, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003616980772891681, 'eval_jaccard_micro': 0.939873417721519, 'eval_jaccard_macro': 0.40566409897292255, 'eval_jaccard_weighted': 0.940239009680328, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9882894951570467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9838124106347597, 'eval_avg_precision': 0.9882894951570467, 'eval_runtime': 0.8583, 'eval_samples_per_second': 1080.027, 'eval_steps_per_second': 33.787, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:31<00:00,  9.45it/s{'train_runtime': 272.3483, 'train_samples_per_second': 136.112, 'train_steps_per_second': 8.519, 'train_loss': 0.06069241859789552, 'epoch': 10.0}███████████| 2320/2320 [04:31<00:00,  9.45it/s]
{'train_runtime': 272.3483, 'train_samples_per_second': 136.112, 'train_steps_per_second': 8.519, 'train_loss': 0.06069241859789552, 'epoch': 10.0}                                              
{'train_runtime': 272.3483, 'train_samples_per_second': 136.112, 'train_steps_per_second': 8.519, 'train_loss': 0.06069241859789552, 'epoch': 10.0}                                              
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:32<00:00,  8.52it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:32<00:00,  8.52it/s]
2025-04-26 11:56:53,751 - INFO - TrainMain - *** Training Finished ***
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0607
  train_runtime            = 0:04:32.34
  train_samples_per_second =    136.112
  train_steps_per_second   =      8.519
2025-04-26 11:56:53,757 - INFO - TrainMain - Training metrics: {'train_runtime': 272.3483, 'train_samples_per_second': 136.112, 'train_steps_per_second': 8.519, 'train_loss': 0.06069241859789552, 'epoch': 10.0}
2025-04-26 11:56:53,757 - INFO - TrainMain - *** Starting Final Evaluation on Validation Set ***
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.07it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.015811562538146973, 'eval_precision_micro': 0.9834437086092715, 'eval_recall_micro': 0.954983922829582, 'eval_f1_micro': 0.9690048939641109, 'eval_precision_macro': 0.4486354850740117, 'eval_recall_macro': 0.42423405023714195, 'eval_f1_macro': 0.4325723017265284, 'eval_precision_weighted': 0.959644267780793, 'eval_recall_weighted': 0.954983922829582, 'eval_f1_weighted': 0.9561830934596441, 'eval_precision_samples': 0.9579288025889967, 'eval_recall_samples': 0.9572096368212872, 'eval_f1_samples': 0.9570298453793599, 'eval_f1': 0.9690048939641109, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003616980772891681, 'eval_jaccard_micro': 0.939873417721519, 'eval_jaccard_macro': 0.40566409897292255, 'eval_jaccard_weighted': 0.940239009680328, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9882894951570467, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9838124106347597, 'eval_avg_precision': 0.9882894951570467, 'eval_runtime': 0.8448, 'eval_samples_per_second': 1097.365, 'eval_steps_per_second': 34.33, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 35.52it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 35.51it/s]
***** eval metrics *****
  eval_epoch                       =       10.0
  eval_eval_avg_precision          =     0.9883
  eval_eval_avg_precision_macro    =        nan
  eval_eval_avg_precision_micro    =     0.9883
  eval_eval_avg_precision_weighted =     0.9838
  eval_eval_f1                     =      0.969
  eval_eval_f1_macro               =     0.4326
  eval_eval_f1_micro               =      0.969
  eval_eval_f1_samples             =      0.957
  eval_eval_f1_weighted            =     0.9562
  eval_eval_hamming_loss           =     0.0036
  eval_eval_jaccard_macro          =     0.4057
  eval_eval_jaccard_micro          =     0.9399
  eval_eval_jaccard_samples        =     0.9561
  eval_eval_jaccard_weighted       =     0.9402
  eval_eval_loss                   =     0.0158
  eval_eval_precision_macro        =     0.4486
  eval_eval_precision_micro        =     0.9834
  eval_eval_precision_samples      =     0.9579
  eval_eval_precision_weighted     =     0.9596
  eval_eval_recall_macro           =     0.4242
  eval_eval_recall_micro           =      0.955
  eval_eval_recall_samples         =     0.9572
  eval_eval_recall_weighted        =      0.955
  eval_eval_roc_auc                =        nan
  eval_eval_roc_auc_macro          =        nan
  eval_eval_roc_auc_micro          =        nan
  eval_eval_roc_auc_weighted       =        nan
  eval_eval_runtime                = 0:00:00.84
  eval_eval_samples_per_second     =   1097.365
  eval_eval_steps_per_second       =      34.33
  eval_eval_subset_accuracy        =     0.9536
2025-04-26 11:56:54,614 - INFO - TrainMain - Final validation metrics: {'eval_eval_loss': 0.015811562538146973, 'eval_eval_precision_micro': 0.9834437086092715, 'eval_eval_recall_micro': 0.954983922829582, 'eval_eval_f1_micro': 0.9690048939641109, 'eval_eval_precision_macro': 0.4486354850740117, 'eval_eval_recall_macro': 0.42423405023714195, 'eval_eval_f1_macro': 0.4325723017265284, 'eval_eval_precision_weighted': 0.959644267780793, 'eval_eval_recall_weighted': 0.954983922829582, 'eval_eval_f1_weighted': 0.9561830934596441, 'eval_eval_precision_samples': 0.9579288025889967, 'eval_eval_recall_samples': 0.9572096368212872, 'eval_eval_f1_samples': 0.9570298453793599, 'eval_eval_f1': 0.9690048939641109, 'eval_eval_subset_accuracy': 0.95361380798274, 'eval_eval_hamming_loss': 0.003616980772891681, 'eval_eval_jaccard_micro': 0.939873417721519, 'eval_eval_jaccard_macro': 0.40566409897292255, 'eval_eval_jaccard_weighted': 0.940239009680328, 'eval_eval_jaccard_samples': 0.956130888169723, 'eval_eval_roc_auc_micro': nan, 'eval_eval_roc_auc_macro': nan, 'eval_eval_roc_auc_weighted': nan, 'eval_eval_roc_auc': nan, 'eval_eval_avg_precision_micro': 0.9882894951570467, 'eval_eval_avg_precision_macro': nan, 'eval_eval_avg_precision_weighted': 0.9838124106347597, 'eval_eval_avg_precision': 0.9882894951570467, 'eval_eval_runtime': 0.8448, 'eval_eval_samples_per_second': 1097.365, 'eval_eval_steps_per_second': 34.33, 'eval_epoch': 10.0}
2025-04-26 11:56:54,614 - INFO - TrainMain - *** Starting Test Set Evaluation ***
2025-04-26 11:56:54,614 - INFO - TrainMain - Loading test dataset from: ./coding_task/data/atis/test.tsv
2025-04-26 11:56:54,614 - INFO - DataProcessor - DataProcessor initialized with precomputed label mappings.
2025-04-26 11:56:54,614 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/test.tsv using Pandas
2025-04-26 11:56:54,614 - INFO - Data Utils - File extension: .tsv
2025-04-26 11:56:54,614 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 11:56:54,614 - INFO - Data Utils -   file_path: ./coding_task/data/atis/test.tsv
2025-04-26 11:56:54,615 - INFO - Data Utils -   file_type: tsv
2025-04-26 11:56:54,615 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 11:56:54,615 - INFO - Data Utils -   use_dask: False
2025-04-26 11:56:54,615 - INFO - Data Utils -   unpack_multi_labels: True
2025-04-26 11:56:54,615 - INFO - Data Utils -   label_column_name: atis_labels
2025-04-26 11:56:54,615 - INFO - Data Utils -   label_delimiter: +
2025-04-26 11:56:54,615 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:56:54,620 - INFO - Data Utils - Loaded initial Pandas df with 850 rows and 2 columns.
2025-04-26 11:56:54,621 - INFO - Data Utils - Unpacking multi-labels in column 'atis_labels' using delimiter '+'.
2025-04-26 11:56:54,627 - INFO - Data Utils - Unpacking complete. Initial rows: 850, Final rows: 865.
2025-04-26 11:56:54,633 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:56:54,633 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:56:54,635 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:56:54,636 - INFO - Data Utils - Resetting index after cleanup. New shape: (865, 2)
2025-04-26 11:56:54,637 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (865, 2)
2025-04-26 11:56:54,642 - INFO - DataProcessor - Preparing labels for task type: multilabel
2025-04-26 11:56:54,642 - INFO - DataProcessor - Grouping unpacked labels by text to prepare for multi-hot encoding...
2025-04-26 11:56:54,664 - INFO - DataProcessor - Using precomputed 17 labels for multilabel task.
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:860: UserWarning: unknown class(es) ['day_name'] will be ignored
  warnings.warn('unknown class(es) {0} will be ignored'
2025-04-26 11:56:54,670 - INFO - DataProcessor - Reconstructed DataFrame shape for multilabel: (850, 2)
2025-04-26 11:56:54,675 - INFO - DataProcessor - Using full dataset for training (validation_split_ratio=0).
2025-04-26 11:56:54,676 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:56:54,688 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 850/850 [00:00<00:00, 12789.85 examples/s]
2025-04-26 11:56:54,797 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:56:54,800 - INFO - TrainMain - *** Starting Test Set Evaluation ***
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 25/27 [00:00<00:00, 39.74it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.025372443720698357, 'eval_precision_micro': 0.9519112207151664, 'eval_recall_micro': 0.8945538818076477, 'eval_f1_micro': 0.922341696535245, 'eval_precision_macro': 0.37516996707095407, 'eval_recall_macro': 0.45627292567352457, 'eval_f1_macro': 0.3982961190502567, 'eval_precision_weighted': 0.8862538466632066, 'eval_recall_weighted': 0.8945538818076477, 'eval_f1_weighted': 0.888234202794817, 'eval_precision_samples': 0.9058823529411765, 'eval_recall_samples': 0.8988235294117647, 'eval_f1_samples': 0.9011764705882352, 'eval_f1': 0.922341696535245, 'eval_subset_accuracy': 0.8917647058823529, 'eval_hamming_loss': 0.008996539792387544, 'eval_jaccard_micro': 0.8558758314855875, 'eval_jaccard_macro': 0.36339557491175933, 'eval_jaccard_weighted': 0.8660813690596818, 'eval_jaccard_samples': 0.8988235294117647, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.974987867292457, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.976728216209286, 'eval_avg_precision': 0.974987867292457, 'eval_runtime': 0.7941, 'eval_samples_per_second': 1070.352, 'eval_steps_per_second': 33.999, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 35.48it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 35.47it/s]
***** test metrics *****
  test_epoch                       =       10.0
  test_eval_avg_precision          =      0.975
  test_eval_avg_precision_macro    =        nan
  test_eval_avg_precision_micro    =      0.975
  test_eval_avg_precision_weighted =     0.9767
  test_eval_f1                     =     0.9223
  test_eval_f1_macro               =     0.3983
  test_eval_f1_micro               =     0.9223
  test_eval_f1_samples             =     0.9012
  test_eval_f1_weighted            =     0.8882
  test_eval_hamming_loss           =      0.009
  test_eval_jaccard_macro          =     0.3634
  test_eval_jaccard_micro          =     0.8559
  test_eval_jaccard_samples        =     0.8988
  test_eval_jaccard_weighted       =     0.8661
  test_eval_loss                   =     0.0254
  test_eval_precision_macro        =     0.3752
  test_eval_precision_micro        =     0.9519
  test_eval_precision_samples      =     0.9059
  test_eval_precision_weighted     =     0.8863
  test_eval_recall_macro           =     0.4563
  test_eval_recall_micro           =     0.8946
  test_eval_recall_samples         =     0.8988
  test_eval_recall_weighted        =     0.8946
  test_eval_roc_auc                =        nan
  test_eval_roc_auc_macro          =        nan
  test_eval_roc_auc_micro          =        nan
  test_eval_roc_auc_weighted       =        nan
  test_eval_runtime                = 0:00:00.79
  test_eval_samples_per_second     =   1070.352
  test_eval_steps_per_second       =     33.999
  test_eval_subset_accuracy        =     0.8918
2025-04-26 11:56:55,605 - INFO - TrainMain - Test set evaluation metrics: {'test_eval_loss': 0.025372443720698357, 'test_eval_precision_micro': 0.9519112207151664, 'test_eval_recall_micro': 0.8945538818076477, 'test_eval_f1_micro': 0.922341696535245, 'test_eval_precision_macro': 0.37516996707095407, 'test_eval_recall_macro': 0.45627292567352457, 'test_eval_f1_macro': 0.3982961190502567, 'test_eval_precision_weighted': 0.8862538466632066, 'test_eval_recall_weighted': 0.8945538818076477, 'test_eval_f1_weighted': 0.888234202794817, 'test_eval_precision_samples': 0.9058823529411765, 'test_eval_recall_samples': 0.8988235294117647, 'test_eval_f1_samples': 0.9011764705882352, 'test_eval_f1': 0.922341696535245, 'test_eval_subset_accuracy': 0.8917647058823529, 'test_eval_hamming_loss': 0.008996539792387544, 'test_eval_jaccard_micro': 0.8558758314855875, 'test_eval_jaccard_macro': 0.36339557491175933, 'test_eval_jaccard_weighted': 0.8660813690596818, 'test_eval_jaccard_samples': 0.8988235294117647, 'test_eval_roc_auc_micro': nan, 'test_eval_roc_auc_macro': nan, 'test_eval_roc_auc_weighted': nan, 'test_eval_roc_auc': nan, 'test_eval_avg_precision_micro': 0.974987867292457, 'test_eval_avg_precision_macro': nan, 'test_eval_avg_precision_weighted': 0.976728216209286, 'test_eval_avg_precision': 0.974987867292457, 'test_eval_runtime': 0.7941, 'test_eval_samples_per_second': 1070.352, 'test_eval_steps_per_second': 33.999, 'test_epoch': 10.0}
2025-04-26 11:56:55,605 - INFO - TrainMain - *** Evaluation Finished ***
2025-04-26 11:56:55,605 - INFO - TrainMain - Saving final model/adapter to ./results/atis_multilabel_xlmr_lora
2025-04-26 11:56:56,050 - INFO - TrainMain - Tokenizer saved to ./results/atis_multilabel_xlmr_lora
2025-04-26 11:56:56,050 - ERROR - TrainMain - Failed to save training arguments: 'TrainingConfig' object has no attribute 'to_dict'
2025-04-26 11:56:56,050 - INFO - TrainMain - Script finished successfully.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(env-pointnet) gururaj@bt69xp2:~/LRP_Experiment/zendesk-mle-master$ 
