(env-pointnet) gururaj@bt69xp2:~/LRP_Experiment/zendesk-mle-master$ bash ./coding_task/train/train_multiclass.sh
2025-04-26 11:34:30,255 - INFO - TrainMain - Parsing configuration from command line arguments.
2025-04-26 11:34:30,396 - INFO - TrainMain - --- Data Configuration ---
2025-04-26 11:34:30,396 - INFO - TrainMain - DataConfig(dataset_path='./coding_task/data/atis/train.tsv', text_column='atis_text', label_column='atis_labels', task_type='multiclass', unpack_multi_labels=False, label_delimiter='+', use_dask=False, validation_split_ratio=0.2, max_seq_length=128, test_dataset_path='./coding_task/data/atis/test.tsv')
2025-04-26 11:34:30,396 - INFO - TrainMain - --- Model Configuration ---
2025-04-26 11:34:30,396 - INFO - TrainMain - ModelConfig(model_name_or_path='xlm-roberta-base', cache_dir=None, freeze_base_model=True)
2025-04-26 11:34:30,397 - INFO - TrainMain - --- PEFT Configuration ---
2025-04-26 11:34:30,397 - INFO - TrainMain - PeftConfig(method='lora', lora_r=64, lora_alpha=16, lora_dropout=0.1, lora_target_modules=None)
2025-04-26 11:34:30,397 - INFO - TrainMain - --- Training Configuration ---
2025-04-26 11:34:30,397 - INFO - TrainMain - TrainingConfig(output_dir='./results/atis_multiclass_xlmr_lora', num_train_epochs=10.0, per_device_train_batch_size=16, per_device_eval_batch_size=32, learning_rate=0.0001, weight_decay=0.01, lr_scheduler_type='linear', warmup_ratio=0.06, logging_dir='/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/logs/training_logs', logging_steps=25, evaluation_strategy='epoch', eval_steps=None, save_strategy='epoch', save_steps=None, save_total_limit=2, load_best_model_at_end=True, metric_for_best_model='eval_f1_weighted', greater_is_better=True, fp16=True, gradient_accumulation_steps=1, gradient_checkpointing=False, seed=42, report_to=['tensorboard'])
2025-04-26 11:34:30,399 - INFO - TrainMain - Set random seed to: 42
2025-04-26 11:34:30,399 - INFO - TrainMain - Output directory: ./results/atis_multiclass_xlmr_lora
2025-04-26 11:34:30,399 - INFO - TrainMain - Loading tokenizer: xlm-roberta-base
2025-04-26 11:34:32,248 - INFO - TrainMain - Starting data processing for Train/Validation...
2025-04-26 11:34:32,248 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/train.tsv using Pandas
2025-04-26 11:34:32,248 - INFO - Data Utils - File extension: .tsv
2025-04-26 11:34:32,248 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 11:34:32,248 - INFO - Data Utils -   file_path: ./coding_task/data/atis/train.tsv
2025-04-26 11:34:32,248 - INFO - Data Utils -   file_type: tsv
2025-04-26 11:34:32,248 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 11:34:32,249 - INFO - Data Utils -   use_dask: False
2025-04-26 11:34:32,249 - INFO - Data Utils -   unpack_multi_labels: False
2025-04-26 11:34:32,249 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:34:32,255 - INFO - Data Utils - Loaded initial Pandas df with 4634 rows and 2 columns.
2025-04-26 11:34:32,256 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:34:32,256 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:34:32,260 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:34:32,263 - INFO - Data Utils - Resetting index after cleanup. New shape: (4634, 2)
2025-04-26 11:34:32,263 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (4634, 2)
2025-04-26 11:34:32,267 - INFO - DataProcessor - Preparing labels for task type: multiclass
2025-04-26 11:34:32,267 - INFO - DataProcessor - Calculated 22 unique labels for multiclass task.
2025-04-26 11:34:32,270 - INFO - DataProcessor - Splitting data into train/validation (0.8/0.2)
2025-04-26 11:34:32,271 - INFO - DataProcessor - Train size: 3707, Validation size: 927
2025-04-26 11:34:32,271 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:34:32,288 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3707/3707 [00:00<00:00, 27351.16 examples/s]
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 927/927 [00:00<00:00, 26775.29 examples/s]
2025-04-26 11:34:32,522 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:34:32,548 - INFO - TrainMain - Data processing complete. Number of labels: 22
2025-04-26 11:34:32,549 - INFO - TrainMain - Label mapping (id2label): {0: 'abbreviation', 1: 'aircraft', 2: 'aircraft+flight+flight_no', 3: 'airfare', 4: 'airfare+flight_time', 5: 'airline', 6: 'airline+flight_no', 7: 'airport', 8: 'capacity', 9: 'cheapest', 10: 'city', 11: 'distance', 12: 'flight', 13: 'flight+airfare', 14: 'flight_no', 15: 'flight_time', 16: 'ground_fare', 17: 'ground_service', 18: 'ground_service+ground_fare', 19: 'meal', 20: 'quantity', 21: 'restriction'}
2025-04-26 11:34:32,550 - INFO - TrainMain - Label mappings saved to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:34:32,550 - INFO - TrainMain - Loading model for training...
2025-04-26 11:34:32,550 - INFO - ModelLoader - Loading base model: xlm-roberta-base
2025-04-26 11:34:32,551 - INFO - ModelLoader - Configuring model for 'single_label_classification' (num_labels=22).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-04-26 11:34:35,210 - INFO - ModelLoader - Freezing base model parameters.
2025-04-26 11:34:35,212 - INFO - ModelLoader - Applying PEFT method: lora
2025-04-26 11:34:35,212 - INFO - ModelLoader - lora_target_modules not specified, attempting auto-detection by PEFT library.
2025-04-26 11:34:35,212 - INFO - ModelLoader - LoRA Config: r=64, alpha=16, dropout=0.1, target_modules=auto
2025-04-26 11:34:35,498 - INFO - ModelLoader - PEFT model created successfully.
trainable params: 2,966,806 || all params: 281,027,372 || trainable%: 1.0557
2025-04-26 11:34:35,499 - INFO - TrainMain - Model loading complete.
2025-04-26 11:34:35,500 - INFO - TrainMain - Setting up Trainer...
2025-04-26 11:34:35,500 - INFO - TrainerSetup - Configuring HuggingFace Trainer...
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-04-26 11:34:35,616 - INFO - TrainerSetup - Adding EarlyStoppingCallback with patience=3 based on 'eval_f1_weighted'.
/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/train/trainer.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-04-26 11:34:36,231] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
You are adding a <class 'transformers.trainer_callback.ProgressCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
TensorBoardCallback
PrinterCallback
ProgressCallback
EarlyStoppingCallback
2025-04-26 11:34:39,220 - INFO - TrainerSetup - Trainer configured.
2025-04-26 11:34:39,221 - INFO - TrainMain - Trainer setup complete.
2025-04-26 11:34:39,221 - INFO - TrainMain - *** Starting Training ***
  1%|█▋                                                                                                                                                        | 25/2320 [00:03<04:15,  8.99it/s{'loss': 3.0863, 'grad_norm': 9.576775550842285, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.10775862068965517}                                         | 25/2320 [00:03<04:15,  8.99it/s]
{'loss': 3.0863, 'grad_norm': 9.576775550842285, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.11}                                                                                         
{'loss': 3.0863, 'grad_norm': 9.576775550842285, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.11}                                                                                         
  2%|███▎                                                                                                                                                      | 50/2320 [00:05<04:13,  8.96it/s{'loss': 2.717, 'grad_norm': 7.616507530212402, 'learning_rate': 3.5e-05, 'epoch': 0.21551724137931033}                                                         | 50/2320 [00:05<04:13,  8.96it/s]
{'loss': 2.717, 'grad_norm': 7.616507530212402, 'learning_rate': 3.5e-05, 'epoch': 0.22}                                                                                                         
{'loss': 2.717, 'grad_norm': 7.616507530212402, 'learning_rate': 3.5e-05, 'epoch': 0.22}                                                                                                         
  3%|████▉                                                                                                                                                     | 75/2320 [00:08<04:09,  9.00it/s{'loss': 1.9434, 'grad_norm': 4.235355377197266, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.3232758620689655}                                           | 75/2320 [00:08<04:09,  9.00it/s]
{'loss': 1.9434, 'grad_norm': 4.235355377197266, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.32}                                                                                          
{'loss': 1.9434, 'grad_norm': 4.235355377197266, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.32}                                                                                          
  4%|██████▌                                                                                                                                                  | 100/2320 [00:11<03:59,  9.27it/s{'loss': 1.2175, 'grad_norm': 2.856937885284424, 'learning_rate': 6.928571428571429e-05, 'epoch': 0.43103448275862066}                                         | 100/2320 [00:11<03:59,  9.27it/s]
{'loss': 1.2175, 'grad_norm': 2.856937885284424, 'learning_rate': 6.928571428571429e-05, 'epoch': 0.43}                                                                                          
{'loss': 1.2175, 'grad_norm': 2.856937885284424, 'learning_rate': 6.928571428571429e-05, 'epoch': 0.43}                                                                                          
  5%|████████▏                                                                                                                                                | 125/2320 [00:14<04:03,  9.00it/s{'loss': 1.2877, 'grad_norm': 3.4575393199920654, 'learning_rate': 8.714285714285715e-05, 'epoch': 0.5387931034482759}                                         | 125/2320 [00:14<04:03,  9.00it/s]
{'loss': 1.2877, 'grad_norm': 3.4575393199920654, 'learning_rate': 8.714285714285715e-05, 'epoch': 0.54}                                                                                         
{'loss': 1.2877, 'grad_norm': 3.4575393199920654, 'learning_rate': 8.714285714285715e-05, 'epoch': 0.54}                                                                                         
  6%|█████████▉                                                                                                                                               | 150/2320 [00:16<03:59,  9.04it/s{'loss': 1.1846, 'grad_norm': 3.2794580459594727, 'learning_rate': 9.967889908256882e-05, 'epoch': 0.646551724137931}                                          | 150/2320 [00:16<03:59,  9.04it/s]
{'loss': 1.1846, 'grad_norm': 3.2794580459594727, 'learning_rate': 9.967889908256882e-05, 'epoch': 0.65}                                                                                         
{'loss': 1.1846, 'grad_norm': 3.2794580459594727, 'learning_rate': 9.967889908256882e-05, 'epoch': 0.65}                                                                                         
  8%|███████████▌                                                                                                                                             | 175/2320 [00:19<03:54,  9.13it/s{'loss': 1.105, 'grad_norm': 8.43746566772461, 'learning_rate': 9.853211009174312e-05, 'epoch': 0.7543103448275862}                                            | 175/2320 [00:19<03:54,  9.13it/s]
{'loss': 1.105, 'grad_norm': 8.43746566772461, 'learning_rate': 9.853211009174312e-05, 'epoch': 0.75}                                                                                            
{'loss': 1.105, 'grad_norm': 8.43746566772461, 'learning_rate': 9.853211009174312e-05, 'epoch': 0.75}                                                                                            
  9%|█████████████▏                                                                                                                                           | 200/2320 [00:22<03:50,  9.19it/s{'loss': 0.9721, 'grad_norm': 1.826577067375183, 'learning_rate': 9.738532110091744e-05, 'epoch': 0.8620689655172413}                                          | 200/2320 [00:22<03:50,  9.19it/s]
{'loss': 0.9721, 'grad_norm': 1.826577067375183, 'learning_rate': 9.738532110091744e-05, 'epoch': 0.86}                                                                                          
{'loss': 0.9721, 'grad_norm': 1.826577067375183, 'learning_rate': 9.738532110091744e-05, 'epoch': 0.86}                                                                                          
 10%|██████████████▊                                                                                                                                          | 225/2320 [00:25<03:51,  9.06it/s{'loss': 0.9428, 'grad_norm': 2.588916540145874, 'learning_rate': 9.623853211009176e-05, 'epoch': 0.9698275862068966}                                          | 225/2320 [00:25<03:51,  9.06it/s]
{'loss': 0.9428, 'grad_norm': 2.588916540145874, 'learning_rate': 9.623853211009176e-05, 'epoch': 0.97}                                                                                          
{'loss': 0.9428, 'grad_norm': 2.588916540145874, 'learning_rate': 9.623853211009176e-05, 'epoch': 0.97}                                                                                          
 10%|███████████████▎                                                                                                                                         | 232/2320 [00:25<03:51,  9.01it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.78it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.78it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in scalar divide
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
{'eval_loss': 0.8013966679573059, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.9004, 'eval_samples_per_second': 1029.576, 'eval_steps_per_second': 32.209, 'epoch': 1.0}
{'eval_loss': 0.8013966679573059, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.9004, 'eval_samples_per_second': 1029.576, 'eval_steps_per_second': 32.209, 'epoch': 1.0}                                                                                  
{'eval_loss': 0.8013966679573059, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.9004, 'eval_samples_per_second': 1029.576, 'eval_steps_per_second': 32.209, 'epoch': 1.0}                                                                                  
 11%|████████████████▍                                                                                                                                        | 250/2320 [00:29<03:52,  8.92it/s{'loss': 0.9563, 'grad_norm': 2.231635808944702, 'learning_rate': 9.509174311926606e-05, 'epoch': 1.0775862068965518}                                          | 250/2320 [00:29<03:52,  8.92it/s]
{'loss': 0.9563, 'grad_norm': 2.231635808944702, 'learning_rate': 9.509174311926606e-05, 'epoch': 1.08}                                                                                          
{'loss': 0.9563, 'grad_norm': 2.231635808944702, 'learning_rate': 9.509174311926606e-05, 'epoch': 1.08}                                                                                          
 12%|██████████████████▏                                                                                                                                      | 275/2320 [00:32<03:46,  9.02it/s{'loss': 0.9133, 'grad_norm': 4.824625492095947, 'learning_rate': 9.39908256880734e-05, 'epoch': 1.1853448275862069}                                           | 275/2320 [00:32<03:46,  9.02it/s]
{'loss': 0.9133, 'grad_norm': 4.824625492095947, 'learning_rate': 9.39908256880734e-05, 'epoch': 1.19}                                                                                           
{'loss': 0.9133, 'grad_norm': 4.824625492095947, 'learning_rate': 9.39908256880734e-05, 'epoch': 1.19}                                                                                           
 13%|███████████████████▊                                                                                                                                     | 300/2320 [00:34<03:37,  9.28it/s{'loss': 0.684, 'grad_norm': 2.2733569145202637, 'learning_rate': 9.284403669724772e-05, 'epoch': 1.293103448275862}                                           | 300/2320 [00:34<03:37,  9.28it/s]
{'loss': 0.684, 'grad_norm': 2.2733569145202637, 'learning_rate': 9.284403669724772e-05, 'epoch': 1.29}                                                                                          
{'loss': 0.684, 'grad_norm': 2.2733569145202637, 'learning_rate': 9.284403669724772e-05, 'epoch': 1.29}                                                                                          
 14%|█████████████████████▍                                                                                                                                   | 325/2320 [00:37<03:39,  9.07it/s{'loss': 0.6752, 'grad_norm': 1.9450695514678955, 'learning_rate': 9.169724770642202e-05, 'epoch': 1.4008620689655173}                                         | 325/2320 [00:37<03:39,  9.07it/s]
{'loss': 0.6752, 'grad_norm': 1.9450695514678955, 'learning_rate': 9.169724770642202e-05, 'epoch': 1.4}                                                                                          
{'loss': 0.6752, 'grad_norm': 1.9450695514678955, 'learning_rate': 9.169724770642202e-05, 'epoch': 1.4}                                                                                          
 15%|███████████████████████                                                                                                                                  | 350/2320 [00:40<03:39,  8.96it/s{'loss': 0.7064, 'grad_norm': 2.4309380054473877, 'learning_rate': 9.055045871559633e-05, 'epoch': 1.5086206896551724}                                         | 350/2320 [00:40<03:39,  8.96it/s]
{'loss': 0.7064, 'grad_norm': 2.4309380054473877, 'learning_rate': 9.055045871559633e-05, 'epoch': 1.51}                                                                                         
{'loss': 0.7064, 'grad_norm': 2.4309380054473877, 'learning_rate': 9.055045871559633e-05, 'epoch': 1.51}                                                                                         
 16%|████████████████████████▋                                                                                                                                | 375/2320 [00:42<03:33,  9.09it/s{'loss': 0.7049, 'grad_norm': 3.0577352046966553, 'learning_rate': 8.940366972477065e-05, 'epoch': 1.6163793103448276}                                         | 375/2320 [00:42<03:33,  9.09it/s]
{'loss': 0.7049, 'grad_norm': 3.0577352046966553, 'learning_rate': 8.940366972477065e-05, 'epoch': 1.62}                                                                                         
{'loss': 0.7049, 'grad_norm': 3.0577352046966553, 'learning_rate': 8.940366972477065e-05, 'epoch': 1.62}                                                                                         
 17%|██████████████████████████▍                                                                                                                              | 400/2320 [00:45<03:30,  9.12it/s{'loss': 0.5765, 'grad_norm': 1.4412469863891602, 'learning_rate': 8.825688073394495e-05, 'epoch': 1.7241379310344827}                                         | 400/2320 [00:45<03:30,  9.12it/s]
{'loss': 0.5765, 'grad_norm': 1.4412469863891602, 'learning_rate': 8.825688073394495e-05, 'epoch': 1.72}                                                                                         
{'loss': 0.5765, 'grad_norm': 1.4412469863891602, 'learning_rate': 8.825688073394495e-05, 'epoch': 1.72}                                                                                         
 18%|████████████████████████████                                                                                                                             | 425/2320 [00:48<03:26,  9.15it/s{'loss': 0.5017, 'grad_norm': 4.435009479522705, 'learning_rate': 8.711009174311927e-05, 'epoch': 1.831896551724138}                                           | 425/2320 [00:48<03:26,  9.16it/s]
{'loss': 0.5017, 'grad_norm': 4.435009479522705, 'learning_rate': 8.711009174311927e-05, 'epoch': 1.83}                                                                                          
{'loss': 0.5017, 'grad_norm': 4.435009479522705, 'learning_rate': 8.711009174311927e-05, 'epoch': 1.83}                                                                                          
 19%|█████████████████████████████▋                                                                                                                           | 450/2320 [00:51<03:22,  9.24it/s{'loss': 0.3987, 'grad_norm': 1.8683792352676392, 'learning_rate': 8.596330275229359e-05, 'epoch': 1.9396551724137931}                                         | 450/2320 [00:51<03:22,  9.24it/s]
{'loss': 0.3987, 'grad_norm': 1.8683792352676392, 'learning_rate': 8.596330275229359e-05, 'epoch': 1.94}                                                                                         
{'loss': 0.3987, 'grad_norm': 1.8683792352676392, 'learning_rate': 8.596330275229359e-05, 'epoch': 1.94}                                                                                         
 20%|██████████████████████████████▌                                                                                                                          | 464/2320 [00:52<03:19,  9.29it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.07it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.06it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.32514047622680664, 'eval_accuracy': 0.9212513484358145, 'eval_precision_micro': 0.9212513484358145, 'eval_recall_micro': 0.9212513484358145, 'eval_f1_micro': 0.9212513484358145, 'eval_precision_macro': 0.31444766061877716, 'eval_recall_macro': 0.35177320746971474, 'eval_f1_macro': 0.33083095487435915, 'eval_precision_weighted': 0.8841735907811882, 'eval_recall_weighted': 0.9212513484358145, 'eval_f1_weighted': 0.9012588176795691, 'eval_f1': 0.9012588176795691, 'eval_balanced_accuracy': 0.35177320746971474, 'eval_cohen_kappa': 0.8140007256255772, 'eval_matthews_corrcoef': 0.8152505643333838, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8858, 'eval_samples_per_second': 1046.512, 'eval_steps_per_second': 32.739, 'epoch': 2.0}
{'eval_loss': 0.32514047622680664, 'eval_accuracy': 0.9212513484358145, 'eval_precision_micro': 0.9212513484358145, 'eval_recall_micro': 0.9212513484358145, 'eval_f1_micro': 0.9212513484358145, 'eval_precision_macro': 0.31444766061877716, 'eval_recall_macro': 0.35177320746971474, 'eval_f1_macro': 0.33083095487435915, 'eval_precision_weighted': 0.8841735907811882, 'eval_recall_weighted': 0.9212513484358145, 'eval_f1_weighted': 0.9012588176795691, 'eval_f1': 0.9012588176795691, 'eval_balanced_accuracy': 0.35177320746971474, 'eval_cohen_kappa': 0.8140007256255772, 'eval_matthews_corrcoef': 0.8152505643333838, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8858, 'eval_samples_per_second': 1046.512, 'eval_steps_per_second': 32.739, 'epoch': 2.0}                          
{'eval_loss': 0.32514047622680664, 'eval_accuracy': 0.9212513484358145, 'eval_precision_micro': 0.9212513484358145, 'eval_recall_micro': 0.9212513484358145, 'eval_f1_micro': 0.9212513484358145, 'eval_precision_macro': 0.31444766061877716, 'eval_recall_macro': 0.35177320746971474, 'eval_f1_macro': 0.33083095487435915, 'eval_precision_weighted': 0.8841735907811882, 'eval_recall_weighted': 0.9212513484358145, 'eval_f1_weighted': 0.9012588176795691, 'eval_f1': 0.9012588176795691, 'eval_balanced_accuracy': 0.35177320746971474, 'eval_cohen_kappa': 0.8140007256255772, 'eval_matthews_corrcoef': 0.8152505643333838, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8858, 'eval_samples_per_second': 1046.512, 'eval_steps_per_second': 32.739, 'epoch': 2.0}                          
 20%|███████████████████████████████▎                                                                                                                         | 475/2320 [00:55<03:46,  8.16it/s{'loss': 0.3178, 'grad_norm': 1.8964956998825073, 'learning_rate': 8.48165137614679e-05, 'epoch': 2.0474137931034484}                                          | 475/2320 [00:55<03:46,  8.16it/s]
{'loss': 0.3178, 'grad_norm': 1.8964956998825073, 'learning_rate': 8.48165137614679e-05, 'epoch': 2.05}                                                                                          
{'loss': 0.3178, 'grad_norm': 1.8964956998825073, 'learning_rate': 8.48165137614679e-05, 'epoch': 2.05}                                                                                          
 22%|████████████████████████████████▉                                                                                                                        | 500/2320 [00:58<03:21,  9.05it/s{'loss': 0.5307, 'grad_norm': 1.8207603693008423, 'learning_rate': 8.366972477064221e-05, 'epoch': 2.1551724137931036}                                         | 500/2320 [00:58<03:21,  9.05it/s]
{'loss': 0.5307, 'grad_norm': 1.8207603693008423, 'learning_rate': 8.366972477064221e-05, 'epoch': 2.16}                                                                                         
{'loss': 0.5307, 'grad_norm': 1.8207603693008423, 'learning_rate': 8.366972477064221e-05, 'epoch': 2.16}                                                                                         
 23%|██████████████████████████████████▌                                                                                                                      | 525/2320 [01:00<03:16,  9.13it/s{'loss': 0.3627, 'grad_norm': 0.720468282699585, 'learning_rate': 8.252293577981652e-05, 'epoch': 2.2629310344827585}                                          | 525/2320 [01:00<03:16,  9.13it/s]
{'loss': 0.3627, 'grad_norm': 0.720468282699585, 'learning_rate': 8.252293577981652e-05, 'epoch': 2.26}                                                                                          
{'loss': 0.3627, 'grad_norm': 0.720468282699585, 'learning_rate': 8.252293577981652e-05, 'epoch': 2.26}                                                                                          
 24%|████████████████████████████████████▎                                                                                                                    | 550/2320 [01:03<03:15,  9.04it/s{'loss': 0.3164, 'grad_norm': 2.152801513671875, 'learning_rate': 8.137614678899082e-05, 'epoch': 2.3706896551724137}                                          | 550/2320 [01:03<03:15,  9.04it/s]
{'loss': 0.3164, 'grad_norm': 2.152801513671875, 'learning_rate': 8.137614678899082e-05, 'epoch': 2.37}                                                                                          
{'loss': 0.3164, 'grad_norm': 2.152801513671875, 'learning_rate': 8.137614678899082e-05, 'epoch': 2.37}                                                                                          
 25%|█████████████████████████████████████▉                                                                                                                   | 575/2320 [01:06<03:12,  9.05it/s{'loss': 0.3743, 'grad_norm': 1.393741488456726, 'learning_rate': 8.022935779816514e-05, 'epoch': 2.478448275862069}                                           | 575/2320 [01:06<03:12,  9.05it/s]
{'loss': 0.3743, 'grad_norm': 1.393741488456726, 'learning_rate': 8.022935779816514e-05, 'epoch': 2.48}                                                                                          
{'loss': 0.3743, 'grad_norm': 1.393741488456726, 'learning_rate': 8.022935779816514e-05, 'epoch': 2.48}                                                                                          
 26%|███████████████████████████████████████▌                                                                                                                 | 600/2320 [01:09<03:09,  9.10it/s{'loss': 0.269, 'grad_norm': 2.4255666732788086, 'learning_rate': 7.908256880733946e-05, 'epoch': 2.586206896551724}                                           | 600/2320 [01:09<03:09,  9.10it/s]
{'loss': 0.269, 'grad_norm': 2.4255666732788086, 'learning_rate': 7.908256880733946e-05, 'epoch': 2.59}                                                                                          
{'loss': 0.269, 'grad_norm': 2.4255666732788086, 'learning_rate': 7.908256880733946e-05, 'epoch': 2.59}                                                                                          
 27%|█████████████████████████████████████████▏                                                                                                               | 625/2320 [01:11<03:11,  8.85it/s{'loss': 0.3138, 'grad_norm': 3.1551198959350586, 'learning_rate': 7.793577981651376e-05, 'epoch': 2.6939655172413794}                                         | 625/2320 [01:11<03:11,  8.85it/s]
{'loss': 0.3138, 'grad_norm': 3.1551198959350586, 'learning_rate': 7.793577981651376e-05, 'epoch': 2.69}                                                                                         
{'loss': 0.3138, 'grad_norm': 3.1551198959350586, 'learning_rate': 7.793577981651376e-05, 'epoch': 2.69}                                                                                         
 28%|██████████████████████████████████████████▊                                                                                                              | 650/2320 [01:14<03:02,  9.17it/s{'loss': 0.2769, 'grad_norm': 1.0401984453201294, 'learning_rate': 7.678899082568808e-05, 'epoch': 2.8017241379310347}                                         | 650/2320 [01:14<03:02,  9.17it/s]
{'loss': 0.2769, 'grad_norm': 1.0401984453201294, 'learning_rate': 7.678899082568808e-05, 'epoch': 2.8}                                                                                          
{'loss': 0.2769, 'grad_norm': 1.0401984453201294, 'learning_rate': 7.678899082568808e-05, 'epoch': 2.8}                                                                                          
 29%|████████████████████████████████████████████▌                                                                                                            | 675/2320 [01:17<03:02,  8.99it/s{'loss': 0.258, 'grad_norm': 1.221661925315857, 'learning_rate': 7.564220183486239e-05, 'epoch': 2.9094827586206895}                                           | 675/2320 [01:17<03:02,  8.99it/s]
{'loss': 0.258, 'grad_norm': 1.221661925315857, 'learning_rate': 7.564220183486239e-05, 'epoch': 2.91}                                                                                           
{'loss': 0.258, 'grad_norm': 1.221661925315857, 'learning_rate': 7.564220183486239e-05, 'epoch': 2.91}                                                                                           
 30%|█████████████████████████████████████████████▉                                                                                                           | 696/2320 [01:19<02:59,  9.05it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.14it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.14it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.1648177057504654, 'eval_accuracy': 0.9568500539374326, 'eval_precision_micro': 0.9568500539374326, 'eval_recall_micro': 0.9568500539374326, 'eval_f1_micro': 0.9568500539374326, 'eval_precision_macro': 0.6050719353754848, 'eval_recall_macro': 0.5774528480550458, 'eval_f1_macro': 0.5740785042976186, 'eval_precision_weighted': 0.9467501748333453, 'eval_recall_weighted': 0.9568500539374326, 'eval_f1_weighted': 0.9500012382009937, 'eval_f1': 0.9500012382009937, 'eval_balanced_accuracy': 0.5774528480550458, 'eval_cohen_kappa': 0.9003699852489542, 'eval_matthews_corrcoef': 0.9005783751539861, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8868, 'eval_samples_per_second': 1045.276, 'eval_steps_per_second': 32.7, 'epoch': 3.0}
{'eval_loss': 0.1648177057504654, 'eval_accuracy': 0.9568500539374326, 'eval_precision_micro': 0.9568500539374326, 'eval_recall_micro': 0.9568500539374326, 'eval_f1_micro': 0.9568500539374326, 'eval_precision_macro': 0.6050719353754848, 'eval_recall_macro': 0.5774528480550458, 'eval_f1_macro': 0.5740785042976186, 'eval_precision_weighted': 0.9467501748333453, 'eval_recall_weighted': 0.9568500539374326, 'eval_f1_weighted': 0.9500012382009937, 'eval_f1': 0.9500012382009937, 'eval_balanced_accuracy': 0.5774528480550458, 'eval_cohen_kappa': 0.9003699852489542, 'eval_matthews_corrcoef': 0.9005783751539861, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8868, 'eval_samples_per_second': 1045.276, 'eval_steps_per_second': 32.7, 'epoch': 3.0}                                 
{'eval_loss': 0.1648177057504654, 'eval_accuracy': 0.9568500539374326, 'eval_precision_micro': 0.9568500539374326, 'eval_recall_micro': 0.9568500539374326, 'eval_f1_micro': 0.9568500539374326, 'eval_precision_macro': 0.6050719353754848, 'eval_recall_macro': 0.5774528480550458, 'eval_f1_macro': 0.5740785042976186, 'eval_precision_weighted': 0.9467501748333453, 'eval_recall_weighted': 0.9568500539374326, 'eval_f1_weighted': 0.9500012382009937, 'eval_f1': 0.9500012382009937, 'eval_balanced_accuracy': 0.5774528480550458, 'eval_cohen_kappa': 0.9003699852489542, 'eval_matthews_corrcoef': 0.9005783751539861, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8868, 'eval_samples_per_second': 1045.276, 'eval_steps_per_second': 32.7, 'epoch': 3.0}                                 
 30%|██████████████████████████████████████████████▏                                                                                                          | 700/2320 [01:21<07:09,  3.77it/s{'loss': 0.2313, 'grad_norm': 2.7589077949523926, 'learning_rate': 7.44954128440367e-05, 'epoch': 3.0172413793103448}                                          | 700/2320 [01:21<07:09,  3.77it/s]
{'loss': 0.2313, 'grad_norm': 2.7589077949523926, 'learning_rate': 7.44954128440367e-05, 'epoch': 3.02}                                                                                          
{'loss': 0.2313, 'grad_norm': 2.7589077949523926, 'learning_rate': 7.44954128440367e-05, 'epoch': 3.02}                                                                                          
 31%|███████████████████████████████████████████████▊                                                                                                         | 725/2320 [01:24<02:55,  9.10it/s{'loss': 0.2468, 'grad_norm': 0.41761499643325806, 'learning_rate': 7.334862385321102e-05, 'epoch': 3.125}                                                     | 725/2320 [01:24<02:55,  9.10it/s]
{'loss': 0.2468, 'grad_norm': 0.41761499643325806, 'learning_rate': 7.334862385321102e-05, 'epoch': 3.12}                                                                                        
{'loss': 0.2468, 'grad_norm': 0.41761499643325806, 'learning_rate': 7.334862385321102e-05, 'epoch': 3.12}                                                                                        
 32%|█████████████████████████████████████████████████▍                                                                                                       | 750/2320 [01:26<02:55,  8.93it/s{'loss': 0.2262, 'grad_norm': 1.268393874168396, 'learning_rate': 7.220183486238531e-05, 'epoch': 3.2327586206896552}                                          | 750/2320 [01:26<02:55,  8.93it/s]
{'loss': 0.2262, 'grad_norm': 1.268393874168396, 'learning_rate': 7.220183486238531e-05, 'epoch': 3.23}                                                                                          
{'loss': 0.2262, 'grad_norm': 1.268393874168396, 'learning_rate': 7.220183486238531e-05, 'epoch': 3.23}                                                                                          
 33%|███████████████████████████████████████████████████                                                                                                      | 775/2320 [01:29<02:51,  9.00it/s{'loss': 0.197, 'grad_norm': 1.182067632675171, 'learning_rate': 7.105504587155963e-05, 'epoch': 3.3405172413793105}                                           | 775/2320 [01:29<02:51,  9.00it/s]
{'loss': 0.197, 'grad_norm': 1.182067632675171, 'learning_rate': 7.105504587155963e-05, 'epoch': 3.34}                                                                                           
{'loss': 0.197, 'grad_norm': 1.182067632675171, 'learning_rate': 7.105504587155963e-05, 'epoch': 3.34}                                                                                           
 34%|████████████████████████████████████████████████████▊                                                                                                    | 800/2320 [01:32<02:44,  9.22it/s{'loss': 0.2654, 'grad_norm': 6.820493698120117, 'learning_rate': 6.990825688073395e-05, 'epoch': 3.4482758620689653}                                          | 800/2320 [01:32<02:44,  9.22it/s]
{'loss': 0.2654, 'grad_norm': 6.820493698120117, 'learning_rate': 6.990825688073395e-05, 'epoch': 3.45}                                                                                          
{'loss': 0.2654, 'grad_norm': 6.820493698120117, 'learning_rate': 6.990825688073395e-05, 'epoch': 3.45}                                                                                          
 36%|██████████████████████████████████████████████████████▍                                                                                                  | 825/2320 [01:35<02:42,  9.18it/s{'loss': 0.2074, 'grad_norm': 0.204848051071167, 'learning_rate': 6.876146788990826e-05, 'epoch': 3.5560344827586206}                                          | 825/2320 [01:35<02:42,  9.18it/s]
{'loss': 0.2074, 'grad_norm': 0.204848051071167, 'learning_rate': 6.876146788990826e-05, 'epoch': 3.56}                                                                                          
{'loss': 0.2074, 'grad_norm': 0.204848051071167, 'learning_rate': 6.876146788990826e-05, 'epoch': 3.56}                                                                                          
 37%|████████████████████████████████████████████████████████                                                                                                 | 850/2320 [01:37<02:41,  9.09it/s{'loss': 0.201, 'grad_norm': 2.297489643096924, 'learning_rate': 6.761467889908257e-05, 'epoch': 3.663793103448276}                                            | 850/2320 [01:37<02:41,  9.09it/s]
{'loss': 0.201, 'grad_norm': 2.297489643096924, 'learning_rate': 6.761467889908257e-05, 'epoch': 3.66}                                                                                           
{'loss': 0.201, 'grad_norm': 2.297489643096924, 'learning_rate': 6.761467889908257e-05, 'epoch': 3.66}                                                                                           
 38%|█████████████████████████████████████████████████████████▋                                                                                               | 875/2320 [01:40<02:35,  9.32it/s{'loss': 0.1818, 'grad_norm': 1.075121521949768, 'learning_rate': 6.646788990825689e-05, 'epoch': 3.771551724137931}                                           | 875/2320 [01:40<02:35,  9.32it/s]
{'loss': 0.1818, 'grad_norm': 1.075121521949768, 'learning_rate': 6.646788990825689e-05, 'epoch': 3.77}                                                                                          
{'loss': 0.1818, 'grad_norm': 1.075121521949768, 'learning_rate': 6.646788990825689e-05, 'epoch': 3.77}                                                                                          
 39%|███████████████████████████████████████████████████████████▎                                                                                             | 900/2320 [01:43<02:36,  9.09it/s{'loss': 0.1828, 'grad_norm': 0.5017428398132324, 'learning_rate': 6.53211009174312e-05, 'epoch': 3.8793103448275863}                                          | 900/2320 [01:43<02:36,  9.09it/s]
{'loss': 0.1828, 'grad_norm': 0.5017428398132324, 'learning_rate': 6.53211009174312e-05, 'epoch': 3.88}                                                                                          
{'loss': 0.1828, 'grad_norm': 0.5017428398132324, 'learning_rate': 6.53211009174312e-05, 'epoch': 3.88}                                                                                          
 40%|█████████████████████████████████████████████████████████████                                                                                            | 925/2320 [01:46<02:32,  9.12it/s{'loss': 0.2175, 'grad_norm': 0.23396004736423492, 'learning_rate': 6.417431192660552e-05, 'epoch': 3.987068965517241}                                         | 925/2320 [01:46<02:32,  9.12it/s]
{'loss': 0.2175, 'grad_norm': 0.23396004736423492, 'learning_rate': 6.417431192660552e-05, 'epoch': 3.99}                                                                                        
{'loss': 0.2175, 'grad_norm': 0.23396004736423492, 'learning_rate': 6.417431192660552e-05, 'epoch': 3.99}                                                                                        
 40%|█████████████████████████████████████████████████████████████▏                                                                                           | 928/2320 [01:46<02:33,  9.07it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.91it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.91it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.1065555289387703, 'eval_accuracy': 0.9751887810140237, 'eval_precision_micro': 0.9751887810140237, 'eval_recall_micro': 0.9751887810140237, 'eval_f1_micro': 0.9751887810140237, 'eval_precision_macro': 0.6895360146954534, 'eval_recall_macro': 0.7007649153568829, 'eval_f1_macro': 0.6939019525499095, 'eval_precision_weighted': 0.9699941577343993, 'eval_recall_weighted': 0.9751887810140237, 'eval_f1_weighted': 0.9723211867328188, 'eval_f1': 0.9723211867328188, 'eval_balanced_accuracy': 0.7007649153568829, 'eval_cohen_kappa': 0.9431127499372989, 'eval_matthews_corrcoef': 0.9431996682977167, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8918, 'eval_samples_per_second': 1039.495, 'eval_steps_per_second': 32.519, 'epoch': 4.0}
{'eval_loss': 0.1065555289387703, 'eval_accuracy': 0.9751887810140237, 'eval_precision_micro': 0.9751887810140237, 'eval_recall_micro': 0.9751887810140237, 'eval_f1_micro': 0.9751887810140237, 'eval_precision_macro': 0.6895360146954534, 'eval_recall_macro': 0.7007649153568829, 'eval_f1_macro': 0.6939019525499095, 'eval_precision_weighted': 0.9699941577343993, 'eval_recall_weighted': 0.9751887810140237, 'eval_f1_weighted': 0.9723211867328188, 'eval_f1': 0.9723211867328188, 'eval_balanced_accuracy': 0.7007649153568829, 'eval_cohen_kappa': 0.9431127499372989, 'eval_matthews_corrcoef': 0.9431996682977167, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8918, 'eval_samples_per_second': 1039.495, 'eval_steps_per_second': 32.519, 'epoch': 4.0}                               
{'eval_loss': 0.1065555289387703, 'eval_accuracy': 0.9751887810140237, 'eval_precision_micro': 0.9751887810140237, 'eval_recall_micro': 0.9751887810140237, 'eval_f1_micro': 0.9751887810140237, 'eval_precision_macro': 0.6895360146954534, 'eval_recall_macro': 0.7007649153568829, 'eval_f1_macro': 0.6939019525499095, 'eval_precision_weighted': 0.9699941577343993, 'eval_recall_weighted': 0.9751887810140237, 'eval_f1_weighted': 0.9723211867328188, 'eval_f1': 0.9723211867328188, 'eval_balanced_accuracy': 0.7007649153568829, 'eval_cohen_kappa': 0.9431127499372989, 'eval_matthews_corrcoef': 0.9431996682977167, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8918, 'eval_samples_per_second': 1039.495, 'eval_steps_per_second': 32.519, 'epoch': 4.0}                               
 41%|██████████████████████████████████████████████████████████████▋                                                                                          | 950/2320 [01:50<02:30,  9.09it/s{'loss': 0.1677, 'grad_norm': 2.538706064224243, 'learning_rate': 6.302752293577982e-05, 'epoch': 4.094827586206897}                                           | 950/2320 [01:50<02:30,  9.08it/s]
{'loss': 0.1677, 'grad_norm': 2.538706064224243, 'learning_rate': 6.302752293577982e-05, 'epoch': 4.09}                                                                                          
{'loss': 0.1677, 'grad_norm': 2.538706064224243, 'learning_rate': 6.302752293577982e-05, 'epoch': 4.09}                                                                                          
 42%|████████████████████████████████████████████████████████████████▎                                                                                        | 975/2320 [01:52<02:25,  9.25it/s{'loss': 0.1637, 'grad_norm': 1.1263186931610107, 'learning_rate': 6.188073394495413e-05, 'epoch': 4.202586206896552}                                          | 975/2320 [01:52<02:25,  9.25it/s]
{'loss': 0.1637, 'grad_norm': 1.1263186931610107, 'learning_rate': 6.188073394495413e-05, 'epoch': 4.2}                                                                                          
{'loss': 0.1637, 'grad_norm': 1.1263186931610107, 'learning_rate': 6.188073394495413e-05, 'epoch': 4.2}                                                                                          
 43%|█████████████████████████████████████████████████████████████████▌                                                                                      | 1000/2320 [01:55<02:25,  9.05it/s{'loss': 0.1593, 'grad_norm': 0.06729990243911743, 'learning_rate': 6.0733944954128444e-05, 'epoch': 4.310344827586207}                                       | 1000/2320 [01:55<02:25,  9.05it/s]
{'loss': 0.1593, 'grad_norm': 0.06729990243911743, 'learning_rate': 6.0733944954128444e-05, 'epoch': 4.31}                                                                                       
{'loss': 0.1593, 'grad_norm': 0.06729990243911743, 'learning_rate': 6.0733944954128444e-05, 'epoch': 4.31}                                                                                       
 44%|███████████████████████████████████████████████████████████████████▏                                                                                    | 1025/2320 [01:58<02:23,  9.02it/s{'loss': 0.1874, 'grad_norm': 0.6281367540359497, 'learning_rate': 5.9587155963302756e-05, 'epoch': 4.418103448275862}                                        | 1025/2320 [01:58<02:23,  9.02it/s]
{'loss': 0.1874, 'grad_norm': 0.6281367540359497, 'learning_rate': 5.9587155963302756e-05, 'epoch': 4.42}                                                                                        
{'loss': 0.1874, 'grad_norm': 0.6281367540359497, 'learning_rate': 5.9587155963302756e-05, 'epoch': 4.42}                                                                                        
 45%|████████████████████████████████████████████████████████████████████▊                                                                                   | 1050/2320 [02:01<02:17,  9.27it/s{'loss': 0.1409, 'grad_norm': 0.1772703230381012, 'learning_rate': 5.844036697247707e-05, 'epoch': 4.525862068965517}                                         | 1050/2320 [02:01<02:17,  9.27it/s]
{'loss': 0.1409, 'grad_norm': 0.1772703230381012, 'learning_rate': 5.844036697247707e-05, 'epoch': 4.53}                                                                                         
{'loss': 0.1409, 'grad_norm': 0.1772703230381012, 'learning_rate': 5.844036697247707e-05, 'epoch': 4.53}                                                                                         
 46%|██████████████████████████████████████████████████████████████████████▍                                                                                 | 1075/2320 [02:03<01:14, 16.70it/s{'loss': 0.1274, 'grad_norm': 0.6428028345108032, 'learning_rate': 5.729357798165138e-05, 'epoch': 4.633620689655173}                                         | 1075/2320 [02:03<01:14, 16.70it/s]
{'loss': 0.1274, 'grad_norm': 0.6428028345108032, 'learning_rate': 5.729357798165138e-05, 'epoch': 4.63}                                                                                         
{'loss': 0.1274, 'grad_norm': 0.6428028345108032, 'learning_rate': 5.729357798165138e-05, 'epoch': 4.63}                                                                                         
 47%|████████████████████████████████████████████████████████████████████████                                                                                | 1100/2320 [02:04<00:53, 22.76it/s{'loss': 0.1087, 'grad_norm': 0.6877471804618835, 'learning_rate': 5.61467889908257e-05, 'epoch': 4.741379310344827}                                          | 1100/2320 [02:04<00:53, 22.76it/s]
{'loss': 0.1087, 'grad_norm': 0.6877471804618835, 'learning_rate': 5.61467889908257e-05, 'epoch': 4.74}                                                                                          
{'loss': 0.1087, 'grad_norm': 0.6877471804618835, 'learning_rate': 5.61467889908257e-05, 'epoch': 4.74}                                                                                          
 48%|█████████████████████████████████████████████████████████████████████████▋                                                                              | 1124/2320 [02:05<00:51, 23.24it/s{'loss': 0.1671, 'grad_norm': 0.0964585691690445, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.849137931034483}                                         | 1124/2320 [02:05<00:51, 23.24it/s]
{'loss': 0.1671, 'grad_norm': 0.0964585691690445, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.85}                                                                                         
{'loss': 0.1671, 'grad_norm': 0.0964585691690445, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.85}                                                                                         
 49%|███████████████████████████████████████████████████████████████████████████▏                                                                            | 1148/2320 [02:06<00:50, 23.19it/s{'loss': 0.135, 'grad_norm': 0.1957368403673172, 'learning_rate': 5.385321100917431e-05, 'epoch': 4.956896551724138}                                          | 1148/2320 [02:06<00:50, 23.19it/s]
{'loss': 0.135, 'grad_norm': 0.1957368403673172, 'learning_rate': 5.385321100917431e-05, 'epoch': 4.96}                                                                                          
{'loss': 0.135, 'grad_norm': 0.1957368403673172, 'learning_rate': 5.385321100917431e-05, 'epoch': 4.96}                                                                                          
 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 1160/2320 [02:06<00:51, 22.65it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.63it/s]
  _warn_prf(average, modifier, msg_start, len(result))████████████████████████████████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.63it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.0896904245018959, 'eval_accuracy': 0.9838187702265372, 'eval_precision_micro': 0.9838187702265372, 'eval_recall_micro': 0.9838187702265372, 'eval_f1_micro': 0.9838187702265372, 'eval_precision_macro': 0.9072169491921174, 'eval_recall_macro': 0.8725110134616767, 'eval_f1_macro': 0.8857269990204085, 'eval_precision_weighted': 0.9832668523117835, 'eval_recall_weighted': 0.9838187702265372, 'eval_f1_weighted': 0.9832014984628151, 'eval_f1': 0.9832014984628151, 'eval_balanced_accuracy': 0.8725110134616767, 'eval_cohen_kappa': 0.9627807503807578, 'eval_matthews_corrcoef': 0.9628114638043077, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.703, 'eval_samples_per_second': 1318.61, 'eval_steps_per_second': 41.251, 'epoch': 5.0}
{'eval_loss': 0.0896904245018959, 'eval_accuracy': 0.9838187702265372, 'eval_precision_micro': 0.9838187702265372, 'eval_recall_micro': 0.9838187702265372, 'eval_f1_micro': 0.9838187702265372, 'eval_precision_macro': 0.9072169491921174, 'eval_recall_macro': 0.8725110134616767, 'eval_f1_macro': 0.8857269990204085, 'eval_precision_weighted': 0.9832668523117835, 'eval_recall_weighted': 0.9838187702265372, 'eval_f1_weighted': 0.9832014984628151, 'eval_f1': 0.9832014984628151, 'eval_balanced_accuracy': 0.8725110134616767, 'eval_cohen_kappa': 0.9627807503807578, 'eval_matthews_corrcoef': 0.9628114638043077, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.703, 'eval_samples_per_second': 1318.61, 'eval_steps_per_second': 41.251, 'epoch': 5.0}                                 
{'eval_loss': 0.0896904245018959, 'eval_accuracy': 0.9838187702265372, 'eval_precision_micro': 0.9838187702265372, 'eval_recall_micro': 0.9838187702265372, 'eval_f1_micro': 0.9838187702265372, 'eval_precision_macro': 0.9072169491921174, 'eval_recall_macro': 0.8725110134616767, 'eval_f1_macro': 0.8857269990204085, 'eval_precision_weighted': 0.9832668523117835, 'eval_recall_weighted': 0.9838187702265372, 'eval_f1_weighted': 0.9832014984628151, 'eval_f1': 0.9832014984628151, 'eval_balanced_accuracy': 0.8725110134616767, 'eval_cohen_kappa': 0.9627807503807578, 'eval_matthews_corrcoef': 0.9628114638043077, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.703, 'eval_samples_per_second': 1318.61, 'eval_steps_per_second': 41.251, 'epoch': 5.0}                                 
 51%|████████████████████████████████████████████████████████████████████████████▉                                                                           | 1174/2320 [02:08<01:25, 13.46it/s{'loss': 0.1792, 'grad_norm': 1.0416868925094604, 'learning_rate': 5.2706422018348625e-05, 'epoch': 5.064655172413793}                                        | 1174/2320 [02:08<01:25, 13.46it/s]
{'loss': 0.1792, 'grad_norm': 1.0416868925094604, 'learning_rate': 5.2706422018348625e-05, 'epoch': 5.06}                                                                                        
{'loss': 0.1792, 'grad_norm': 1.0416868925094604, 'learning_rate': 5.2706422018348625e-05, 'epoch': 5.06}                                                                                        
 52%|██████████████████████████████████████████████████████████████████████████████▍                                                                         | 1198/2320 [02:09<00:49, 22.72it/s{'loss': 0.1442, 'grad_norm': 1.6728637218475342, 'learning_rate': 5.155963302752294e-05, 'epoch': 5.172413793103448}                                         | 1198/2320 [02:09<00:49, 22.72it/s]
{'loss': 0.1442, 'grad_norm': 1.6728637218475342, 'learning_rate': 5.155963302752294e-05, 'epoch': 5.17}                                                                                         
{'loss': 0.1442, 'grad_norm': 1.6728637218475342, 'learning_rate': 5.155963302752294e-05, 'epoch': 5.17}                                                                                         
 53%|████████████████████████████████████████████████████████████████████████████████▎                                                                       | 1225/2320 [02:10<00:48, 22.76it/s{'loss': 0.1133, 'grad_norm': 4.056453704833984, 'learning_rate': 5.041284403669725e-05, 'epoch': 5.280172413793103}                                          | 1225/2320 [02:10<00:48, 22.76it/s]
{'loss': 0.1133, 'grad_norm': 4.056453704833984, 'learning_rate': 5.041284403669725e-05, 'epoch': 5.28}                                                                                          
{'loss': 0.1133, 'grad_norm': 4.056453704833984, 'learning_rate': 5.041284403669725e-05, 'epoch': 5.28}                                                                                          
 54%|█████████████████████████████████████████████████████████████████████████████████▊                                                                      | 1249/2320 [02:12<00:46, 23.03it/s{'loss': 0.1395, 'grad_norm': 1.6277704238891602, 'learning_rate': 4.926605504587156e-05, 'epoch': 5.387931034482759}                                         | 1249/2320 [02:12<00:46, 23.03it/s]
{'loss': 0.1395, 'grad_norm': 1.6277704238891602, 'learning_rate': 4.926605504587156e-05, 'epoch': 5.39}                                                                                         
{'loss': 0.1395, 'grad_norm': 1.6277704238891602, 'learning_rate': 4.926605504587156e-05, 'epoch': 5.39}                                                                                         
 55%|███████████████████████████████████████████████████████████████████████████████████▍                                                                    | 1273/2320 [02:13<00:46, 22.72it/s{'loss': 0.0939, 'grad_norm': 1.3581132888793945, 'learning_rate': 4.811926605504588e-05, 'epoch': 5.495689655172414}                                         | 1273/2320 [02:13<00:46, 22.72it/s]
{'loss': 0.0939, 'grad_norm': 1.3581132888793945, 'learning_rate': 4.811926605504588e-05, 'epoch': 5.5}                                                                                          
{'loss': 0.0939, 'grad_norm': 1.3581132888793945, 'learning_rate': 4.811926605504588e-05, 'epoch': 5.5}                                                                                          
 56%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 1300/2320 [02:14<00:43, 23.20it/s{'loss': 0.1369, 'grad_norm': 0.2333616018295288, 'learning_rate': 4.697247706422019e-05, 'epoch': 5.603448275862069}                                         | 1300/2320 [02:14<00:43, 23.19it/s]
{'loss': 0.1369, 'grad_norm': 0.2333616018295288, 'learning_rate': 4.697247706422019e-05, 'epoch': 5.6}                                                                                          
{'loss': 0.1369, 'grad_norm': 0.2333616018295288, 'learning_rate': 4.697247706422019e-05, 'epoch': 5.6}                                                                                          
 57%|██████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 1324/2320 [02:15<00:43, 23.09it/s{'loss': 0.1147, 'grad_norm': 2.8028225898742676, 'learning_rate': 4.5825688073394495e-05, 'epoch': 5.711206896551724}                                        | 1324/2320 [02:15<00:43, 23.09it/s]
{'loss': 0.1147, 'grad_norm': 2.8028225898742676, 'learning_rate': 4.5825688073394495e-05, 'epoch': 5.71}                                                                                        
{'loss': 0.1147, 'grad_norm': 2.8028225898742676, 'learning_rate': 4.5825688073394495e-05, 'epoch': 5.71}                                                                                        
 58%|████████████████████████████████████████████████████████████████████████████████████████▎                                                               | 1348/2320 [02:16<00:41, 23.54it/s{'loss': 0.1086, 'grad_norm': 0.24779391288757324, 'learning_rate': 4.4678899082568806e-05, 'epoch': 5.818965517241379}                                       | 1348/2320 [02:16<00:41, 23.54it/s]
{'loss': 0.1086, 'grad_norm': 0.24779391288757324, 'learning_rate': 4.4678899082568806e-05, 'epoch': 5.82}                                                                                       
{'loss': 0.1086, 'grad_norm': 0.24779391288757324, 'learning_rate': 4.4678899082568806e-05, 'epoch': 5.82}                                                                                       
 59%|██████████████████████████████████████████████████████████████████████████████████████████                                                              | 1375/2320 [02:17<00:40, 23.11it/s{'loss': 0.1441, 'grad_norm': 2.255124807357788, 'learning_rate': 4.3532110091743125e-05, 'epoch': 5.926724137931035}                                         | 1375/2320 [02:17<00:40, 23.11it/s]
{'loss': 0.1441, 'grad_norm': 2.255124807357788, 'learning_rate': 4.3532110091743125e-05, 'epoch': 5.93}                                                                                         
{'loss': 0.1441, 'grad_norm': 2.255124807357788, 'learning_rate': 4.3532110091743125e-05, 'epoch': 5.93}                                                                                         
 60%|███████████████████████████████████████████████████████████████████████████████████████████                                                             | 1390/2320 [02:18<00:40, 23.14it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.84it/s]
  _warn_prf(average, modifier, msg_start, len(result))████████████████████████████████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.84it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08431972563266754, 'eval_accuracy': 0.9848975188781014, 'eval_precision_micro': 0.9848975188781014, 'eval_recall_micro': 0.9848975188781014, 'eval_f1_micro': 0.9848975188781014, 'eval_precision_macro': 0.9144130656616783, 'eval_recall_macro': 0.8823842018674738, 'eval_f1_macro': 0.8947715049675609, 'eval_precision_weighted': 0.9842851021821529, 'eval_recall_weighted': 0.9848975188781014, 'eval_f1_weighted': 0.9842747157947057, 'eval_f1': 0.9842747157947057, 'eval_balanced_accuracy': 0.8823842018674738, 'eval_cohen_kappa': 0.9651957971068905, 'eval_matthews_corrcoef': 0.9652199169295801, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6973, 'eval_samples_per_second': 1329.322, 'eval_steps_per_second': 41.586, 'epoch': 6.0}
{'eval_loss': 0.08431972563266754, 'eval_accuracy': 0.9848975188781014, 'eval_precision_micro': 0.9848975188781014, 'eval_recall_micro': 0.9848975188781014, 'eval_f1_micro': 0.9848975188781014, 'eval_precision_macro': 0.9144130656616783, 'eval_recall_macro': 0.8823842018674738, 'eval_f1_macro': 0.8947715049675609, 'eval_precision_weighted': 0.9842851021821529, 'eval_recall_weighted': 0.9848975188781014, 'eval_f1_weighted': 0.9842747157947057, 'eval_f1': 0.9842747157947057, 'eval_balanced_accuracy': 0.8823842018674738, 'eval_cohen_kappa': 0.9651957971068905, 'eval_matthews_corrcoef': 0.9652199169295801, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6973, 'eval_samples_per_second': 1329.322, 'eval_steps_per_second': 41.586, 'epoch': 6.0}                              
{'eval_loss': 0.08431972563266754, 'eval_accuracy': 0.9848975188781014, 'eval_precision_micro': 0.9848975188781014, 'eval_recall_micro': 0.9848975188781014, 'eval_f1_micro': 0.9848975188781014, 'eval_precision_macro': 0.9144130656616783, 'eval_recall_macro': 0.8823842018674738, 'eval_f1_macro': 0.8947715049675609, 'eval_precision_weighted': 0.9842851021821529, 'eval_recall_weighted': 0.9848975188781014, 'eval_f1_weighted': 0.9842747157947057, 'eval_f1': 0.9842747157947057, 'eval_balanced_accuracy': 0.8823842018674738, 'eval_cohen_kappa': 0.9651957971068905, 'eval_matthews_corrcoef': 0.9652199169295801, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6973, 'eval_samples_per_second': 1329.322, 'eval_steps_per_second': 41.586, 'epoch': 6.0}                              
 60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 1399/2320 [02:19<01:34,  9.77it/s{'loss': 0.1114, 'grad_norm': 0.25228288769721985, 'learning_rate': 4.2385321100917436e-05, 'epoch': 6.0344827586206895}                                      | 1399/2320 [02:19<01:34,  9.77it/s]
{'loss': 0.1114, 'grad_norm': 0.25228288769721985, 'learning_rate': 4.2385321100917436e-05, 'epoch': 6.03}                                                                                       
{'loss': 0.1114, 'grad_norm': 0.25228288769721985, 'learning_rate': 4.2385321100917436e-05, 'epoch': 6.03}                                                                                       
 61%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 1424/2320 [02:20<00:41, 21.78it/s{'loss': 0.1006, 'grad_norm': 2.8449363708496094, 'learning_rate': 4.123853211009174e-05, 'epoch': 6.142241379310345}                                         | 1424/2320 [02:20<00:41, 21.78it/s]
{'loss': 0.1006, 'grad_norm': 2.8449363708496094, 'learning_rate': 4.123853211009174e-05, 'epoch': 6.14}                                                                                         
{'loss': 0.1006, 'grad_norm': 2.8449363708496094, 'learning_rate': 4.123853211009174e-05, 'epoch': 6.14}                                                                                         
 62%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                         | 1448/2320 [02:22<00:38, 22.49it/s{'loss': 0.2109, 'grad_norm': 2.4204556941986084, 'learning_rate': 4.009174311926606e-05, 'epoch': 6.25}                                                      | 1448/2320 [02:22<00:38, 22.49it/s]
{'loss': 0.2109, 'grad_norm': 2.4204556941986084, 'learning_rate': 4.009174311926606e-05, 'epoch': 6.25}                                                                                         
{'loss': 0.2109, 'grad_norm': 2.4204556941986084, 'learning_rate': 4.009174311926606e-05, 'epoch': 6.25}                                                                                         
 64%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 1475/2320 [02:23<00:36, 23.28it/s{'loss': 0.1409, 'grad_norm': 1.2019054889678955, 'learning_rate': 3.894495412844037e-05, 'epoch': 6.357758620689655}                                         | 1475/2320 [02:23<00:36, 23.29it/s]
{'loss': 0.1409, 'grad_norm': 1.2019054889678955, 'learning_rate': 3.894495412844037e-05, 'epoch': 6.36}                                                                                         
{'loss': 0.1409, 'grad_norm': 1.2019054889678955, 'learning_rate': 3.894495412844037e-05, 'epoch': 6.36}                                                                                         
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                     | 1499/2320 [02:24<00:34, 23.78it/s{'loss': 0.079, 'grad_norm': 2.1383373737335205, 'learning_rate': 3.779816513761468e-05, 'epoch': 6.4655172413793105}                                         | 1499/2320 [02:24<00:34, 23.78it/s]
{'loss': 0.079, 'grad_norm': 2.1383373737335205, 'learning_rate': 3.779816513761468e-05, 'epoch': 6.47}                                                                                          
{'loss': 0.079, 'grad_norm': 2.1383373737335205, 'learning_rate': 3.779816513761468e-05, 'epoch': 6.47}                                                                                          
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                                    | 1523/2320 [02:25<00:34, 23.19it/s{'loss': 0.0957, 'grad_norm': 0.26314014196395874, 'learning_rate': 3.665137614678899e-05, 'epoch': 6.573275862068965}                                        | 1523/2320 [02:25<00:34, 23.19it/s]
{'loss': 0.0957, 'grad_norm': 0.26314014196395874, 'learning_rate': 3.665137614678899e-05, 'epoch': 6.57}                                                                                        
{'loss': 0.0957, 'grad_norm': 0.26314014196395874, 'learning_rate': 3.665137614678899e-05, 'epoch': 6.57}                                                                                        
 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 1550/2320 [02:26<00:32, 23.42it/s{'loss': 0.0487, 'grad_norm': 0.5215297937393188, 'learning_rate': 3.5504587155963306e-05, 'epoch': 6.681034482758621}                                        | 1550/2320 [02:26<00:32, 23.42it/s]
{'loss': 0.0487, 'grad_norm': 0.5215297937393188, 'learning_rate': 3.5504587155963306e-05, 'epoch': 6.68}                                                                                        
{'loss': 0.0487, 'grad_norm': 0.5215297937393188, 'learning_rate': 3.5504587155963306e-05, 'epoch': 6.68}                                                                                        
 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████                                                 | 1574/2320 [02:27<00:32, 23.29it/s{'loss': 0.0996, 'grad_norm': 0.9764324426651001, 'learning_rate': 3.435779816513762e-05, 'epoch': 6.788793103448276}                                         | 1574/2320 [02:27<00:32, 23.29it/s]
{'loss': 0.0996, 'grad_norm': 0.9764324426651001, 'learning_rate': 3.435779816513762e-05, 'epoch': 6.79}                                                                                         
{'loss': 0.0996, 'grad_norm': 0.9764324426651001, 'learning_rate': 3.435779816513762e-05, 'epoch': 6.79}                                                                                         
 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 1598/2320 [02:28<00:31, 23.22it/s{'loss': 0.0803, 'grad_norm': 4.977000713348389, 'learning_rate': 3.321100917431193e-05, 'epoch': 6.896551724137931}                                          | 1598/2320 [02:28<00:31, 23.22it/s]
{'loss': 0.0803, 'grad_norm': 4.977000713348389, 'learning_rate': 3.321100917431193e-05, 'epoch': 6.9}                                                                                           
{'loss': 0.0803, 'grad_norm': 4.977000713348389, 'learning_rate': 3.321100917431193e-05, 'epoch': 6.9}                                                                                           
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                             | 1622/2320 [02:29<00:30, 23.02it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.61it/s]
  _warn_prf(average, modifier, msg_start, len(result))████████████████████████████████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.61it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08185289800167084, 'eval_accuracy': 0.9870550161812298, 'eval_precision_micro': 0.9870550161812298, 'eval_recall_micro': 0.9870550161812298, 'eval_f1_micro': 0.9870550161812298, 'eval_precision_macro': 0.9163420121314129, 'eval_recall_macro': 0.8843130168717364, 'eval_f1_macro': 0.8967003856568925, 'eval_precision_weighted': 0.9864441651582153, 'eval_recall_weighted': 0.9870550161812298, 'eval_f1_weighted': 0.9864329953666169, 'eval_f1': 0.9864329953666169, 'eval_balanced_accuracy': 0.8843130168717364, 'eval_cohen_kappa': 0.9701678260916204, 'eval_matthews_corrcoef': 0.9701920701631493, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6976, 'eval_samples_per_second': 1328.798, 'eval_steps_per_second': 41.57, 'epoch': 7.0}
{'eval_loss': 0.08185289800167084, 'eval_accuracy': 0.9870550161812298, 'eval_precision_micro': 0.9870550161812298, 'eval_recall_micro': 0.9870550161812298, 'eval_f1_micro': 0.9870550161812298, 'eval_precision_macro': 0.9163420121314129, 'eval_recall_macro': 0.8843130168717364, 'eval_f1_macro': 0.8967003856568925, 'eval_precision_weighted': 0.9864441651582153, 'eval_recall_weighted': 0.9870550161812298, 'eval_f1_weighted': 0.9864329953666169, 'eval_f1': 0.9864329953666169, 'eval_balanced_accuracy': 0.8843130168717364, 'eval_cohen_kappa': 0.9701678260916204, 'eval_matthews_corrcoef': 0.9701920701631493, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6976, 'eval_samples_per_second': 1328.798, 'eval_steps_per_second': 41.57, 'epoch': 7.0}                               
{'eval_loss': 0.08185289800167084, 'eval_accuracy': 0.9870550161812298, 'eval_precision_micro': 0.9870550161812298, 'eval_recall_micro': 0.9870550161812298, 'eval_f1_micro': 0.9870550161812298, 'eval_precision_macro': 0.9163420121314129, 'eval_recall_macro': 0.8843130168717364, 'eval_f1_macro': 0.8967003856568925, 'eval_precision_weighted': 0.9864441651582153, 'eval_recall_weighted': 0.9870550161812298, 'eval_f1_weighted': 0.9864329953666169, 'eval_f1': 0.9864329953666169, 'eval_balanced_accuracy': 0.8843130168717364, 'eval_cohen_kappa': 0.9701678260916204, 'eval_matthews_corrcoef': 0.9701920701631493, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6976, 'eval_samples_per_second': 1328.798, 'eval_steps_per_second': 41.57, 'epoch': 7.0}                               
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1625/2320 [02:30<01:52,  6.19it/s{'loss': 0.1017, 'grad_norm': 1.335604190826416, 'learning_rate': 3.206422018348624e-05, 'epoch': 7.004310344827586}                                          | 1625/2320 [02:30<01:52,  6.19it/s]
{'loss': 0.1017, 'grad_norm': 1.335604190826416, 'learning_rate': 3.206422018348624e-05, 'epoch': 7.0}                                                                                           
{'loss': 0.1017, 'grad_norm': 1.335604190826416, 'learning_rate': 3.206422018348624e-05, 'epoch': 7.0}                                                                                           
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 1650/2320 [02:32<00:33, 20.12it/s{'loss': 0.1271, 'grad_norm': 1.1391029357910156, 'learning_rate': 3.091743119266055e-05, 'epoch': 7.112068965517241}                                         | 1650/2320 [02:32<00:33, 20.12it/s]
{'loss': 0.1271, 'grad_norm': 1.1391029357910156, 'learning_rate': 3.091743119266055e-05, 'epoch': 7.11}                                                                                         
{'loss': 0.1271, 'grad_norm': 1.1391029357910156, 'learning_rate': 3.091743119266055e-05, 'epoch': 7.11}                                                                                         
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 1674/2320 [02:33<00:28, 23.07it/s{'loss': 0.0587, 'grad_norm': 0.09524192661046982, 'learning_rate': 2.9770642201834864e-05, 'epoch': 7.219827586206897}                                       | 1674/2320 [02:33<00:28, 23.07it/s]
{'loss': 0.0587, 'grad_norm': 0.09524192661046982, 'learning_rate': 2.9770642201834864e-05, 'epoch': 7.22}                                                                                       
{'loss': 0.0587, 'grad_norm': 0.09524192661046982, 'learning_rate': 2.9770642201834864e-05, 'epoch': 7.22}                                                                                       
 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 1698/2320 [02:34<00:26, 23.13it/s{'loss': 0.1009, 'grad_norm': 0.3439198434352875, 'learning_rate': 2.862385321100918e-05, 'epoch': 7.327586206896552}▏                                        | 1698/2320 [02:34<00:26, 23.13it/s]
{'loss': 0.1009, 'grad_norm': 0.3439198434352875, 'learning_rate': 2.862385321100918e-05, 'epoch': 7.33}                                                                                         
{'loss': 0.1009, 'grad_norm': 0.3439198434352875, 'learning_rate': 2.862385321100918e-05, 'epoch': 7.33}                                                                                         
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 1725/2320 [02:35<00:25, 23.00it/s{'loss': 0.1011, 'grad_norm': 0.9862363934516907, 'learning_rate': 2.7477064220183484e-05, 'epoch': 7.435344827586207}█                                       | 1725/2320 [02:35<00:25, 23.00it/s]
{'loss': 0.1011, 'grad_norm': 0.9862363934516907, 'learning_rate': 2.7477064220183484e-05, 'epoch': 7.44}                                                                                        
{'loss': 0.1011, 'grad_norm': 0.9862363934516907, 'learning_rate': 2.7477064220183484e-05, 'epoch': 7.44}                                                                                        
 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 1749/2320 [02:36<00:24, 23.47it/s{'loss': 0.0757, 'grad_norm': 0.9902796149253845, 'learning_rate': 2.63302752293578e-05, 'epoch': 7.543103448275862}████▌                                     | 1749/2320 [02:36<00:24, 23.47it/s]
{'loss': 0.0757, 'grad_norm': 0.9902796149253845, 'learning_rate': 2.63302752293578e-05, 'epoch': 7.54}                                                                                          
{'loss': 0.0757, 'grad_norm': 0.9902796149253845, 'learning_rate': 2.63302752293578e-05, 'epoch': 7.54}                                                                                          
 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 1773/2320 [02:37<00:23, 23.07it/s{'loss': 0.1144, 'grad_norm': 0.8392692804336548, 'learning_rate': 2.518348623853211e-05, 'epoch': 7.650862068965517}█████▏                                   | 1773/2320 [02:37<00:23, 23.07it/s]
{'loss': 0.1144, 'grad_norm': 0.8392692804336548, 'learning_rate': 2.518348623853211e-05, 'epoch': 7.65}                                                                                         
{'loss': 0.1144, 'grad_norm': 0.8392692804336548, 'learning_rate': 2.518348623853211e-05, 'epoch': 7.65}                                                                                         
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 1800/2320 [02:38<00:22, 23.09it/s{'loss': 0.0786, 'grad_norm': 0.8049864172935486, 'learning_rate': 2.4036697247706425e-05, 'epoch': 7.758620689655173}█████▉                                  | 1800/2320 [02:38<00:22, 23.09it/s]
{'loss': 0.0786, 'grad_norm': 0.8049864172935486, 'learning_rate': 2.4036697247706425e-05, 'epoch': 7.76}                                                                                        
{'loss': 0.0786, 'grad_norm': 0.8049864172935486, 'learning_rate': 2.4036697247706425e-05, 'epoch': 7.76}                                                                                        
 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                | 1824/2320 [02:39<00:21, 23.34it/s{'loss': 0.1443, 'grad_norm': 0.36014506220817566, 'learning_rate': 2.2889908256880733e-05, 'epoch': 7.866379310344827}██████▌                                | 1824/2320 [02:39<00:21, 23.34it/s]
{'loss': 0.1443, 'grad_norm': 0.36014506220817566, 'learning_rate': 2.2889908256880733e-05, 'epoch': 7.87}                                                                                       
{'loss': 0.1443, 'grad_norm': 0.36014506220817566, 'learning_rate': 2.2889908256880733e-05, 'epoch': 7.87}                                                                                       
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                               | 1848/2320 [02:40<00:20, 23.30it/s{'loss': 0.1223, 'grad_norm': 1.437406063079834, 'learning_rate': 2.1743119266055048e-05, 'epoch': 7.974137931034483}██████████                               | 1848/2320 [02:40<00:20, 23.30it/s]
{'loss': 0.1223, 'grad_norm': 1.437406063079834, 'learning_rate': 2.1743119266055048e-05, 'epoch': 7.97}                                                                                         
{'loss': 0.1223, 'grad_norm': 1.437406063079834, 'learning_rate': 2.1743119266055048e-05, 'epoch': 7.97}                                                                                         
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 1854/2320 [02:40<00:19, 23.50it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.61it/s]
  _warn_prf(average, modifier, msg_start, len(result))████████████████████████████████████████████████████████████████████████████████                           | 24/29 [00:00<00:00, 49.61it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08002770692110062, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.920462891252292, 'eval_recall_macro': 0.8999380168717364, 'eval_f1_macro': 0.907765709222216, 'eval_precision_weighted': 0.9873688068595561, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875288670126504, 'eval_f1': 0.9875288670126504, 'eval_balanced_accuracy': 0.8999380168717364, 'eval_cohen_kappa': 0.9726545005966827, 'eval_matthews_corrcoef': 0.9726762321072991, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7043, 'eval_samples_per_second': 1316.281, 'eval_steps_per_second': 41.178, 'epoch': 8.0}
{'eval_loss': 0.08002770692110062, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.920462891252292, 'eval_recall_macro': 0.8999380168717364, 'eval_f1_macro': 0.907765709222216, 'eval_precision_weighted': 0.9873688068595561, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875288670126504, 'eval_f1': 0.9875288670126504, 'eval_balanced_accuracy': 0.8999380168717364, 'eval_cohen_kappa': 0.9726545005966827, 'eval_matthews_corrcoef': 0.9726762321072991, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7043, 'eval_samples_per_second': 1316.281, 'eval_steps_per_second': 41.178, 'epoch': 8.0}                                
{'eval_loss': 0.08002770692110062, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.920462891252292, 'eval_recall_macro': 0.8999380168717364, 'eval_f1_macro': 0.907765709222216, 'eval_precision_weighted': 0.9873688068595561, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875288670126504, 'eval_f1': 0.9875288670126504, 'eval_balanced_accuracy': 0.8999380168717364, 'eval_cohen_kappa': 0.9726545005966827, 'eval_matthews_corrcoef': 0.9726762321072991, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7043, 'eval_samples_per_second': 1316.281, 'eval_steps_per_second': 41.178, 'epoch': 8.0}                                
 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 1875/2320 [02:43<00:24, 17.82it/s{'loss': 0.071, 'grad_norm': 0.17045986652374268, 'learning_rate': 2.059633027522936e-05, 'epoch': 8.081896551724139}███████████▊                             | 1875/2320 [02:43<00:24, 17.83it/s]
{'loss': 0.071, 'grad_norm': 0.17045986652374268, 'learning_rate': 2.059633027522936e-05, 'epoch': 8.08}                                                                                         
{'loss': 0.071, 'grad_norm': 0.17045986652374268, 'learning_rate': 2.059633027522936e-05, 'epoch': 8.08}                                                                                         
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 1899/2320 [02:44<00:18, 22.42it/s{'loss': 0.1149, 'grad_norm': 2.345311164855957, 'learning_rate': 1.944954128440367e-05, 'epoch': 8.189655172413794}██████████████▍                           | 1899/2320 [02:44<00:18, 22.42it/s]
{'loss': 0.1149, 'grad_norm': 2.345311164855957, 'learning_rate': 1.944954128440367e-05, 'epoch': 8.19}                                                                                          
{'loss': 0.1149, 'grad_norm': 2.345311164855957, 'learning_rate': 1.944954128440367e-05, 'epoch': 8.19}                                                                                          
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 1923/2320 [02:45<00:17, 22.97it/s{'loss': 0.0699, 'grad_norm': 1.8835704326629639, 'learning_rate': 1.8302752293577983e-05, 'epoch': 8.297413793103448}█████████████▉                          | 1923/2320 [02:45<00:17, 22.97it/s]
{'loss': 0.0699, 'grad_norm': 1.8835704326629639, 'learning_rate': 1.8302752293577983e-05, 'epoch': 8.3}                                                                                         
{'loss': 0.0699, 'grad_norm': 1.8835704326629639, 'learning_rate': 1.8302752293577983e-05, 'epoch': 8.3}                                                                                         
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 1950/2320 [02:46<00:16, 23.01it/s{'loss': 0.0658, 'grad_norm': 1.4495842456817627, 'learning_rate': 1.7155963302752295e-05, 'epoch': 8.405172413793103}███████████████▊                        | 1950/2320 [02:46<00:16, 23.01it/s]
{'loss': 0.0658, 'grad_norm': 1.4495842456817627, 'learning_rate': 1.7155963302752295e-05, 'epoch': 8.41}                                                                                        
{'loss': 0.0658, 'grad_norm': 1.4495842456817627, 'learning_rate': 1.7155963302752295e-05, 'epoch': 8.41}                                                                                        
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 1974/2320 [02:47<00:15, 22.82it/s{'loss': 0.0449, 'grad_norm': 0.786544144153595, 'learning_rate': 1.6009174311926606e-05, 'epoch': 8.512931034482758}██████████████████▎                      | 1974/2320 [02:47<00:15, 22.82it/s]
{'loss': 0.0449, 'grad_norm': 0.786544144153595, 'learning_rate': 1.6009174311926606e-05, 'epoch': 8.51}                                                                                         
{'loss': 0.0449, 'grad_norm': 0.786544144153595, 'learning_rate': 1.6009174311926606e-05, 'epoch': 8.51}                                                                                         
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 1998/2320 [02:48<00:13, 23.50it/s{'loss': 0.0972, 'grad_norm': 0.7433424592018127, 'learning_rate': 1.486238532110092e-05, 'epoch': 8.620689655172415}███████████████████▉                     | 1998/2320 [02:48<00:13, 23.50it/s]
{'loss': 0.0972, 'grad_norm': 0.7433424592018127, 'learning_rate': 1.486238532110092e-05, 'epoch': 8.62}                                                                                         
{'loss': 0.0972, 'grad_norm': 0.7433424592018127, 'learning_rate': 1.486238532110092e-05, 'epoch': 8.62}                                                                                         
 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 2025/2320 [02:49<00:12, 23.23it/s{'loss': 0.1242, 'grad_norm': 0.5173259377479553, 'learning_rate': 1.371559633027523e-05, 'epoch': 8.72844827586207}██████████████████████▋                   | 2025/2320 [02:49<00:12, 23.23it/s]
{'loss': 0.1242, 'grad_norm': 0.5173259377479553, 'learning_rate': 1.371559633027523e-05, 'epoch': 8.73}                                                                                         
{'loss': 0.1242, 'grad_norm': 0.5173259377479553, 'learning_rate': 1.371559633027523e-05, 'epoch': 8.73}                                                                                         
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 2049/2320 [02:50<00:11, 23.78it/s{'loss': 0.0725, 'grad_norm': 1.0220974683761597, 'learning_rate': 1.2568807339449543e-05, 'epoch': 8.836206896551724}██████████████████████▏                 | 2049/2320 [02:50<00:11, 23.78it/s]
{'loss': 0.0725, 'grad_norm': 1.0220974683761597, 'learning_rate': 1.2568807339449543e-05, 'epoch': 8.84}                                                                                        
{'loss': 0.0725, 'grad_norm': 1.0220974683761597, 'learning_rate': 1.2568807339449543e-05, 'epoch': 8.84}                                                                                        
 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                | 2073/2320 [02:51<00:10, 23.69it/s{'loss': 0.1331, 'grad_norm': 0.48770612478256226, 'learning_rate': 1.1422018348623854e-05, 'epoch': 8.943965517241379}██████████████████████▊                | 2073/2320 [02:51<00:10, 23.68it/s]
{'loss': 0.1331, 'grad_norm': 0.48770612478256226, 'learning_rate': 1.1422018348623854e-05, 'epoch': 8.94}                                                                                       
{'loss': 0.1331, 'grad_norm': 0.48770612478256226, 'learning_rate': 1.1422018348623854e-05, 'epoch': 8.94}                                                                                       
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 2088/2320 [02:52<00:10, 22.97it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 49.79it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 49.78it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.0823030099272728, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7012, 'eval_samples_per_second': 1322.096, 'eval_steps_per_second': 41.36, 'epoch': 9.0}
{'eval_loss': 0.0823030099272728, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7012, 'eval_samples_per_second': 1322.096, 'eval_steps_per_second': 41.36, 'epoch': 9.0}                                 
{'eval_loss': 0.0823030099272728, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.7012, 'eval_samples_per_second': 1322.096, 'eval_steps_per_second': 41.36, 'epoch': 9.0}                                 
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 2099/2320 [02:54<00:20, 10.94it/s{'loss': 0.0997, 'grad_norm': 1.403548002243042, 'learning_rate': 1.0275229357798166e-05, 'epoch': 9.051724137931034}██████████████████████████▌              | 2099/2320 [02:54<00:20, 10.94it/s]
{'loss': 0.0997, 'grad_norm': 1.403548002243042, 'learning_rate': 1.0275229357798166e-05, 'epoch': 9.05}                                                                                         
{'loss': 0.0997, 'grad_norm': 1.403548002243042, 'learning_rate': 1.0275229357798166e-05, 'epoch': 9.05}                                                                                         
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 2123/2320 [02:55<00:08, 21.94it/s{'loss': 0.1128, 'grad_norm': 0.2740180790424347, 'learning_rate': 9.128440366972477e-06, 'epoch': 9.15948275862069}█████████████████████████████             | 2123/2320 [02:55<00:08, 21.94it/s]
{'loss': 0.1128, 'grad_norm': 0.2740180790424347, 'learning_rate': 9.128440366972477e-06, 'epoch': 9.16}                                                                                         
{'loss': 0.1128, 'grad_norm': 0.2740180790424347, 'learning_rate': 9.128440366972477e-06, 'epoch': 9.16}                                                                                         
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 2150/2320 [02:56<00:07, 23.18it/s{'loss': 0.127, 'grad_norm': 0.4952039420604706, 'learning_rate': 7.981651376146789e-06, 'epoch': 9.267241379310345}██████████████████████████████▊           | 2150/2320 [02:56<00:07, 23.18it/s]
{'loss': 0.127, 'grad_norm': 0.4952039420604706, 'learning_rate': 7.981651376146789e-06, 'epoch': 9.27}                                                                                          
{'loss': 0.127, 'grad_norm': 0.4952039420604706, 'learning_rate': 7.981651376146789e-06, 'epoch': 9.27}                                                                                          
 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍         | 2174/2320 [02:57<00:06, 23.26it/s{'loss': 0.0953, 'grad_norm': 1.7899279594421387, 'learning_rate': 6.8348623853211015e-06, 'epoch': 9.375}██████████████████████████████████████████▍         | 2174/2320 [02:57<00:06, 23.26it/s]
{'loss': 0.0953, 'grad_norm': 1.7899279594421387, 'learning_rate': 6.8348623853211015e-06, 'epoch': 9.38}                                                                                        
{'loss': 0.0953, 'grad_norm': 1.7899279594421387, 'learning_rate': 6.8348623853211015e-06, 'epoch': 9.38}                                                                                        
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 2198/2320 [02:58<00:05, 23.23it/s{'loss': 0.1569, 'grad_norm': 2.912008047103882, 'learning_rate': 5.688073394495413e-06, 'epoch': 9.482758620689655}██████████████████████████████████        | 2198/2320 [02:58<00:05, 23.23it/s]
{'loss': 0.1569, 'grad_norm': 2.912008047103882, 'learning_rate': 5.688073394495413e-06, 'epoch': 9.48}                                                                                          
{'loss': 0.1569, 'grad_norm': 2.912008047103882, 'learning_rate': 5.688073394495413e-06, 'epoch': 9.48}                                                                                          
 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2225/2320 [02:59<00:04, 23.04it/s{'loss': 0.1237, 'grad_norm': 0.536015510559082, 'learning_rate': 4.5412844036697256e-06, 'epoch': 9.59051724137931}███████████████████████████████████▊      | 2225/2320 [02:59<00:04, 23.04it/s]
{'loss': 0.1237, 'grad_norm': 0.536015510559082, 'learning_rate': 4.5412844036697256e-06, 'epoch': 9.59}                                                                                         
{'loss': 0.1237, 'grad_norm': 0.536015510559082, 'learning_rate': 4.5412844036697256e-06, 'epoch': 9.59}                                                                                         
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 2249/2320 [03:00<00:03, 23.11it/s{'loss': 0.0681, 'grad_norm': 0.7523512244224548, 'learning_rate': 3.394495412844037e-06, 'epoch': 9.698275862068966}████████████████████████████████████▎    | 2249/2320 [03:00<00:03, 23.11it/s]
{'loss': 0.0681, 'grad_norm': 0.7523512244224548, 'learning_rate': 3.394495412844037e-06, 'epoch': 9.7}                                                                                          
{'loss': 0.0681, 'grad_norm': 0.7523512244224548, 'learning_rate': 3.394495412844037e-06, 'epoch': 9.7}                                                                                          
 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 2273/2320 [03:01<00:01, 23.59it/s{'loss': 0.0483, 'grad_norm': 7.050137996673584, 'learning_rate': 2.2477064220183487e-06, 'epoch': 9.806034482758621}█████████████████████████████████████▉   | 2273/2320 [03:01<00:01, 23.59it/s]
{'loss': 0.0483, 'grad_norm': 7.050137996673584, 'learning_rate': 2.2477064220183487e-06, 'epoch': 9.81}                                                                                         
{'loss': 0.0483, 'grad_norm': 7.050137996673584, 'learning_rate': 2.2477064220183487e-06, 'epoch': 9.81}                                                                                         
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 2300/2320 [03:02<00:00, 23.38it/s{'loss': 0.0736, 'grad_norm': 0.5052005052566528, 'learning_rate': 1.1009174311926608e-06, 'epoch': 9.913793103448276}██████████████████████████████████████▋ | 2300/2320 [03:02<00:00, 23.38it/s]
{'loss': 0.0736, 'grad_norm': 0.5052005052566528, 'learning_rate': 1.1009174311926608e-06, 'epoch': 9.91}                                                                                        
{'loss': 0.0736, 'grad_norm': 0.5052005052566528, 'learning_rate': 1.1009174311926608e-06, 'epoch': 9.91}                                                                                        
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 2318/2320 [03:03<00:00, 23.72it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.04it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.03it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08184702694416046, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8319, 'eval_samples_per_second': 1114.322, 'eval_steps_per_second': 34.86, 'epoch': 10.0}
{'eval_loss': 0.08184702694416046, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8319, 'eval_samples_per_second': 1114.322, 'eval_steps_per_second': 34.86, 'epoch': 10.0}                               
{'eval_loss': 0.08184702694416046, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.8890664759768534, 'eval_recall_macro': 0.8980092018674738, 'eval_f1_macro': 0.8858814583250998, 'eval_precision_weighted': 0.9857164288561981, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.9855262277934522, 'eval_f1': 0.9855262277934522, 'eval_balanced_accuracy': 0.8980092018674738, 'eval_cohen_kappa': 0.967685451347177, 'eval_matthews_corrcoef': 0.9677097790357755, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8319, 'eval_samples_per_second': 1114.322, 'eval_steps_per_second': 34.86, 'epoch': 10.0}                               
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:04<00:00, 23.72it/s{'train_runtime': 185.4167, 'train_samples_per_second': 199.928, 'train_steps_per_second': 12.512, 'train_loss': 0.3453203541451487, 'epoch': 10.0}███████████| 2320/2320 [03:04<00:00, 23.72it/s]
{'train_runtime': 185.4167, 'train_samples_per_second': 199.928, 'train_steps_per_second': 12.512, 'train_loss': 0.3453203541451487, 'epoch': 10.0}                                              
{'train_runtime': 185.4167, 'train_samples_per_second': 199.928, 'train_steps_per_second': 12.512, 'train_loss': 0.3453203541451487, 'epoch': 10.0}                                              
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:05<00:00, 12.51it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:05<00:00, 12.51it/s]
2025-04-26 11:37:44,900 - INFO - TrainMain - *** Training Finished ***
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.3453
  train_runtime            = 0:03:05.41
  train_samples_per_second =    199.928
  train_steps_per_second   =     12.512
2025-04-26 11:37:44,905 - INFO - TrainMain - Training metrics: {'train_runtime': 185.4167, 'train_samples_per_second': 199.928, 'train_steps_per_second': 12.512, 'train_loss': 0.3453203541451487, 'epoch': 10.0}
2025-04-26 11:37:44,905 - INFO - TrainMain - *** Starting Final Evaluation on Validation Set ***
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.04it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08002770692110062, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.920462891252292, 'eval_recall_macro': 0.8999380168717364, 'eval_f1_macro': 0.907765709222216, 'eval_precision_weighted': 0.9873688068595561, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875288670126504, 'eval_f1': 0.9875288670126504, 'eval_balanced_accuracy': 0.8999380168717364, 'eval_cohen_kappa': 0.9726545005966827, 'eval_matthews_corrcoef': 0.9726762321072991, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8283, 'eval_samples_per_second': 1119.198, 'eval_steps_per_second': 35.013, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 36.37it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 36.37it/s]
***** eval metrics *****
  eval_epoch                   =       10.0
  eval_eval_accuracy           =     0.9881
  eval_eval_balanced_accuracy  =     0.8999
  eval_eval_cohen_kappa        =     0.9727
  eval_eval_f1                 =     0.9875
  eval_eval_f1_macro           =     0.9078
  eval_eval_f1_micro           =     0.9881
  eval_eval_f1_weighted        =     0.9875
  eval_eval_loss               =       0.08
  eval_eval_matthews_corrcoef  =     0.9727
  eval_eval_precision_macro    =     0.9205
  eval_eval_precision_micro    =     0.9881
  eval_eval_precision_weighted =     0.9874
  eval_eval_recall_macro       =     0.8999
  eval_eval_recall_micro       =     0.9881
  eval_eval_recall_weighted    =     0.9881
  eval_eval_roc_auc_ovr        =        nan
  eval_eval_runtime            = 0:00:00.82
  eval_eval_samples_per_second =   1119.198
  eval_eval_steps_per_second   =     35.013
2025-04-26 11:37:45,741 - INFO - TrainMain - Final validation metrics: {'eval_eval_loss': 0.08002770692110062, 'eval_eval_accuracy': 0.9881337648327939, 'eval_eval_precision_micro': 0.9881337648327939, 'eval_eval_recall_micro': 0.9881337648327939, 'eval_eval_f1_micro': 0.9881337648327939, 'eval_eval_precision_macro': 0.920462891252292, 'eval_eval_recall_macro': 0.8999380168717364, 'eval_eval_f1_macro': 0.907765709222216, 'eval_eval_precision_weighted': 0.9873688068595561, 'eval_eval_recall_weighted': 0.9881337648327939, 'eval_eval_f1_weighted': 0.9875288670126504, 'eval_eval_f1': 0.9875288670126504, 'eval_eval_balanced_accuracy': 0.8999380168717364, 'eval_eval_cohen_kappa': 0.9726545005966827, 'eval_eval_matthews_corrcoef': 0.9726762321072991, 'eval_eval_roc_auc_ovr': nan, 'eval_eval_runtime': 0.8283, 'eval_eval_samples_per_second': 1119.198, 'eval_eval_steps_per_second': 35.013, 'eval_epoch': 10.0}
2025-04-26 11:37:45,741 - INFO - TrainMain - *** Starting Test Set Evaluation ***
2025-04-26 11:37:45,742 - INFO - TrainMain - Loading test dataset from: ./coding_task/data/atis/test.tsv
2025-04-26 11:37:45,742 - INFO - DataProcessor - DataProcessor initialized with precomputed label mappings.
2025-04-26 11:37:45,742 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/test.tsv using Pandas
2025-04-26 11:37:45,742 - INFO - Data Utils - File extension: .tsv
2025-04-26 11:37:45,742 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 11:37:45,742 - INFO - Data Utils -   file_path: ./coding_task/data/atis/test.tsv
2025-04-26 11:37:45,742 - INFO - Data Utils -   file_type: tsv
2025-04-26 11:37:45,742 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 11:37:45,742 - INFO - Data Utils -   use_dask: False
2025-04-26 11:37:45,742 - INFO - Data Utils -   unpack_multi_labels: False
2025-04-26 11:37:45,742 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:37:45,745 - INFO - Data Utils - Loaded initial Pandas df with 850 rows and 2 columns.
2025-04-26 11:37:45,745 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:37:45,745 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:37:45,747 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:37:45,748 - INFO - Data Utils - Resetting index after cleanup. New shape: (850, 2)
2025-04-26 11:37:45,748 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (850, 2)
2025-04-26 11:37:45,752 - INFO - DataProcessor - Preparing labels for task type: multiclass
2025-04-26 11:37:45,752 - INFO - DataProcessor - Using precomputed 22 labels for multiclass task.
2025-04-26 11:37:45,754 - WARNING - DataProcessor - Labels found in data but not in precomputed label_map: ['day_name' 'airfare+flight' 'flight+airline' 'flight_no+airline']. These rows will have NaN labels.
2025-04-26 11:37:45,755 - WARNING - DataProcessor - NaN values found in the 'labels' column after processing. Check data and label mapping.
2025-04-26 11:37:45,756 - INFO - DataProcessor - Using full dataset for training (validation_split_ratio=0).
2025-04-26 11:37:45,756 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:37:45,764 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 845/845 [00:00<00:00, 26998.80 examples/s]
2025-04-26 11:37:45,822 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:37:45,823 - INFO - TrainMain - *** Starting Test Set Evaluation ***
 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                 | 24/27 [00:00<00:00, 52.31it/s{'eval_loss': 0.09865611791610718, 'eval_accuracy': 0.9798816568047337, 'eval_precision_micro': 0.9798816568047337, 'eval_recall_micro': 0.9798816568047337, 'eval_f1_micro': 0.9798816568047337, 'eval_precision_macro': 0.9375893243235316, 'eval_recall_macro': 0.9365355297910355, 'eval_f1_macro': 0.9253472886144412, 'eval_precision_weighted': 0.9825867915628786, 'eval_recall_weighted': 0.9798816568047337, 'eval_f1_weighted': 0.979967124026431, 'eval_f1': 0.979967124026431, 'eval_balanced_accuracy': 0.9365355297910355, 'eval_cohen_kappa': 0.9569597042159416, 'eval_matthews_corrcoef': 0.9570657483418739, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6406, 'eval_samples_per_second': 1319.081, 'eval_steps_per_second': 42.148, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 43.75it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 43.75it/s]
***** test metrics *****
  test_epoch                   =       10.0
  test_eval_accuracy           =     0.9799
  test_eval_balanced_accuracy  =     0.9365
  test_eval_cohen_kappa        =      0.957
  test_eval_f1                 =       0.98
  test_eval_f1_macro           =     0.9253
  test_eval_f1_micro           =     0.9799
  test_eval_f1_weighted        =       0.98
  test_eval_loss               =     0.0987
  test_eval_matthews_corrcoef  =     0.9571
  test_eval_precision_macro    =     0.9376
  test_eval_precision_micro    =     0.9799
  test_eval_precision_weighted =     0.9826
  test_eval_recall_macro       =     0.9365
  test_eval_recall_micro       =     0.9799
  test_eval_recall_weighted    =     0.9799
  test_eval_roc_auc_ovr        =        nan
  test_eval_runtime            = 0:00:00.64
  test_eval_samples_per_second =   1319.081
  test_eval_steps_per_second   =     42.148
2025-04-26 11:37:46,471 - INFO - TrainMain - Test set evaluation metrics: {'test_eval_loss': 0.09865611791610718, 'test_eval_accuracy': 0.9798816568047337, 'test_eval_precision_micro': 0.9798816568047337, 'test_eval_recall_micro': 0.9798816568047337, 'test_eval_f1_micro': 0.9798816568047337, 'test_eval_precision_macro': 0.9375893243235316, 'test_eval_recall_macro': 0.9365355297910355, 'test_eval_f1_macro': 0.9253472886144412, 'test_eval_precision_weighted': 0.9825867915628786, 'test_eval_recall_weighted': 0.9798816568047337, 'test_eval_f1_weighted': 0.979967124026431, 'test_eval_f1': 0.979967124026431, 'test_eval_balanced_accuracy': 0.9365355297910355, 'test_eval_cohen_kappa': 0.9569597042159416, 'test_eval_matthews_corrcoef': 0.9570657483418739, 'test_eval_roc_auc_ovr': nan, 'test_eval_runtime': 0.6406, 'test_eval_samples_per_second': 1319.081, 'test_eval_steps_per_second': 42.148, 'test_epoch': 10.0}
2025-04-26 11:37:46,471 - INFO - TrainMain - *** Evaluation Finished ***
2025-04-26 11:37:46,471 - INFO - TrainMain - Saving final model/adapter to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:37:46,902 - INFO - TrainMain - Tokenizer saved to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:37:46,902 - ERROR - TrainMain - Failed to save training arguments: 'TrainingConfig' object has no attribute 'to_dict'
2025-04-26 11:37:46,902 - INFO - TrainMain - Script finished successfully.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
