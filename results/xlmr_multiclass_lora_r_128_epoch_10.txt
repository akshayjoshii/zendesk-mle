(env-pointnet) gururaj@bt69xp2:~/LRP_Experiment/zendesk-mle-master$ bash ./coding_task/train/train_multiclass.sh
2025-04-26 11:44:16,743 - INFO - TrainMain - Parsing configuration from command line arguments.
2025-04-26 11:44:16,890 - INFO - TrainMain - --- Data Configuration ---
2025-04-26 11:44:16,891 - INFO - TrainMain - DataConfig(dataset_path='./coding_task/data/atis/train.tsv', text_column='atis_text', label_column='atis_labels', task_type='multiclass', unpack_multi_labels=False, label_delimiter='+', use_dask=False, validation_split_ratio=0.2, max_seq_length=128, test_dataset_path='./coding_task/data/atis/test.tsv')
2025-04-26 11:44:16,891 - INFO - TrainMain - --- Model Configuration ---
2025-04-26 11:44:16,891 - INFO - TrainMain - ModelConfig(model_name_or_path='xlm-roberta-base', cache_dir=None, freeze_base_model=True)
2025-04-26 11:44:16,891 - INFO - TrainMain - --- PEFT Configuration ---
2025-04-26 11:44:16,891 - INFO - TrainMain - PeftConfig(method='lora', lora_r=128, lora_alpha=16, lora_dropout=0.1, lora_target_modules=None)
2025-04-26 11:44:16,891 - INFO - TrainMain - --- Training Configuration ---
2025-04-26 11:44:16,891 - INFO - TrainMain - TrainingConfig(output_dir='./results/atis_multiclass_xlmr_lora', num_train_epochs=10.0, per_device_train_batch_size=16, per_device_eval_batch_size=32, learning_rate=0.0001, weight_decay=0.01, lr_scheduler_type='linear', warmup_ratio=0.06, logging_dir='/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/logs/training_logs', logging_steps=25, evaluation_strategy='epoch', eval_steps=None, save_strategy='epoch', save_steps=None, save_total_limit=2, load_best_model_at_end=True, metric_for_best_model='eval_f1_weighted', greater_is_better=True, fp16=True, gradient_accumulation_steps=1, gradient_checkpointing=False, seed=42, report_to=['tensorboard'])
2025-04-26 11:44:16,894 - INFO - TrainMain - Set random seed to: 42
2025-04-26 11:44:16,894 - INFO - TrainMain - Output directory: ./results/atis_multiclass_xlmr_lora
2025-04-26 11:44:16,894 - INFO - TrainMain - Loading tokenizer: xlm-roberta-base
2025-04-26 11:44:18,520 - INFO - TrainMain - Starting data processing for Train/Validation...
2025-04-26 11:44:18,520 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/train.tsv using Pandas
2025-04-26 11:44:18,520 - INFO - Data Utils - File extension: .tsv
2025-04-26 11:44:18,520 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 11:44:18,520 - INFO - Data Utils -   file_path: ./coding_task/data/atis/train.tsv
2025-04-26 11:44:18,520 - INFO - Data Utils -   file_type: tsv
2025-04-26 11:44:18,520 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 11:44:18,520 - INFO - Data Utils -   use_dask: False
2025-04-26 11:44:18,520 - INFO - Data Utils -   unpack_multi_labels: False
2025-04-26 11:44:18,520 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:44:18,527 - INFO - Data Utils - Loaded initial Pandas df with 4634 rows and 2 columns.
2025-04-26 11:44:18,527 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:44:18,527 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:44:18,532 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:44:18,534 - INFO - Data Utils - Resetting index after cleanup. New shape: (4634, 2)
2025-04-26 11:44:18,535 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (4634, 2)
2025-04-26 11:44:18,538 - INFO - DataProcessor - Preparing labels for task type: multiclass
2025-04-26 11:44:18,539 - INFO - DataProcessor - Calculated 22 unique labels for multiclass task.
2025-04-26 11:44:18,541 - INFO - DataProcessor - Splitting data into train/validation (0.8/0.2)
2025-04-26 11:44:18,543 - INFO - DataProcessor - Train size: 3707, Validation size: 927
2025-04-26 11:44:18,543 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:44:18,561 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3707/3707 [00:00<00:00, 21552.72 examples/s]
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 927/927 [00:00<00:00, 18212.78 examples/s]
2025-04-26 11:44:18,875 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:44:18,905 - INFO - TrainMain - Data processing complete. Number of labels: 22
2025-04-26 11:44:18,905 - INFO - TrainMain - Label mapping (id2label): {0: 'abbreviation', 1: 'aircraft', 2: 'aircraft+flight+flight_no', 3: 'airfare', 4: 'airfare+flight_time', 5: 'airline', 6: 'airline+flight_no', 7: 'airport', 8: 'capacity', 9: 'cheapest', 10: 'city', 11: 'distance', 12: 'flight', 13: 'flight+airfare', 14: 'flight_no', 15: 'flight_time', 16: 'ground_fare', 17: 'ground_service', 18: 'ground_service+ground_fare', 19: 'meal', 20: 'quantity', 21: 'restriction'}
2025-04-26 11:44:18,906 - INFO - TrainMain - Label mappings saved to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:44:18,906 - INFO - TrainMain - Loading model for training...
2025-04-26 11:44:18,907 - INFO - ModelLoader - Loading base model: xlm-roberta-base
2025-04-26 11:44:18,907 - INFO - ModelLoader - Configuring model for 'single_label_classification' (num_labels=22).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-04-26 11:44:21,610 - INFO - ModelLoader - Freezing base model parameters.
2025-04-26 11:44:21,611 - INFO - ModelLoader - Applying PEFT method: lora
2025-04-26 11:44:21,611 - INFO - ModelLoader - lora_target_modules not specified, attempting auto-detection by PEFT library.
2025-04-26 11:44:21,611 - INFO - ModelLoader - LoRA Config: r=128, alpha=16, dropout=0.1, target_modules=auto
2025-04-26 11:44:21,918 - INFO - ModelLoader - PEFT model created successfully.
trainable params: 5,326,102 || all params: 283,386,668 || trainable%: 1.8794
2025-04-26 11:44:21,919 - INFO - TrainMain - Model loading complete.
2025-04-26 11:44:21,919 - INFO - TrainMain - Setting up Trainer...
2025-04-26 11:44:21,919 - INFO - TrainerSetup - Configuring HuggingFace Trainer...
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-04-26 11:44:21,992 - INFO - TrainerSetup - Adding EarlyStoppingCallback with patience=3 based on 'eval_f1_weighted'.
/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/train/trainer.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-04-26 11:44:22,500] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
You are adding a <class 'transformers.trainer_callback.ProgressCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
TensorBoardCallback
PrinterCallback
ProgressCallback
EarlyStoppingCallback
2025-04-26 11:44:25,424 - INFO - TrainerSetup - Trainer configured.
2025-04-26 11:44:25,424 - INFO - TrainMain - Trainer setup complete.
2025-04-26 11:44:25,424 - INFO - TrainMain - *** Starting Training ***
  1%|█▋                                                                                                                                                        | 25/2320 [00:03<04:06,  9.31it/s{'loss': 3.0862, 'grad_norm': 9.576790809631348, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.10775862068965517}                                         | 25/2320 [00:03<04:06,  9.31it/s]
{'loss': 3.0862, 'grad_norm': 9.576790809631348, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.11}                                                                                         
{'loss': 3.0862, 'grad_norm': 9.576790809631348, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.11}                                                                                         
  2%|███▎                                                                                                                                                      | 50/2320 [00:05<04:09,  9.08it/s{'loss': 2.7166, 'grad_norm': 7.614360809326172, 'learning_rate': 3.5e-05, 'epoch': 0.21551724137931033}                                                        | 50/2320 [00:05<04:10,  9.08it/s]
{'loss': 2.7166, 'grad_norm': 7.614360809326172, 'learning_rate': 3.5e-05, 'epoch': 0.22}                                                                                                        
{'loss': 2.7166, 'grad_norm': 7.614360809326172, 'learning_rate': 3.5e-05, 'epoch': 0.22}                                                                                                        
  3%|████▉                                                                                                                                                     | 75/2320 [00:08<04:02,  9.28it/s{'loss': 1.9362, 'grad_norm': 3.7437093257904053, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.3232758620689655}                                          | 75/2320 [00:08<04:02,  9.28it/s]
{'loss': 1.9362, 'grad_norm': 3.7437093257904053, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.32}                                                                                         
{'loss': 1.9362, 'grad_norm': 3.7437093257904053, 'learning_rate': 5.285714285714286e-05, 'epoch': 0.32}                                                                                         
  4%|██████▌                                                                                                                                                  | 100/2320 [00:11<04:00,  9.21it/s{'loss': 1.1939, 'grad_norm': 2.9591803550720215, 'learning_rate': 7.071428571428573e-05, 'epoch': 0.43103448275862066}                                        | 100/2320 [00:11<04:01,  9.21it/s]
{'loss': 1.1939, 'grad_norm': 2.9591803550720215, 'learning_rate': 7.071428571428573e-05, 'epoch': 0.43}                                                                                         
{'loss': 1.1939, 'grad_norm': 2.9591803550720215, 'learning_rate': 7.071428571428573e-05, 'epoch': 0.43}                                                                                         
  5%|████████▏                                                                                                                                                | 125/2320 [00:14<04:02,  9.04it/s{'loss': 1.2898, 'grad_norm': 3.402405261993408, 'learning_rate': 8.857142857142857e-05, 'epoch': 0.5387931034482759}                                          | 125/2320 [00:14<04:02,  9.04it/s]
{'loss': 1.2898, 'grad_norm': 3.402405261993408, 'learning_rate': 8.857142857142857e-05, 'epoch': 0.54}                                                                                          
{'loss': 1.2898, 'grad_norm': 3.402405261993408, 'learning_rate': 8.857142857142857e-05, 'epoch': 0.54}                                                                                          
  6%|█████████▉                                                                                                                                               | 150/2320 [00:16<03:54,  9.26it/s{'loss': 1.1782, 'grad_norm': 3.2111194133758545, 'learning_rate': 9.958715596330276e-05, 'epoch': 0.646551724137931}                                          | 150/2320 [00:16<03:54,  9.26it/s]
{'loss': 1.1782, 'grad_norm': 3.2111194133758545, 'learning_rate': 9.958715596330276e-05, 'epoch': 0.65}                                                                                         
{'loss': 1.1782, 'grad_norm': 3.2111194133758545, 'learning_rate': 9.958715596330276e-05, 'epoch': 0.65}                                                                                         
  8%|███████████▌                                                                                                                                             | 175/2320 [00:19<03:54,  9.16it/s{'loss': 1.1088, 'grad_norm': 5.570509910583496, 'learning_rate': 9.844036697247708e-05, 'epoch': 0.7543103448275862}                                          | 175/2320 [00:19<03:54,  9.16it/s]
{'loss': 1.1088, 'grad_norm': 5.570509910583496, 'learning_rate': 9.844036697247708e-05, 'epoch': 0.75}                                                                                          
{'loss': 1.1088, 'grad_norm': 5.570509910583496, 'learning_rate': 9.844036697247708e-05, 'epoch': 0.75}                                                                                          
  9%|█████████████▏                                                                                                                                           | 200/2320 [00:22<03:40,  9.62it/s{'loss': 0.9728, 'grad_norm': 1.8349449634552002, 'learning_rate': 9.729357798165138e-05, 'epoch': 0.8620689655172413}                                         | 200/2320 [00:22<03:40,  9.61it/s]
{'loss': 0.9728, 'grad_norm': 1.8349449634552002, 'learning_rate': 9.729357798165138e-05, 'epoch': 0.86}                                                                                         
{'loss': 0.9728, 'grad_norm': 1.8349449634552002, 'learning_rate': 9.729357798165138e-05, 'epoch': 0.86}                                                                                         
 10%|██████████████▊                                                                                                                                          | 225/2320 [00:24<03:43,  9.39it/s{'loss': 0.9426, 'grad_norm': 2.50594425201416, 'learning_rate': 9.61467889908257e-05, 'epoch': 0.9698275862068966}                                            | 225/2320 [00:24<03:43,  9.39it/s]
{'loss': 0.9426, 'grad_norm': 2.50594425201416, 'learning_rate': 9.61467889908257e-05, 'epoch': 0.97}                                                                                            
{'loss': 0.9426, 'grad_norm': 2.50594425201416, 'learning_rate': 9.61467889908257e-05, 'epoch': 0.97}                                                                                            
 10%|███████████████▎                                                                                                                                         | 232/2320 [00:25<03:39,  9.52it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.44it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.44it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in scalar divide
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
{'eval_loss': 0.806633472442627, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8873, 'eval_samples_per_second': 1044.758, 'eval_steps_per_second': 32.684, 'epoch': 1.0}
{'eval_loss': 0.806633472442627, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8873, 'eval_samples_per_second': 1044.758, 'eval_steps_per_second': 32.684, 'epoch': 1.0}                                                                                   
{'eval_loss': 0.806633472442627, 'eval_accuracy': 0.7443365695792881, 'eval_precision_micro': 0.7443365695792881, 'eval_recall_micro': 0.7443365695792881, 'eval_f1_micro': 0.7443365695792881, 'eval_precision_macro': 0.046521035598705504, 'eval_recall_macro': 0.0625, 'eval_f1_macro': 0.05333951762523191, 'eval_precision_weighted': 0.5540369288130623, 'eval_recall_weighted': 0.7443365695792881, 'eval_f1_weighted': 0.6352408571548654, 'eval_f1': 0.6352408571548654, 'eval_balanced_accuracy': 0.0625, 'eval_cohen_kappa': 0.0, 'eval_matthews_corrcoef': 0.0, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8873, 'eval_samples_per_second': 1044.758, 'eval_steps_per_second': 32.684, 'epoch': 1.0}                                                                                   
 11%|████████████████▍                                                                                                                                        | 249/2320 [00:28<03:45,  9.20it/s{'loss': 0.9565, 'grad_norm': 2.1565353870391846, 'learning_rate': 9.5e-05, 'epoch': 1.0775862068965518}                                                       | 249/2320 [00:28<03:45,  9.20it/s]
{'loss': 0.9565, 'grad_norm': 2.1565353870391846, 'learning_rate': 9.5e-05, 'epoch': 1.08}                                                                                                       
{'loss': 0.9565, 'grad_norm': 2.1565353870391846, 'learning_rate': 9.5e-05, 'epoch': 1.08}                                                                                                       
 12%|██████████████████▏                                                                                                                                      | 275/2320 [00:31<03:38,  9.34it/s{'loss': 0.9107, 'grad_norm': 4.59646463394165, 'learning_rate': 9.385321100917431e-05, 'epoch': 1.1853448275862069}                                           | 275/2320 [00:31<03:38,  9.34it/s]
{'loss': 0.9107, 'grad_norm': 4.59646463394165, 'learning_rate': 9.385321100917431e-05, 'epoch': 1.19}                                                                                           
{'loss': 0.9107, 'grad_norm': 4.59646463394165, 'learning_rate': 9.385321100917431e-05, 'epoch': 1.19}                                                                                           
 13%|███████████████████▊                                                                                                                                     | 300/2320 [00:34<03:35,  9.38it/s{'loss': 0.6886, 'grad_norm': 2.246331214904785, 'learning_rate': 9.270642201834863e-05, 'epoch': 1.293103448275862}                                           | 300/2320 [00:34<03:35,  9.38it/s]
{'loss': 0.6886, 'grad_norm': 2.246331214904785, 'learning_rate': 9.270642201834863e-05, 'epoch': 1.29}                                                                                          
{'loss': 0.6886, 'grad_norm': 2.246331214904785, 'learning_rate': 9.270642201834863e-05, 'epoch': 1.29}                                                                                          
 14%|█████████████████████▍                                                                                                                                   | 325/2320 [00:37<03:39,  9.10it/s{'loss': 0.6728, 'grad_norm': 1.7775150537490845, 'learning_rate': 9.155963302752293e-05, 'epoch': 1.4008620689655173}                                         | 325/2320 [00:37<03:39,  9.10it/s]
{'loss': 0.6728, 'grad_norm': 1.7775150537490845, 'learning_rate': 9.155963302752293e-05, 'epoch': 1.4}                                                                                          
{'loss': 0.6728, 'grad_norm': 1.7775150537490845, 'learning_rate': 9.155963302752293e-05, 'epoch': 1.4}                                                                                          
 15%|███████████████████████                                                                                                                                  | 350/2320 [00:39<03:33,  9.21it/s{'loss': 0.7107, 'grad_norm': 2.197504997253418, 'learning_rate': 9.041284403669725e-05, 'epoch': 1.5086206896551724}                                          | 350/2320 [00:39<03:33,  9.21it/s]
{'loss': 0.7107, 'grad_norm': 2.197504997253418, 'learning_rate': 9.041284403669725e-05, 'epoch': 1.51}                                                                                          
{'loss': 0.7107, 'grad_norm': 2.197504997253418, 'learning_rate': 9.041284403669725e-05, 'epoch': 1.51}                                                                                          
 16%|████████████████████████▋                                                                                                                                | 375/2320 [00:42<03:32,  9.15it/s{'loss': 0.7075, 'grad_norm': 2.9129879474639893, 'learning_rate': 8.926605504587157e-05, 'epoch': 1.6163793103448276}                                         | 375/2320 [00:42<03:32,  9.15it/s]
{'loss': 0.7075, 'grad_norm': 2.9129879474639893, 'learning_rate': 8.926605504587157e-05, 'epoch': 1.62}                                                                                         
{'loss': 0.7075, 'grad_norm': 2.9129879474639893, 'learning_rate': 8.926605504587157e-05, 'epoch': 1.62}                                                                                         
 17%|██████████████████████████▍                                                                                                                              | 400/2320 [00:45<03:29,  9.16it/s{'loss': 0.5717, 'grad_norm': 1.3795982599258423, 'learning_rate': 8.811926605504587e-05, 'epoch': 1.7241379310344827}                                         | 400/2320 [00:45<03:29,  9.16it/s]
{'loss': 0.5717, 'grad_norm': 1.3795982599258423, 'learning_rate': 8.811926605504587e-05, 'epoch': 1.72}                                                                                         
{'loss': 0.5717, 'grad_norm': 1.3795982599258423, 'learning_rate': 8.811926605504587e-05, 'epoch': 1.72}                                                                                         
 18%|████████████████████████████                                                                                                                             | 425/2320 [00:48<03:23,  9.29it/s{'loss': 0.5023, 'grad_norm': 4.193526268005371, 'learning_rate': 8.697247706422019e-05, 'epoch': 1.831896551724138}                                           | 425/2320 [00:48<03:23,  9.29it/s]
{'loss': 0.5023, 'grad_norm': 4.193526268005371, 'learning_rate': 8.697247706422019e-05, 'epoch': 1.83}                                                                                          
{'loss': 0.5023, 'grad_norm': 4.193526268005371, 'learning_rate': 8.697247706422019e-05, 'epoch': 1.83}                                                                                          
 19%|█████████████████████████████▋                                                                                                                           | 450/2320 [00:50<03:22,  9.24it/s{'loss': 0.4003, 'grad_norm': 1.6605494022369385, 'learning_rate': 8.58256880733945e-05, 'epoch': 1.9396551724137931}                                          | 450/2320 [00:50<03:22,  9.24it/s]
{'loss': 0.4003, 'grad_norm': 1.6605494022369385, 'learning_rate': 8.58256880733945e-05, 'epoch': 1.94}                                                                                          
{'loss': 0.4003, 'grad_norm': 1.6605494022369385, 'learning_rate': 8.58256880733945e-05, 'epoch': 1.94}                                                                                          
 20%|██████████████████████████████▌                                                                                                                          | 464/2320 [00:52<03:19,  9.31it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.82it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.82it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.31562140583992004, 'eval_accuracy': 0.9223300970873787, 'eval_precision_micro': 0.9223300970873787, 'eval_recall_micro': 0.9223300970873787, 'eval_f1_micro': 0.9223300970873787, 'eval_precision_macro': 0.3146342628061692, 'eval_recall_macro': 0.35011613159588695, 'eval_f1_macro': 0.3297317546929214, 'eval_precision_weighted': 0.8851347453713894, 'eval_recall_weighted': 0.9223300970873787, 'eval_f1_weighted': 0.9022931827848739, 'eval_f1': 0.9022931827848739, 'eval_balanced_accuracy': 0.35011613159588695, 'eval_cohen_kappa': 0.8159314293278618, 'eval_matthews_corrcoef': 0.8172415993077842, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8752, 'eval_samples_per_second': 1059.243, 'eval_steps_per_second': 33.137, 'epoch': 2.0}
{'eval_loss': 0.31562140583992004, 'eval_accuracy': 0.9223300970873787, 'eval_precision_micro': 0.9223300970873787, 'eval_recall_micro': 0.9223300970873787, 'eval_f1_micro': 0.9223300970873787, 'eval_precision_macro': 0.3146342628061692, 'eval_recall_macro': 0.35011613159588695, 'eval_f1_macro': 0.3297317546929214, 'eval_precision_weighted': 0.8851347453713894, 'eval_recall_weighted': 0.9223300970873787, 'eval_f1_weighted': 0.9022931827848739, 'eval_f1': 0.9022931827848739, 'eval_balanced_accuracy': 0.35011613159588695, 'eval_cohen_kappa': 0.8159314293278618, 'eval_matthews_corrcoef': 0.8172415993077842, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8752, 'eval_samples_per_second': 1059.243, 'eval_steps_per_second': 33.137, 'epoch': 2.0}                            
{'eval_loss': 0.31562140583992004, 'eval_accuracy': 0.9223300970873787, 'eval_precision_micro': 0.9223300970873787, 'eval_recall_micro': 0.9223300970873787, 'eval_f1_micro': 0.9223300970873787, 'eval_precision_macro': 0.3146342628061692, 'eval_recall_macro': 0.35011613159588695, 'eval_f1_macro': 0.3297317546929214, 'eval_precision_weighted': 0.8851347453713894, 'eval_recall_weighted': 0.9223300970873787, 'eval_f1_weighted': 0.9022931827848739, 'eval_f1': 0.9022931827848739, 'eval_balanced_accuracy': 0.35011613159588695, 'eval_cohen_kappa': 0.8159314293278618, 'eval_matthews_corrcoef': 0.8172415993077842, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8752, 'eval_samples_per_second': 1059.243, 'eval_steps_per_second': 33.137, 'epoch': 2.0}                            
 20%|███████████████████████████████▎                                                                                                                         | 475/2320 [00:54<03:57,  7.78it/s{'loss': 0.3221, 'grad_norm': 1.7962208986282349, 'learning_rate': 8.46788990825688e-05, 'epoch': 2.0474137931034484}                                          | 475/2320 [00:54<03:57,  7.78it/s]
{'loss': 0.3221, 'grad_norm': 1.7962208986282349, 'learning_rate': 8.46788990825688e-05, 'epoch': 2.05}                                                                                          
{'loss': 0.3221, 'grad_norm': 1.7962208986282349, 'learning_rate': 8.46788990825688e-05, 'epoch': 2.05}                                                                                          
 22%|████████████████████████████████▉                                                                                                                        | 499/2320 [00:57<03:17,  9.21it/s{'loss': 0.5448, 'grad_norm': 1.7475560903549194, 'learning_rate': 8.353211009174312e-05, 'epoch': 2.1551724137931036}                                         | 499/2320 [00:57<03:17,  9.21it/s]
{'loss': 0.5448, 'grad_norm': 1.7475560903549194, 'learning_rate': 8.353211009174312e-05, 'epoch': 2.16}                                                                                         
{'loss': 0.5448, 'grad_norm': 1.7475560903549194, 'learning_rate': 8.353211009174312e-05, 'epoch': 2.16}                                                                                         
 23%|██████████████████████████████████▌                                                                                                                      | 525/2320 [01:00<03:17,  9.08it/s{'loss': 0.3792, 'grad_norm': 0.609300434589386, 'learning_rate': 8.238532110091744e-05, 'epoch': 2.2629310344827585}                                          | 525/2320 [01:00<03:17,  9.08it/s]
{'loss': 0.3792, 'grad_norm': 0.609300434589386, 'learning_rate': 8.238532110091744e-05, 'epoch': 2.26}                                                                                          
{'loss': 0.3792, 'grad_norm': 0.609300434589386, 'learning_rate': 8.238532110091744e-05, 'epoch': 2.26}                                                                                          
 24%|████████████████████████████████████▎                                                                                                                    | 550/2320 [01:02<03:16,  9.03it/s{'loss': 0.3292, 'grad_norm': 2.284886121749878, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.3706896551724137}                                          | 550/2320 [01:02<03:16,  9.03it/s]
{'loss': 0.3292, 'grad_norm': 2.284886121749878, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.37}                                                                                          
{'loss': 0.3292, 'grad_norm': 2.284886121749878, 'learning_rate': 8.123853211009174e-05, 'epoch': 2.37}                                                                                          
 25%|█████████████████████████████████████▉                                                                                                                   | 575/2320 [01:05<03:08,  9.25it/s{'loss': 0.3804, 'grad_norm': 1.306384563446045, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.478448275862069}                                           | 575/2320 [01:05<03:08,  9.25it/s]
{'loss': 0.3804, 'grad_norm': 1.306384563446045, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.48}                                                                                          
{'loss': 0.3804, 'grad_norm': 1.306384563446045, 'learning_rate': 8.009174311926606e-05, 'epoch': 2.48}                                                                                          
 26%|███████████████████████████████████████▌                                                                                                                 | 600/2320 [01:08<03:10,  9.04it/s{'loss': 0.273, 'grad_norm': 1.7877840995788574, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.586206896551724}                                           | 600/2320 [01:08<03:10,  9.04it/s]
{'loss': 0.273, 'grad_norm': 1.7877840995788574, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.59}                                                                                          
{'loss': 0.273, 'grad_norm': 1.7877840995788574, 'learning_rate': 7.894495412844037e-05, 'epoch': 2.59}                                                                                          
 27%|█████████████████████████████████████████▏                                                                                                               | 625/2320 [01:11<03:06,  9.10it/s{'loss': 0.325, 'grad_norm': 2.110694646835327, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.6939655172413794}                                           | 625/2320 [01:11<03:06,  9.10it/s]
{'loss': 0.325, 'grad_norm': 2.110694646835327, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.69}                                                                                           
{'loss': 0.325, 'grad_norm': 2.110694646835327, 'learning_rate': 7.779816513761469e-05, 'epoch': 2.69}                                                                                           
 28%|██████████████████████████████████████████▊                                                                                                              | 650/2320 [01:13<03:02,  9.17it/s{'loss': 0.2642, 'grad_norm': 1.3247092962265015, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8017241379310347}                                         | 650/2320 [01:13<03:02,  9.17it/s]
{'loss': 0.2642, 'grad_norm': 1.3247092962265015, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8}                                                                                          
{'loss': 0.2642, 'grad_norm': 1.3247092962265015, 'learning_rate': 7.665137614678899e-05, 'epoch': 2.8}                                                                                          
 29%|████████████████████████████████████████████▌                                                                                                            | 675/2320 [01:16<02:53,  9.50it/s{'loss': 0.278, 'grad_norm': 1.5349282026290894, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.9094827586206895}                                           | 675/2320 [01:16<02:53,  9.50it/s]
{'loss': 0.278, 'grad_norm': 1.5349282026290894, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.91}                                                                                           
{'loss': 0.278, 'grad_norm': 1.5349282026290894, 'learning_rate': 7.55045871559633e-05, 'epoch': 2.91}                                                                                           
 30%|█████████████████████████████████████████████▉                                                                                                           | 696/2320 [01:18<02:54,  9.32it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.79it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.79it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.15607914328575134, 'eval_accuracy': 0.9600862998921251, 'eval_precision_micro': 0.9600862998921251, 'eval_recall_micro': 0.9600862998921251, 'eval_f1_micro': 0.9600862998921251, 'eval_precision_macro': 0.6889003076294482, 'eval_recall_macro': 0.6024080558906885, 'eval_f1_macro': 0.6130630343398489, 'eval_precision_weighted': 0.9557135100412846, 'eval_recall_weighted': 0.9600862998921251, 'eval_f1_weighted': 0.9547446909938921, 'eval_f1': 0.9547446909938921, 'eval_balanced_accuracy': 0.6024080558906885, 'eval_cohen_kappa': 0.9081858821639853, 'eval_matthews_corrcoef': 0.9083916266517321, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8735, 'eval_samples_per_second': 1061.303, 'eval_steps_per_second': 33.202, 'epoch': 3.0}
{'eval_loss': 0.15607914328575134, 'eval_accuracy': 0.9600862998921251, 'eval_precision_micro': 0.9600862998921251, 'eval_recall_micro': 0.9600862998921251, 'eval_f1_micro': 0.9600862998921251, 'eval_precision_macro': 0.6889003076294482, 'eval_recall_macro': 0.6024080558906885, 'eval_f1_macro': 0.6130630343398489, 'eval_precision_weighted': 0.9557135100412846, 'eval_recall_weighted': 0.9600862998921251, 'eval_f1_weighted': 0.9547446909938921, 'eval_f1': 0.9547446909938921, 'eval_balanced_accuracy': 0.6024080558906885, 'eval_cohen_kappa': 0.9081858821639853, 'eval_matthews_corrcoef': 0.9083916266517321, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8735, 'eval_samples_per_second': 1061.303, 'eval_steps_per_second': 33.202, 'epoch': 3.0}                              
{'eval_loss': 0.15607914328575134, 'eval_accuracy': 0.9600862998921251, 'eval_precision_micro': 0.9600862998921251, 'eval_recall_micro': 0.9600862998921251, 'eval_f1_micro': 0.9600862998921251, 'eval_precision_macro': 0.6889003076294482, 'eval_recall_macro': 0.6024080558906885, 'eval_f1_macro': 0.6130630343398489, 'eval_precision_weighted': 0.9557135100412846, 'eval_recall_weighted': 0.9600862998921251, 'eval_f1_weighted': 0.9547446909938921, 'eval_f1': 0.9547446909938921, 'eval_balanced_accuracy': 0.6024080558906885, 'eval_cohen_kappa': 0.9081858821639853, 'eval_matthews_corrcoef': 0.9083916266517321, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8735, 'eval_samples_per_second': 1061.303, 'eval_steps_per_second': 33.202, 'epoch': 3.0}                              
 30%|██████████████████████████████████████████████                                                                                                           | 699/2320 [01:20<08:37,  3.14it/s{'loss': 0.2419, 'grad_norm': 2.4445276260375977, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.0172413793103448}                                         | 699/2320 [01:20<08:37,  3.14it/s]
{'loss': 0.2419, 'grad_norm': 2.4445276260375977, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.02}                                                                                         
{'loss': 0.2419, 'grad_norm': 2.4445276260375977, 'learning_rate': 7.435779816513761e-05, 'epoch': 3.02}                                                                                         
 31%|███████████████████████████████████████████████▊                                                                                                         | 725/2320 [01:23<02:57,  8.98it/s{'loss': 0.2466, 'grad_norm': 0.4860590100288391, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.125}                                                      | 725/2320 [01:23<02:57,  8.98it/s]
{'loss': 0.2466, 'grad_norm': 0.4860590100288391, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.12}                                                                                         
{'loss': 0.2466, 'grad_norm': 0.4860590100288391, 'learning_rate': 7.321100917431193e-05, 'epoch': 3.12}                                                                                         
 32%|█████████████████████████████████████████████████▍                                                                                                       | 750/2320 [01:25<02:44,  9.56it/s{'loss': 0.2339, 'grad_norm': 1.1771479845046997, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.2327586206896552}                                         | 750/2320 [01:25<02:44,  9.56it/s]
{'loss': 0.2339, 'grad_norm': 1.1771479845046997, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.23}                                                                                         
{'loss': 0.2339, 'grad_norm': 1.1771479845046997, 'learning_rate': 7.206422018348624e-05, 'epoch': 3.23}                                                                                         
 33%|███████████████████████████████████████████████████                                                                                                      | 775/2320 [01:28<02:47,  9.25it/s{'loss': 0.1972, 'grad_norm': 1.0960886478424072, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.3405172413793105}                                         | 775/2320 [01:28<02:47,  9.25it/s]
{'loss': 0.1972, 'grad_norm': 1.0960886478424072, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.34}                                                                                         
{'loss': 0.1972, 'grad_norm': 1.0960886478424072, 'learning_rate': 7.091743119266056e-05, 'epoch': 3.34}                                                                                         
 34%|████████████████████████████████████████████████████▊                                                                                                    | 800/2320 [01:31<02:44,  9.23it/s{'loss': 0.2627, 'grad_norm': 4.72091817855835, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.4482758620689653}                                           | 800/2320 [01:31<02:44,  9.24it/s]
{'loss': 0.2627, 'grad_norm': 4.72091817855835, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.45}                                                                                           
{'loss': 0.2627, 'grad_norm': 4.72091817855835, 'learning_rate': 6.977064220183487e-05, 'epoch': 3.45}                                                                                           
 36%|██████████████████████████████████████████████████████▍                                                                                                  | 825/2320 [01:34<02:39,  9.38it/s{'loss': 0.2066, 'grad_norm': 0.20100148022174835, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.5560344827586206}                                        | 825/2320 [01:34<02:39,  9.38it/s]
{'loss': 0.2066, 'grad_norm': 0.20100148022174835, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.56}                                                                                        
{'loss': 0.2066, 'grad_norm': 0.20100148022174835, 'learning_rate': 6.862385321100918e-05, 'epoch': 3.56}                                                                                        
 37%|████████████████████████████████████████████████████████                                                                                                 | 850/2320 [01:36<02:31,  9.73it/s{'loss': 0.2085, 'grad_norm': 2.003620147705078, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.663793103448276}                                            | 850/2320 [01:36<02:31,  9.73it/s]
{'loss': 0.2085, 'grad_norm': 2.003620147705078, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.66}                                                                                           
{'loss': 0.2085, 'grad_norm': 2.003620147705078, 'learning_rate': 6.74770642201835e-05, 'epoch': 3.66}                                                                                           
 38%|█████████████████████████████████████████████████████████▋                                                                                               | 875/2320 [01:39<02:36,  9.23it/s{'loss': 0.1885, 'grad_norm': 0.9751369953155518, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.771551724137931}                                           | 875/2320 [01:39<02:36,  9.23it/s]
{'loss': 0.1885, 'grad_norm': 0.9751369953155518, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.77}                                                                                          
{'loss': 0.1885, 'grad_norm': 0.9751369953155518, 'learning_rate': 6.63302752293578e-05, 'epoch': 3.77}                                                                                          
 39%|███████████████████████████████████████████████████████████▎                                                                                             | 900/2320 [01:42<02:32,  9.28it/s{'loss': 0.184, 'grad_norm': 0.36700597405433655, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.8793103448275863}                                          | 900/2320 [01:42<02:32,  9.28it/s]
{'loss': 0.184, 'grad_norm': 0.36700597405433655, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.88}                                                                                          
{'loss': 0.184, 'grad_norm': 0.36700597405433655, 'learning_rate': 6.51834862385321e-05, 'epoch': 3.88}                                                                                          
 40%|█████████████████████████████████████████████████████████████                                                                                            | 925/2320 [01:44<02:29,  9.31it/s{'loss': 0.2173, 'grad_norm': 0.29103249311447144, 'learning_rate': 6.408256880733945e-05, 'epoch': 3.987068965517241}                                         | 925/2320 [01:44<02:29,  9.31it/s]
{'loss': 0.2173, 'grad_norm': 0.29103249311447144, 'learning_rate': 6.408256880733945e-05, 'epoch': 3.99}                                                                                        
{'loss': 0.2173, 'grad_norm': 0.29103249311447144, 'learning_rate': 6.408256880733945e-05, 'epoch': 3.99}                                                                                        
 40%|█████████████████████████████████████████████████████████████▏                                                                                           | 928/2320 [01:45<02:23,  9.71it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.87it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.87it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.10158032923936844, 'eval_accuracy': 0.9784250269687162, 'eval_precision_micro': 0.9784250269687162, 'eval_recall_micro': 0.9784250269687162, 'eval_f1_micro': 0.9784250269687162, 'eval_precision_macro': 0.70408586814863, 'eval_recall_macro': 0.6903748897814354, 'eval_f1_macro': 0.6923359116373822, 'eval_precision_weighted': 0.973351300838092, 'eval_recall_weighted': 0.9784250269687162, 'eval_f1_weighted': 0.9753079506458527, 'eval_f1': 0.9753079506458527, 'eval_balanced_accuracy': 0.6903748897814354, 'eval_cohen_kappa': 0.9504324461614555, 'eval_matthews_corrcoef': 0.9505124838294953, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8755, 'eval_samples_per_second': 1058.774, 'eval_steps_per_second': 33.122, 'epoch': 4.0}
{'eval_loss': 0.10158032923936844, 'eval_accuracy': 0.9784250269687162, 'eval_precision_micro': 0.9784250269687162, 'eval_recall_micro': 0.9784250269687162, 'eval_f1_micro': 0.9784250269687162, 'eval_precision_macro': 0.70408586814863, 'eval_recall_macro': 0.6903748897814354, 'eval_f1_macro': 0.6923359116373822, 'eval_precision_weighted': 0.973351300838092, 'eval_recall_weighted': 0.9784250269687162, 'eval_f1_weighted': 0.9753079506458527, 'eval_f1': 0.9753079506458527, 'eval_balanced_accuracy': 0.6903748897814354, 'eval_cohen_kappa': 0.9504324461614555, 'eval_matthews_corrcoef': 0.9505124838294953, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8755, 'eval_samples_per_second': 1058.774, 'eval_steps_per_second': 33.122, 'epoch': 4.0}                                 
{'eval_loss': 0.10158032923936844, 'eval_accuracy': 0.9784250269687162, 'eval_precision_micro': 0.9784250269687162, 'eval_recall_micro': 0.9784250269687162, 'eval_f1_micro': 0.9784250269687162, 'eval_precision_macro': 0.70408586814863, 'eval_recall_macro': 0.6903748897814354, 'eval_f1_macro': 0.6923359116373822, 'eval_precision_weighted': 0.973351300838092, 'eval_recall_weighted': 0.9784250269687162, 'eval_f1_weighted': 0.9753079506458527, 'eval_f1': 0.9753079506458527, 'eval_balanced_accuracy': 0.6903748897814354, 'eval_cohen_kappa': 0.9504324461614555, 'eval_matthews_corrcoef': 0.9505124838294953, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8755, 'eval_samples_per_second': 1058.774, 'eval_steps_per_second': 33.122, 'epoch': 4.0}                                 
 41%|██████████████████████████████████████████████████████████████▋                                                                                          | 950/2320 [01:49<02:26,  9.33it/s{'loss': 0.1544, 'grad_norm': 2.1887989044189453, 'learning_rate': 6.293577981651376e-05, 'epoch': 4.094827586206897}                                          | 950/2320 [01:49<02:26,  9.32it/s]
{'loss': 0.1544, 'grad_norm': 2.1887989044189453, 'learning_rate': 6.293577981651376e-05, 'epoch': 4.09}                                                                                         
{'loss': 0.1544, 'grad_norm': 2.1887989044189453, 'learning_rate': 6.293577981651376e-05, 'epoch': 4.09}                                                                                         
 42%|████████████████████████████████████████████████████████████████▎                                                                                        | 975/2320 [01:52<02:24,  9.33it/s{'loss': 0.1669, 'grad_norm': 1.1088786125183105, 'learning_rate': 6.178899082568807e-05, 'epoch': 4.202586206896552}                                          | 975/2320 [01:52<02:24,  9.33it/s]
{'loss': 0.1669, 'grad_norm': 1.1088786125183105, 'learning_rate': 6.178899082568807e-05, 'epoch': 4.2}                                                                                          
{'loss': 0.1669, 'grad_norm': 1.1088786125183105, 'learning_rate': 6.178899082568807e-05, 'epoch': 4.2}                                                                                          
 43%|█████████████████████████████████████████████████████████████████▌                                                                                      | 1000/2320 [01:54<02:18,  9.56it/s{'loss': 0.1655, 'grad_norm': 0.06590496748685837, 'learning_rate': 6.064220183486239e-05, 'epoch': 4.310344827586207}                                        | 1000/2320 [01:54<02:17,  9.57it/s]
{'loss': 0.1655, 'grad_norm': 0.06590496748685837, 'learning_rate': 6.064220183486239e-05, 'epoch': 4.31}                                                                                        
{'loss': 0.1655, 'grad_norm': 0.06590496748685837, 'learning_rate': 6.064220183486239e-05, 'epoch': 4.31}                                                                                        
 44%|███████████████████████████████████████████████████████████████████▏                                                                                    | 1025/2320 [01:57<02:16,  9.47it/s{'loss': 0.1956, 'grad_norm': 0.6026273965835571, 'learning_rate': 5.94954128440367e-05, 'epoch': 4.418103448275862}                                          | 1025/2320 [01:57<02:16,  9.47it/s]
{'loss': 0.1956, 'grad_norm': 0.6026273965835571, 'learning_rate': 5.94954128440367e-05, 'epoch': 4.42}                                                                                          
{'loss': 0.1956, 'grad_norm': 0.6026273965835571, 'learning_rate': 5.94954128440367e-05, 'epoch': 4.42}                                                                                          
 45%|████████████████████████████████████████████████████████████████████▊                                                                                   | 1050/2320 [02:00<02:18,  9.17it/s{'loss': 0.1529, 'grad_norm': 0.08533351123332977, 'learning_rate': 5.834862385321102e-05, 'epoch': 4.525862068965517}                                        | 1050/2320 [02:00<02:18,  9.17it/s]
{'loss': 0.1529, 'grad_norm': 0.08533351123332977, 'learning_rate': 5.834862385321102e-05, 'epoch': 4.53}                                                                                        
{'loss': 0.1529, 'grad_norm': 0.08533351123332977, 'learning_rate': 5.834862385321102e-05, 'epoch': 4.53}                                                                                        
 46%|██████████████████████████████████████████████████████████████████████▍                                                                                 | 1075/2320 [02:03<02:14,  9.24it/s{'loss': 0.1321, 'grad_norm': 0.6234331727027893, 'learning_rate': 5.720183486238533e-05, 'epoch': 4.633620689655173}                                         | 1075/2320 [02:03<02:14,  9.24it/s]
{'loss': 0.1321, 'grad_norm': 0.6234331727027893, 'learning_rate': 5.720183486238533e-05, 'epoch': 4.63}                                                                                         
{'loss': 0.1321, 'grad_norm': 0.6234331727027893, 'learning_rate': 5.720183486238533e-05, 'epoch': 4.63}                                                                                         
 47%|████████████████████████████████████████████████████████████████████████                                                                                | 1100/2320 [02:05<02:12,  9.18it/s{'loss': 0.1188, 'grad_norm': 0.7984396815299988, 'learning_rate': 5.605504587155963e-05, 'epoch': 4.741379310344827}                                         | 1100/2320 [02:05<02:12,  9.18it/s]
{'loss': 0.1188, 'grad_norm': 0.7984396815299988, 'learning_rate': 5.605504587155963e-05, 'epoch': 4.74}                                                                                         
{'loss': 0.1188, 'grad_norm': 0.7984396815299988, 'learning_rate': 5.605504587155963e-05, 'epoch': 4.74}                                                                                         
 48%|█████████████████████████████████████████████████████████████████████████▋                                                                              | 1125/2320 [02:08<02:08,  9.28it/s{'loss': 0.1619, 'grad_norm': 0.1134246438741684, 'learning_rate': 5.4908256880733946e-05, 'epoch': 4.849137931034483}                                        | 1125/2320 [02:08<02:08,  9.28it/s]
{'loss': 0.1619, 'grad_norm': 0.1134246438741684, 'learning_rate': 5.4908256880733946e-05, 'epoch': 4.85}                                                                                        
{'loss': 0.1619, 'grad_norm': 0.1134246438741684, 'learning_rate': 5.4908256880733946e-05, 'epoch': 4.85}                                                                                        
 50%|███████████████████████████████████████████████████████████████████████████▎                                                                            | 1150/2320 [02:11<02:01,  9.63it/s{'loss': 0.1323, 'grad_norm': 0.21281830966472626, 'learning_rate': 5.376146788990826e-05, 'epoch': 4.956896551724138}                                        | 1150/2320 [02:11<02:01,  9.63it/s]
{'loss': 0.1323, 'grad_norm': 0.21281830966472626, 'learning_rate': 5.376146788990826e-05, 'epoch': 4.96}                                                                                        
{'loss': 0.1323, 'grad_norm': 0.21281830966472626, 'learning_rate': 5.376146788990826e-05, 'epoch': 4.96}                                                                                        
 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 1160/2320 [02:12<02:05,  9.27it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.90it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.90it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.08201836049556732, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.9090935065010874, 'eval_recall_macro': 0.8744398284659393, 'eval_f1_macro': 0.8876293043989238, 'eval_precision_weighted': 0.98539666412211, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.985344927400449, 'eval_f1': 0.985344927400449, 'eval_balanced_accuracy': 0.8744398284659393, 'eval_cohen_kappa': 0.9677433169966568, 'eval_matthews_corrcoef': 0.9677741887297805, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8745, 'eval_samples_per_second': 1059.976, 'eval_steps_per_second': 33.16, 'epoch': 5.0}
{'eval_loss': 0.08201836049556732, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.9090935065010874, 'eval_recall_macro': 0.8744398284659393, 'eval_f1_macro': 0.8876293043989238, 'eval_precision_weighted': 0.98539666412211, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.985344927400449, 'eval_f1': 0.985344927400449, 'eval_balanced_accuracy': 0.8744398284659393, 'eval_cohen_kappa': 0.9677433169966568, 'eval_matthews_corrcoef': 0.9677741887297805, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8745, 'eval_samples_per_second': 1059.976, 'eval_steps_per_second': 33.16, 'epoch': 5.0}                                   
{'eval_loss': 0.08201836049556732, 'eval_accuracy': 0.9859762675296656, 'eval_precision_micro': 0.9859762675296656, 'eval_recall_micro': 0.9859762675296656, 'eval_f1_micro': 0.9859762675296656, 'eval_precision_macro': 0.9090935065010874, 'eval_recall_macro': 0.8744398284659393, 'eval_f1_macro': 0.8876293043989238, 'eval_precision_weighted': 0.98539666412211, 'eval_recall_weighted': 0.9859762675296656, 'eval_f1_weighted': 0.985344927400449, 'eval_f1': 0.985344927400449, 'eval_balanced_accuracy': 0.8744398284659393, 'eval_cohen_kappa': 0.9677433169966568, 'eval_matthews_corrcoef': 0.9677741887297805, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8745, 'eval_samples_per_second': 1059.976, 'eval_steps_per_second': 33.16, 'epoch': 5.0}                                   
 51%|████████████████████████████████████████████████████████████████████████████▉                                                                           | 1175/2320 [02:15<02:10,  8.78it/s{'loss': 0.1882, 'grad_norm': 1.025784969329834, 'learning_rate': 5.261467889908257e-05, 'epoch': 5.064655172413793}                                          | 1175/2320 [02:15<02:10,  8.77it/s]
{'loss': 0.1882, 'grad_norm': 1.025784969329834, 'learning_rate': 5.261467889908257e-05, 'epoch': 5.06}                                                                                          
{'loss': 0.1882, 'grad_norm': 1.025784969329834, 'learning_rate': 5.261467889908257e-05, 'epoch': 5.06}                                                                                          
 52%|██████████████████████████████████████████████████████████████████████████████▌                                                                         | 1200/2320 [02:17<01:56,  9.61it/s{'loss': 0.1461, 'grad_norm': 1.0720069408416748, 'learning_rate': 5.146788990825688e-05, 'epoch': 5.172413793103448}                                         | 1200/2320 [02:17<01:56,  9.61it/s]
{'loss': 0.1461, 'grad_norm': 1.0720069408416748, 'learning_rate': 5.146788990825688e-05, 'epoch': 5.17}                                                                                         
{'loss': 0.1461, 'grad_norm': 1.0720069408416748, 'learning_rate': 5.146788990825688e-05, 'epoch': 5.17}                                                                                         
 53%|████████████████████████████████████████████████████████████████████████████████▎                                                                       | 1225/2320 [02:20<02:00,  9.08it/s{'loss': 0.113, 'grad_norm': 1.7990145683288574, 'learning_rate': 5.03211009174312e-05, 'epoch': 5.280172413793103}                                           | 1225/2320 [02:20<02:00,  9.08it/s]
{'loss': 0.113, 'grad_norm': 1.7990145683288574, 'learning_rate': 5.03211009174312e-05, 'epoch': 5.28}                                                                                           
{'loss': 0.113, 'grad_norm': 1.7990145683288574, 'learning_rate': 5.03211009174312e-05, 'epoch': 5.28}                                                                                           
 54%|█████████████████████████████████████████████████████████████████████████████████▉                                                                      | 1250/2320 [02:23<01:55,  9.24it/s{'loss': 0.1391, 'grad_norm': 1.8956949710845947, 'learning_rate': 4.917431192660551e-05, 'epoch': 5.387931034482759}                                         | 1250/2320 [02:23<01:55,  9.24it/s]
{'loss': 0.1391, 'grad_norm': 1.8956949710845947, 'learning_rate': 4.917431192660551e-05, 'epoch': 5.39}                                                                                         
{'loss': 0.1391, 'grad_norm': 1.8956949710845947, 'learning_rate': 4.917431192660551e-05, 'epoch': 5.39}                                                                                         
 55%|███████████████████████████████████████████████████████████████████████████████████▌                                                                    | 1275/2320 [02:25<01:53,  9.20it/s{'loss': 0.104, 'grad_norm': 0.7871352434158325, 'learning_rate': 4.8027522935779815e-05, 'epoch': 5.495689655172414}                                         | 1275/2320 [02:25<01:53,  9.20it/s]
{'loss': 0.104, 'grad_norm': 0.7871352434158325, 'learning_rate': 4.8027522935779815e-05, 'epoch': 5.5}                                                                                          
{'loss': 0.104, 'grad_norm': 0.7871352434158325, 'learning_rate': 4.8027522935779815e-05, 'epoch': 5.5}                                                                                          
 56%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 1300/2320 [02:28<01:48,  9.43it/s{'loss': 0.1474, 'grad_norm': 0.29952681064605713, 'learning_rate': 4.6880733944954134e-05, 'epoch': 5.603448275862069}                                       | 1300/2320 [02:28<01:48,  9.43it/s]
{'loss': 0.1474, 'grad_norm': 0.29952681064605713, 'learning_rate': 4.6880733944954134e-05, 'epoch': 5.6}                                                                                        
{'loss': 0.1474, 'grad_norm': 0.29952681064605713, 'learning_rate': 4.6880733944954134e-05, 'epoch': 5.6}                                                                                        
 57%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 1325/2320 [02:31<01:47,  9.25it/s{'loss': 0.1337, 'grad_norm': 2.2285029888153076, 'learning_rate': 4.5733944954128445e-05, 'epoch': 5.711206896551724}                                        | 1325/2320 [02:31<01:47,  9.25it/s]
{'loss': 0.1337, 'grad_norm': 2.2285029888153076, 'learning_rate': 4.5733944954128445e-05, 'epoch': 5.71}                                                                                        
{'loss': 0.1337, 'grad_norm': 2.2285029888153076, 'learning_rate': 4.5733944954128445e-05, 'epoch': 5.71}                                                                                        
 58%|████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 1350/2320 [02:33<01:46,  9.09it/s{'loss': 0.1109, 'grad_norm': 0.26978495717048645, 'learning_rate': 4.458715596330276e-05, 'epoch': 5.818965517241379}                                        | 1350/2320 [02:33<01:46,  9.09it/s]
{'loss': 0.1109, 'grad_norm': 0.26978495717048645, 'learning_rate': 4.458715596330276e-05, 'epoch': 5.82}                                                                                        
{'loss': 0.1109, 'grad_norm': 0.26978495717048645, 'learning_rate': 4.458715596330276e-05, 'epoch': 5.82}                                                                                        
 59%|██████████████████████████████████████████████████████████████████████████████████████████                                                              | 1375/2320 [02:36<01:41,  9.29it/s{'loss': 0.1447, 'grad_norm': 1.7870559692382812, 'learning_rate': 4.344036697247706e-05, 'epoch': 5.926724137931035}                                         | 1375/2320 [02:36<01:41,  9.29it/s]
{'loss': 0.1447, 'grad_norm': 1.7870559692382812, 'learning_rate': 4.344036697247706e-05, 'epoch': 5.93}                                                                                         
{'loss': 0.1447, 'grad_norm': 1.7870559692382812, 'learning_rate': 4.344036697247706e-05, 'epoch': 5.93}                                                                                         
 60%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 1392/2320 [02:38<01:39,  9.34it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.90it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.90it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07645971328020096, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.9170770953213153, 'eval_recall_macro': 0.8870304081760843, 'eval_f1_macro': 0.8984590169886065, 'eval_precision_weighted': 0.9874972293180756, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875140407551637, 'eval_f1': 0.9875140407551637, 'eval_balanced_accuracy': 0.8870304081760843, 'eval_cohen_kappa': 0.9726582400669266, 'eval_matthews_corrcoef': 0.9726801549317026, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8768, 'eval_samples_per_second': 1057.202, 'eval_steps_per_second': 33.073, 'epoch': 6.0}
{'eval_loss': 0.07645971328020096, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.9170770953213153, 'eval_recall_macro': 0.8870304081760843, 'eval_f1_macro': 0.8984590169886065, 'eval_precision_weighted': 0.9874972293180756, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875140407551637, 'eval_f1': 0.9875140407551637, 'eval_balanced_accuracy': 0.8870304081760843, 'eval_cohen_kappa': 0.9726582400669266, 'eval_matthews_corrcoef': 0.9726801549317026, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8768, 'eval_samples_per_second': 1057.202, 'eval_steps_per_second': 33.073, 'epoch': 6.0}                              
{'eval_loss': 0.07645971328020096, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.9170770953213153, 'eval_recall_macro': 0.8870304081760843, 'eval_f1_macro': 0.8984590169886065, 'eval_precision_weighted': 0.9874972293180756, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9875140407551637, 'eval_f1': 0.9875140407551637, 'eval_balanced_accuracy': 0.8870304081760843, 'eval_cohen_kappa': 0.9726582400669266, 'eval_matthews_corrcoef': 0.9726801549317026, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8768, 'eval_samples_per_second': 1057.202, 'eval_steps_per_second': 33.073, 'epoch': 6.0}                              
 60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 1400/2320 [02:40<02:34,  5.97it/s{'loss': 0.1093, 'grad_norm': 0.2717888057231903, 'learning_rate': 4.229357798165138e-05, 'epoch': 6.0344827586206895}                                        | 1400/2320 [02:40<02:34,  5.97it/s]
{'loss': 0.1093, 'grad_norm': 0.2717888057231903, 'learning_rate': 4.229357798165138e-05, 'epoch': 6.03}                                                                                         
{'loss': 0.1093, 'grad_norm': 0.2717888057231903, 'learning_rate': 4.229357798165138e-05, 'epoch': 6.03}                                                                                         
 61%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 1425/2320 [02:43<01:36,  9.27it/s{'loss': 0.1034, 'grad_norm': 2.3412516117095947, 'learning_rate': 4.114678899082569e-05, 'epoch': 6.142241379310345}                                         | 1425/2320 [02:43<01:36,  9.27it/s]
{'loss': 0.1034, 'grad_norm': 2.3412516117095947, 'learning_rate': 4.114678899082569e-05, 'epoch': 6.14}                                                                                         
{'loss': 0.1034, 'grad_norm': 2.3412516117095947, 'learning_rate': 4.114678899082569e-05, 'epoch': 6.14}                                                                                         
 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 1450/2320 [02:46<01:32,  9.36it/s{'loss': 0.2083, 'grad_norm': 1.8705120086669922, 'learning_rate': 4e-05, 'epoch': 6.25}█████████████                                                         | 1450/2320 [02:46<01:32,  9.36it/s]
{'loss': 0.2083, 'grad_norm': 1.8705120086669922, 'learning_rate': 4e-05, 'epoch': 6.25}                                                                                                         
{'loss': 0.2083, 'grad_norm': 1.8705120086669922, 'learning_rate': 4e-05, 'epoch': 6.25}                                                                                                         
 64%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 1475/2320 [02:48<01:32,  9.17it/s{'loss': 0.1362, 'grad_norm': 1.0265142917633057, 'learning_rate': 3.8853211009174315e-05, 'epoch': 6.357758620689655}                                        | 1475/2320 [02:48<01:32,  9.17it/s]
{'loss': 0.1362, 'grad_norm': 1.0265142917633057, 'learning_rate': 3.8853211009174315e-05, 'epoch': 6.36}                                                                                        
{'loss': 0.1362, 'grad_norm': 1.0265142917633057, 'learning_rate': 3.8853211009174315e-05, 'epoch': 6.36}                                                                                        
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 1500/2320 [02:51<01:26,  9.45it/s{'loss': 0.0777, 'grad_norm': 0.9082285165786743, 'learning_rate': 3.7706422018348626e-05, 'epoch': 6.4655172413793105}                                       | 1500/2320 [02:51<01:26,  9.45it/s]
{'loss': 0.0777, 'grad_norm': 0.9082285165786743, 'learning_rate': 3.7706422018348626e-05, 'epoch': 6.47}                                                                                        
{'loss': 0.0777, 'grad_norm': 0.9082285165786743, 'learning_rate': 3.7706422018348626e-05, 'epoch': 6.47}                                                                                        
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                    | 1525/2320 [02:54<01:26,  9.20it/s{'loss': 0.0918, 'grad_norm': 0.38055771589279175, 'learning_rate': 3.655963302752294e-05, 'epoch': 6.573275862068965}                                        | 1525/2320 [02:54<01:26,  9.20it/s]
{'loss': 0.0918, 'grad_norm': 0.38055771589279175, 'learning_rate': 3.655963302752294e-05, 'epoch': 6.57}                                                                                        
{'loss': 0.0918, 'grad_norm': 0.38055771589279175, 'learning_rate': 3.655963302752294e-05, 'epoch': 6.57}                                                                                        
 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 1550/2320 [02:56<01:21,  9.42it/s{'loss': 0.0527, 'grad_norm': 0.4975675344467163, 'learning_rate': 3.541284403669725e-05, 'epoch': 6.681034482758621}                                         | 1550/2320 [02:56<01:21,  9.42it/s]
{'loss': 0.0527, 'grad_norm': 0.4975675344467163, 'learning_rate': 3.541284403669725e-05, 'epoch': 6.68}                                                                                         
{'loss': 0.0527, 'grad_norm': 0.4975675344467163, 'learning_rate': 3.541284403669725e-05, 'epoch': 6.68}                                                                                         
 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 1575/2320 [02:59<01:21,  9.17it/s{'loss': 0.0991, 'grad_norm': 1.069494605064392, 'learning_rate': 3.426605504587156e-05, 'epoch': 6.788793103448276}                                          | 1575/2320 [02:59<01:21,  9.17it/s]
{'loss': 0.0991, 'grad_norm': 1.069494605064392, 'learning_rate': 3.426605504587156e-05, 'epoch': 6.79}                                                                                          
{'loss': 0.0991, 'grad_norm': 1.069494605064392, 'learning_rate': 3.426605504587156e-05, 'epoch': 6.79}                                                                                          
 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 1600/2320 [03:02<01:17,  9.26it/s{'loss': 0.094, 'grad_norm': 1.383640170097351, 'learning_rate': 3.311926605504587e-05, 'epoch': 6.896551724137931}                                           | 1600/2320 [03:02<01:17,  9.25it/s]
{'loss': 0.094, 'grad_norm': 1.383640170097351, 'learning_rate': 3.311926605504587e-05, 'epoch': 6.9}                                                                                            
{'loss': 0.094, 'grad_norm': 1.383640170097351, 'learning_rate': 3.311926605504587e-05, 'epoch': 6.9}                                                                                            
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                             | 1623/2320 [03:04<01:13,  9.47it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.83it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.83it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07599655538797379, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8727, 'eval_samples_per_second': 1062.191, 'eval_steps_per_second': 33.229, 'epoch': 7.0}
{'eval_loss': 0.07599655538797379, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8727, 'eval_samples_per_second': 1062.191, 'eval_steps_per_second': 33.229, 'epoch': 7.0}                               
{'eval_loss': 0.07599655538797379, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8727, 'eval_samples_per_second': 1062.191, 'eval_steps_per_second': 33.229, 'epoch': 7.0}                               
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1625/2320 [03:06<04:48,  2.41it/s{'loss': 0.0958, 'grad_norm': 0.5748868584632874, 'learning_rate': 3.1972477064220184e-05, 'epoch': 7.004310344827586}                                        | 1625/2320 [03:06<04:48,  2.41it/s]
{'loss': 0.0958, 'grad_norm': 0.5748868584632874, 'learning_rate': 3.1972477064220184e-05, 'epoch': 7.0}                                                                                         
{'loss': 0.0958, 'grad_norm': 0.5748868584632874, 'learning_rate': 3.1972477064220184e-05, 'epoch': 7.0}                                                                                         
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 1649/2320 [03:08<01:12,  9.31it/s{'loss': 0.1231, 'grad_norm': 1.0975604057312012, 'learning_rate': 3.0825688073394496e-05, 'epoch': 7.112068965517241}                                        | 1649/2320 [03:08<01:12,  9.32it/s]
{'loss': 0.1231, 'grad_norm': 1.0975604057312012, 'learning_rate': 3.0825688073394496e-05, 'epoch': 7.11}                                                                                        
{'loss': 0.1231, 'grad_norm': 1.0975604057312012, 'learning_rate': 3.0825688073394496e-05, 'epoch': 7.11}                                                                                        
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 1675/2320 [03:11<01:11,  9.06it/s{'loss': 0.066, 'grad_norm': 0.1137687936425209, 'learning_rate': 2.9678899082568808e-05, 'epoch': 7.219827586206897}                                         | 1675/2320 [03:11<01:11,  9.06it/s]
{'loss': 0.066, 'grad_norm': 0.1137687936425209, 'learning_rate': 2.9678899082568808e-05, 'epoch': 7.22}                                                                                         
{'loss': 0.066, 'grad_norm': 0.1137687936425209, 'learning_rate': 2.9678899082568808e-05, 'epoch': 7.22}                                                                                         
 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 1700/2320 [03:14<01:04,  9.55it/s{'loss': 0.0885, 'grad_norm': 0.17566406726837158, 'learning_rate': 2.853211009174312e-05, 'epoch': 7.327586206896552}                                        | 1700/2320 [03:14<01:04,  9.55it/s]
{'loss': 0.0885, 'grad_norm': 0.17566406726837158, 'learning_rate': 2.853211009174312e-05, 'epoch': 7.33}                                                                                        
{'loss': 0.0885, 'grad_norm': 0.17566406726837158, 'learning_rate': 2.853211009174312e-05, 'epoch': 7.33}                                                                                        
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 1725/2320 [03:16<01:05,  9.10it/s{'loss': 0.1059, 'grad_norm': 0.9481992125511169, 'learning_rate': 2.7385321100917434e-05, 'epoch': 7.435344827586207}█                                       | 1725/2320 [03:16<01:05,  9.10it/s]
{'loss': 0.1059, 'grad_norm': 0.9481992125511169, 'learning_rate': 2.7385321100917434e-05, 'epoch': 7.44}                                                                                        
{'loss': 0.1059, 'grad_norm': 0.9481992125511169, 'learning_rate': 2.7385321100917434e-05, 'epoch': 7.44}                                                                                        
 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 1750/2320 [03:19<01:02,  9.19it/s{'loss': 0.075, 'grad_norm': 0.8980851769447327, 'learning_rate': 2.6238532110091746e-05, 'epoch': 7.543103448275862}███▋                                     | 1750/2320 [03:19<01:02,  9.19it/s]
{'loss': 0.075, 'grad_norm': 0.8980851769447327, 'learning_rate': 2.6238532110091746e-05, 'epoch': 7.54}                                                                                         
{'loss': 0.075, 'grad_norm': 0.8980851769447327, 'learning_rate': 2.6238532110091746e-05, 'epoch': 7.54}                                                                                         
 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 1775/2320 [03:22<00:56,  9.63it/s{'loss': 0.1449, 'grad_norm': 0.45109647512435913, 'learning_rate': 2.5091743119266054e-05, 'epoch': 7.650862068965517}███▎                                   | 1775/2320 [03:22<00:56,  9.63it/s]
{'loss': 0.1449, 'grad_norm': 0.45109647512435913, 'learning_rate': 2.5091743119266054e-05, 'epoch': 7.65}                                                                                       
{'loss': 0.1449, 'grad_norm': 0.45109647512435913, 'learning_rate': 2.5091743119266054e-05, 'epoch': 7.65}                                                                                       
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 1800/2320 [03:24<00:54,  9.56it/s{'loss': 0.0869, 'grad_norm': 0.7499722838401794, 'learning_rate': 2.394495412844037e-05, 'epoch': 7.758620689655173}██████▉                                  | 1800/2320 [03:24<00:54,  9.56it/s]
{'loss': 0.0869, 'grad_norm': 0.7499722838401794, 'learning_rate': 2.394495412844037e-05, 'epoch': 7.76}                                                                                         
{'loss': 0.0869, 'grad_norm': 0.7499722838401794, 'learning_rate': 2.394495412844037e-05, 'epoch': 7.76}                                                                                         
 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                | 1825/2320 [03:27<00:55,  9.00it/s{'loss': 0.1315, 'grad_norm': 0.35910043120384216, 'learning_rate': 2.279816513761468e-05, 'epoch': 7.866379310344827}███████▌                                | 1825/2320 [03:27<00:55,  9.00it/s]
{'loss': 0.1315, 'grad_norm': 0.35910043120384216, 'learning_rate': 2.279816513761468e-05, 'epoch': 7.87}                                                                                        
{'loss': 0.1315, 'grad_norm': 0.35910043120384216, 'learning_rate': 2.279816513761468e-05, 'epoch': 7.87}                                                                                        
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 1850/2320 [03:30<00:50,  9.23it/s{'loss': 0.1157, 'grad_norm': 1.3131976127624512, 'learning_rate': 2.1651376146788992e-05, 'epoch': 7.974137931034483}█████████▏                              | 1850/2320 [03:30<00:50,  9.22it/s]
{'loss': 0.1157, 'grad_norm': 1.3131976127624512, 'learning_rate': 2.1651376146788992e-05, 'epoch': 7.97}                                                                                        
{'loss': 0.1157, 'grad_norm': 1.3131976127624512, 'learning_rate': 2.1651376146788992e-05, 'epoch': 7.97}                                                                                        
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 1856/2320 [03:30<00:49,  9.46it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.87it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.87it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07592456787824631, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.288, 'eval_steps_per_second': 33.232, 'epoch': 8.0}
{'eval_loss': 0.07592456787824631, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.288, 'eval_steps_per_second': 33.232, 'epoch': 8.0}                               
{'eval_loss': 0.07592456787824631, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.288, 'eval_steps_per_second': 33.232, 'epoch': 8.0}                               
 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 1875/2320 [03:34<00:49,  8.95it/s{'loss': 0.0763, 'grad_norm': 0.08712289482355118, 'learning_rate': 2.0504587155963304e-05, 'epoch': 8.081896551724139}█████████▊                             | 1875/2320 [03:34<00:49,  8.95it/s]
{'loss': 0.0763, 'grad_norm': 0.08712289482355118, 'learning_rate': 2.0504587155963304e-05, 'epoch': 8.08}                                                                                       
{'loss': 0.0763, 'grad_norm': 0.08712289482355118, 'learning_rate': 2.0504587155963304e-05, 'epoch': 8.08}                                                                                       
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 1900/2320 [03:37<00:45,  9.20it/s{'loss': 0.1135, 'grad_norm': 1.5037720203399658, 'learning_rate': 1.9357798165137615e-05, 'epoch': 8.189655172413794}████████████▍                           | 1900/2320 [03:37<00:45,  9.20it/s]
{'loss': 0.1135, 'grad_norm': 1.5037720203399658, 'learning_rate': 1.9357798165137615e-05, 'epoch': 8.19}                                                                                        
{'loss': 0.1135, 'grad_norm': 1.5037720203399658, 'learning_rate': 1.9357798165137615e-05, 'epoch': 8.19}                                                                                        
 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 1925/2320 [03:40<00:42,  9.28it/s{'loss': 0.0727, 'grad_norm': 1.946158528327942, 'learning_rate': 1.8211009174311927e-05, 'epoch': 8.297413793103448}███████████████                          | 1925/2320 [03:40<00:42,  9.27it/s]
{'loss': 0.0727, 'grad_norm': 1.946158528327942, 'learning_rate': 1.8211009174311927e-05, 'epoch': 8.3}                                                                                          
{'loss': 0.0727, 'grad_norm': 1.946158528327942, 'learning_rate': 1.8211009174311927e-05, 'epoch': 8.3}                                                                                          
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 1950/2320 [03:42<00:38,  9.53it/s{'loss': 0.0694, 'grad_norm': 1.2114388942718506, 'learning_rate': 1.7064220183486242e-05, 'epoch': 8.405172413793103}███████████████▊                        | 1950/2320 [03:42<00:38,  9.53it/s]
{'loss': 0.0694, 'grad_norm': 1.2114388942718506, 'learning_rate': 1.7064220183486242e-05, 'epoch': 8.41}                                                                                        
{'loss': 0.0694, 'grad_norm': 1.2114388942718506, 'learning_rate': 1.7064220183486242e-05, 'epoch': 8.41}                                                                                        
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 1975/2320 [03:45<00:37,  9.16it/s{'loss': 0.05, 'grad_norm': 0.9698402285575867, 'learning_rate': 1.5963302752293578e-05, 'epoch': 8.512931034482758}███████████████████▍                      | 1975/2320 [03:45<00:37,  9.16it/s]
{'loss': 0.05, 'grad_norm': 0.9698402285575867, 'learning_rate': 1.5963302752293578e-05, 'epoch': 8.51}                                                                                          
{'loss': 0.05, 'grad_norm': 0.9698402285575867, 'learning_rate': 1.5963302752293578e-05, 'epoch': 8.51}                                                                                          
 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 2000/2320 [03:48<00:34,  9.15it/s{'loss': 0.0951, 'grad_norm': 0.44426780939102173, 'learning_rate': 1.481651376146789e-05, 'epoch': 8.620689655172415}███████████████████                     | 2000/2320 [03:48<00:34,  9.15it/s]
{'loss': 0.0951, 'grad_norm': 0.44426780939102173, 'learning_rate': 1.481651376146789e-05, 'epoch': 8.62}                                                                                        
{'loss': 0.0951, 'grad_norm': 0.44426780939102173, 'learning_rate': 1.481651376146789e-05, 'epoch': 8.62}                                                                                        
 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 2025/2320 [03:51<00:32,  9.12it/s{'loss': 0.1136, 'grad_norm': 0.5718989372253418, 'learning_rate': 1.3669724770642203e-05, 'epoch': 8.72844827586207}█████████████████████▋                   | 2025/2320 [03:51<00:32,  9.12it/s]
{'loss': 0.1136, 'grad_norm': 0.5718989372253418, 'learning_rate': 1.3669724770642203e-05, 'epoch': 8.73}                                                                                        
{'loss': 0.1136, 'grad_norm': 0.5718989372253418, 'learning_rate': 1.3669724770642203e-05, 'epoch': 8.73}                                                                                        
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 2050/2320 [03:53<00:29,  9.24it/s{'loss': 0.0774, 'grad_norm': 1.657057762145996, 'learning_rate': 1.2522935779816513e-05, 'epoch': 8.836206896551724}███████████████████████▎                 | 2050/2320 [03:53<00:29,  9.24it/s]
{'loss': 0.0774, 'grad_norm': 1.657057762145996, 'learning_rate': 1.2522935779816513e-05, 'epoch': 8.84}                                                                                         
{'loss': 0.0774, 'grad_norm': 1.657057762145996, 'learning_rate': 1.2522935779816513e-05, 'epoch': 8.84}                                                                                         
 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 2074/2320 [03:56<00:25,  9.48it/s{'loss': 0.1186, 'grad_norm': 0.5132780075073242, 'learning_rate': 1.1376146788990826e-05, 'epoch': 8.943965517241379}███████████████████████▉                | 2074/2320 [03:56<00:25,  9.48it/s]
{'loss': 0.1186, 'grad_norm': 0.5132780075073242, 'learning_rate': 1.1376146788990826e-05, 'epoch': 8.94}                                                                                        
{'loss': 0.1186, 'grad_norm': 0.5132780075073242, 'learning_rate': 1.1376146788990826e-05, 'epoch': 8.94}                                                                                        
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 2087/2320 [03:57<00:25,  9.26it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.78it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.78it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07836952805519104, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8729, 'eval_samples_per_second': 1061.974, 'eval_steps_per_second': 33.222, 'epoch': 9.0}
{'eval_loss': 0.07836952805519104, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8729, 'eval_samples_per_second': 1061.974, 'eval_steps_per_second': 33.222, 'epoch': 9.0}                              
{'eval_loss': 0.07836952805519104, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8729, 'eval_samples_per_second': 1061.974, 'eval_steps_per_second': 33.222, 'epoch': 9.0}                              
 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 2100/2320 [04:00<00:25,  8.55it/s{'loss': 0.0981, 'grad_norm': 1.4402124881744385, 'learning_rate': 1.022935779816514e-05, 'epoch': 9.051724137931034}██████████████████████████▌              | 2100/2320 [04:00<00:25,  8.55it/s]
{'loss': 0.0981, 'grad_norm': 1.4402124881744385, 'learning_rate': 1.022935779816514e-05, 'epoch': 9.05}                                                                                         
{'loss': 0.0981, 'grad_norm': 1.4402124881744385, 'learning_rate': 1.022935779816514e-05, 'epoch': 9.05}                                                                                         
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 2125/2320 [04:03<00:20,  9.29it/s{'loss': 0.1071, 'grad_norm': 0.3178022801876068, 'learning_rate': 9.082568807339451e-06, 'epoch': 9.15948275862069}█████████████████████████████▏            | 2125/2320 [04:03<00:20,  9.29it/s]
{'loss': 0.1071, 'grad_norm': 0.3178022801876068, 'learning_rate': 9.082568807339451e-06, 'epoch': 9.16}                                                                                         
{'loss': 0.1071, 'grad_norm': 0.3178022801876068, 'learning_rate': 9.082568807339451e-06, 'epoch': 9.16}                                                                                         
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 2149/2320 [04:05<00:18,  9.49it/s{'loss': 0.1153, 'grad_norm': 0.4698549807071686, 'learning_rate': 7.935779816513763e-06, 'epoch': 9.267241379310345}█████████████████████████████▊           | 2149/2320 [04:05<00:18,  9.49it/s]
{'loss': 0.1153, 'grad_norm': 0.4698549807071686, 'learning_rate': 7.935779816513763e-06, 'epoch': 9.27}                                                                                         
{'loss': 0.1153, 'grad_norm': 0.4698549807071686, 'learning_rate': 7.935779816513763e-06, 'epoch': 9.27}                                                                                         
 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 2175/2320 [04:08<00:15,  9.49it/s{'loss': 0.0809, 'grad_norm': 1.3454346656799316, 'learning_rate': 6.788990825688074e-06, 'epoch': 9.375}███████████████████████████████████████████▌         | 2175/2320 [04:08<00:15,  9.49it/s]
{'loss': 0.0809, 'grad_norm': 1.3454346656799316, 'learning_rate': 6.788990825688074e-06, 'epoch': 9.38}                                                                                         
{'loss': 0.0809, 'grad_norm': 1.3454346656799316, 'learning_rate': 6.788990825688074e-06, 'epoch': 9.38}                                                                                         
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 2200/2320 [04:10<00:12,  9.69it/s{'loss': 0.1557, 'grad_norm': 2.251117706298828, 'learning_rate': 5.642201834862386e-06, 'epoch': 9.482758620689655}██████████████████████████████████▏       | 2200/2320 [04:10<00:12,  9.69it/s]
{'loss': 0.1557, 'grad_norm': 2.251117706298828, 'learning_rate': 5.642201834862386e-06, 'epoch': 9.48}                                                                                          
{'loss': 0.1557, 'grad_norm': 2.251117706298828, 'learning_rate': 5.642201834862386e-06, 'epoch': 9.48}                                                                                          
 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2225/2320 [04:13<00:09,  9.51it/s{'loss': 0.1084, 'grad_norm': 0.47722434997558594, 'learning_rate': 4.4954128440366975e-06, 'epoch': 9.59051724137931}█████████████████████████████████▊      | 2225/2320 [04:13<00:09,  9.51it/s]
{'loss': 0.1084, 'grad_norm': 0.47722434997558594, 'learning_rate': 4.4954128440366975e-06, 'epoch': 9.59}                                                                                       
{'loss': 0.1084, 'grad_norm': 0.47722434997558594, 'learning_rate': 4.4954128440366975e-06, 'epoch': 9.59}                                                                                       
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 2250/2320 [04:16<00:07,  9.31it/s{'loss': 0.0647, 'grad_norm': 0.6885249614715576, 'learning_rate': 3.3486238532110095e-06, 'epoch': 9.698275862068966}███████████████████████████████████▍    | 2250/2320 [04:16<00:07,  9.31it/s]
{'loss': 0.0647, 'grad_norm': 0.6885249614715576, 'learning_rate': 3.3486238532110095e-06, 'epoch': 9.7}                                                                                         
{'loss': 0.0647, 'grad_norm': 0.6885249614715576, 'learning_rate': 3.3486238532110095e-06, 'epoch': 9.7}                                                                                         
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 2275/2320 [04:19<00:04,  9.39it/s{'loss': 0.048, 'grad_norm': 3.74078631401062, 'learning_rate': 2.2018348623853215e-06, 'epoch': 9.806034482758621}████████████████████████████████████████   | 2275/2320 [04:19<00:04,  9.39it/s]
{'loss': 0.048, 'grad_norm': 3.74078631401062, 'learning_rate': 2.2018348623853215e-06, 'epoch': 9.81}                                                                                           
{'loss': 0.048, 'grad_norm': 3.74078631401062, 'learning_rate': 2.2018348623853215e-06, 'epoch': 9.81}                                                                                           
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 2300/2320 [04:21<00:02,  9.22it/s{'loss': 0.0834, 'grad_norm': 0.5247811675071716, 'learning_rate': 1.055045871559633e-06, 'epoch': 9.913793103448276}███████████████████████████████████████▋ | 2300/2320 [04:21<00:02,  9.22it/s]
{'loss': 0.0834, 'grad_norm': 0.5247811675071716, 'learning_rate': 1.055045871559633e-06, 'epoch': 9.91}                                                                                         
{'loss': 0.0834, 'grad_norm': 0.5247811675071716, 'learning_rate': 1.055045871559633e-06, 'epoch': 9.91}                                                                                         
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:23<00:00,  9.36it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.84it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.84it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07752340286970139, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8576, 'eval_samples_per_second': 1080.923, 'eval_steps_per_second': 33.815, 'epoch': 10.0}
{'eval_loss': 0.07752340286970139, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8576, 'eval_samples_per_second': 1080.923, 'eval_steps_per_second': 33.815, 'epoch': 10.0}                             
{'eval_loss': 0.07752340286970139, 'eval_accuracy': 0.9881337648327939, 'eval_precision_micro': 0.9881337648327939, 'eval_recall_micro': 0.9881337648327939, 'eval_f1_micro': 0.9881337648327939, 'eval_precision_macro': 0.8899475788992774, 'eval_recall_macro': 0.9025648284659393, 'eval_f1_macro': 0.8886454214013781, 'eval_precision_weighted': 0.9878777860207076, 'eval_recall_weighted': 0.9881337648327939, 'eval_f1_weighted': 0.9877074304405641, 'eval_f1': 0.9877074304405641, 'eval_balanced_accuracy': 0.9025648284659393, 'eval_cohen_kappa': 0.9727093168898738, 'eval_matthews_corrcoef': 0.9727381474111542, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8576, 'eval_samples_per_second': 1080.923, 'eval_steps_per_second': 33.815, 'epoch': 10.0}                             
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:25<00:00,  9.36it/s{'train_runtime': 265.7693, 'train_samples_per_second': 139.482, 'train_steps_per_second': 8.729, 'train_loss': 0.3465902406593849, 'epoch': 10.0}████████████| 2320/2320 [04:25<00:00,  9.36it/s]
{'train_runtime': 265.7693, 'train_samples_per_second': 139.482, 'train_steps_per_second': 8.729, 'train_loss': 0.3465902406593849, 'epoch': 10.0}                                               
{'train_runtime': 265.7693, 'train_samples_per_second': 139.482, 'train_steps_per_second': 8.729, 'train_loss': 0.3465902406593849, 'epoch': 10.0}                                               
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:25<00:00,  8.73it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [04:25<00:00,  8.73it/s]
2025-04-26 11:48:51,451 - INFO - TrainMain - *** Training Finished ***
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.3466
  train_runtime            = 0:04:25.76
  train_samples_per_second =    139.482
  train_steps_per_second   =      8.729
2025-04-26 11:48:51,457 - INFO - TrainMain - Training metrics: {'train_runtime': 265.7693, 'train_samples_per_second': 139.482, 'train_steps_per_second': 8.729, 'train_loss': 0.3465902406593849, 'epoch': 10.0}
2025-04-26 11:48:51,457 - INFO - TrainMain - *** Starting Final Evaluation on Validation Set ***
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 40.61it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
{'eval_loss': 0.07599655538797379, 'eval_accuracy': 0.9892125134843581, 'eval_precision_micro': 0.9892125134843581, 'eval_recall_micro': 0.9892125134843581, 'eval_f1_micro': 0.9892125134843581, 'eval_precision_macro': 0.9211979744421944, 'eval_recall_macro': 0.9026554081760843, 'eval_f1_macro': 0.9095243405539302, 'eval_precision_weighted': 0.9884218710194164, 'eval_recall_weighted': 0.9892125134843581, 'eval_f1_weighted': 0.9886099124011972, 'eval_f1': 0.9886099124011972, 'eval_balanced_accuracy': 0.9026554081760843, 'eval_cohen_kappa': 0.9751444544247966, 'eval_matthews_corrcoef': 0.975163848764251, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.8246, 'eval_samples_per_second': 1124.171, 'eval_steps_per_second': 35.168, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 36.50it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 36.51it/s]
***** eval metrics *****
  eval_epoch                   =       10.0
  eval_eval_accuracy           =     0.9892
  eval_eval_balanced_accuracy  =     0.9027
  eval_eval_cohen_kappa        =     0.9751
  eval_eval_f1                 =     0.9886
  eval_eval_f1_macro           =     0.9095
  eval_eval_f1_micro           =     0.9892
  eval_eval_f1_weighted        =     0.9886
  eval_eval_loss               =      0.076
  eval_eval_matthews_corrcoef  =     0.9752
  eval_eval_precision_macro    =     0.9212
  eval_eval_precision_micro    =     0.9892
  eval_eval_precision_weighted =     0.9884
  eval_eval_recall_macro       =     0.9027
  eval_eval_recall_micro       =     0.9892
  eval_eval_recall_weighted    =     0.9892
  eval_eval_roc_auc_ovr        =        nan
  eval_eval_runtime            = 0:00:00.82
  eval_eval_samples_per_second =   1124.171
  eval_eval_steps_per_second   =     35.168
2025-04-26 11:48:52,289 - INFO - TrainMain - Final validation metrics: {'eval_eval_loss': 0.07599655538797379, 'eval_eval_accuracy': 0.9892125134843581, 'eval_eval_precision_micro': 0.9892125134843581, 'eval_eval_recall_micro': 0.9892125134843581, 'eval_eval_f1_micro': 0.9892125134843581, 'eval_eval_precision_macro': 0.9211979744421944, 'eval_eval_recall_macro': 0.9026554081760843, 'eval_eval_f1_macro': 0.9095243405539302, 'eval_eval_precision_weighted': 0.9884218710194164, 'eval_eval_recall_weighted': 0.9892125134843581, 'eval_eval_f1_weighted': 0.9886099124011972, 'eval_eval_f1': 0.9886099124011972, 'eval_eval_balanced_accuracy': 0.9026554081760843, 'eval_eval_cohen_kappa': 0.9751444544247966, 'eval_eval_matthews_corrcoef': 0.975163848764251, 'eval_eval_roc_auc_ovr': nan, 'eval_eval_runtime': 0.8246, 'eval_eval_samples_per_second': 1124.171, 'eval_eval_steps_per_second': 35.168, 'eval_epoch': 10.0}
2025-04-26 11:48:52,289 - INFO - TrainMain - *** Starting Test Set Evaluation ***
2025-04-26 11:48:52,289 - INFO - TrainMain - Loading test dataset from: ./coding_task/data/atis/test.tsv
2025-04-26 11:48:52,289 - INFO - DataProcessor - DataProcessor initialized with precomputed label mappings.
2025-04-26 11:48:52,289 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/test.tsv using Pandas
2025-04-26 11:48:52,289 - INFO - Data Utils - File extension: .tsv
2025-04-26 11:48:52,289 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 11:48:52,290 - INFO - Data Utils -   file_path: ./coding_task/data/atis/test.tsv
2025-04-26 11:48:52,290 - INFO - Data Utils -   file_type: tsv
2025-04-26 11:48:52,290 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 11:48:52,290 - INFO - Data Utils -   use_dask: False
2025-04-26 11:48:52,290 - INFO - Data Utils -   unpack_multi_labels: False
2025-04-26 11:48:52,290 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:48:52,293 - INFO - Data Utils - Loaded initial Pandas df with 850 rows and 2 columns.
2025-04-26 11:48:52,293 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:48:52,293 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:48:52,294 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:48:52,295 - INFO - Data Utils - Resetting index after cleanup. New shape: (850, 2)
2025-04-26 11:48:52,296 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (850, 2)
2025-04-26 11:48:52,299 - INFO - DataProcessor - Preparing labels for task type: multiclass
2025-04-26 11:48:52,299 - INFO - DataProcessor - Using precomputed 22 labels for multiclass task.
2025-04-26 11:48:52,301 - WARNING - DataProcessor - Labels found in data but not in precomputed label_map: ['day_name' 'airfare+flight' 'flight+airline' 'flight_no+airline']. These rows will have NaN labels.
2025-04-26 11:48:52,302 - WARNING - DataProcessor - NaN values found in the 'labels' column after processing. Check data and label mapping.
2025-04-26 11:48:52,303 - INFO - DataProcessor - Using full dataset for training (validation_split_ratio=0).
2025-04-26 11:48:52,304 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:48:52,311 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 845/845 [00:00<00:00, 31269.29 examples/s]
2025-04-26 11:48:52,365 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:48:52,366 - INFO - TrainMain - *** Starting Test Set Evaluation ***
 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                 | 24/27 [00:00<00:00, 51.37it/s{'eval_loss': 0.1058998629450798, 'eval_accuracy': 0.9727810650887574, 'eval_precision_micro': 0.9727810650887574, 'eval_recall_micro': 0.9727810650887574, 'eval_f1_micro': 0.9727810650887574, 'eval_precision_macro': 0.9239884922978774, 'eval_recall_macro': 0.9037974345529403, 'eval_f1_macro': 0.8921447382683866, 'eval_precision_weighted': 0.9751582119619254, 'eval_recall_weighted': 0.9727810650887574, 'eval_f1_weighted': 0.9708477858716882, 'eval_f1': 0.9708477858716882, 'eval_balanced_accuracy': 0.9037974345529403, 'eval_cohen_kappa': 0.9414423236264481, 'eval_matthews_corrcoef': 0.9416412346018491, 'eval_roc_auc_ovr': nan, 'eval_runtime': 0.6515, 'eval_samples_per_second': 1297.084, 'eval_steps_per_second': 41.445, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 42.99it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 42.99it/s]
***** test metrics *****
  test_epoch                   =       10.0
  test_eval_accuracy           =     0.9728
  test_eval_balanced_accuracy  =     0.9038
  test_eval_cohen_kappa        =     0.9414
  test_eval_f1                 =     0.9708
  test_eval_f1_macro           =     0.8921
  test_eval_f1_micro           =     0.9728
  test_eval_f1_weighted        =     0.9708
  test_eval_loss               =     0.1059
  test_eval_matthews_corrcoef  =     0.9416
  test_eval_precision_macro    =      0.924
  test_eval_precision_micro    =     0.9728
  test_eval_precision_weighted =     0.9752
  test_eval_recall_macro       =     0.9038
  test_eval_recall_micro       =     0.9728
  test_eval_recall_weighted    =     0.9728
  test_eval_roc_auc_ovr        =        nan
  test_eval_runtime            = 0:00:00.65
  test_eval_samples_per_second =   1297.084
  test_eval_steps_per_second   =     41.445
2025-04-26 11:48:53,024 - INFO - TrainMain - Test set evaluation metrics: {'test_eval_loss': 0.1058998629450798, 'test_eval_accuracy': 0.9727810650887574, 'test_eval_precision_micro': 0.9727810650887574, 'test_eval_recall_micro': 0.9727810650887574, 'test_eval_f1_micro': 0.9727810650887574, 'test_eval_precision_macro': 0.9239884922978774, 'test_eval_recall_macro': 0.9037974345529403, 'test_eval_f1_macro': 0.8921447382683866, 'test_eval_precision_weighted': 0.9751582119619254, 'test_eval_recall_weighted': 0.9727810650887574, 'test_eval_f1_weighted': 0.9708477858716882, 'test_eval_f1': 0.9708477858716882, 'test_eval_balanced_accuracy': 0.9037974345529403, 'test_eval_cohen_kappa': 0.9414423236264481, 'test_eval_matthews_corrcoef': 0.9416412346018491, 'test_eval_roc_auc_ovr': nan, 'test_eval_runtime': 0.6515, 'test_eval_samples_per_second': 1297.084, 'test_eval_steps_per_second': 41.445, 'test_epoch': 10.0}
2025-04-26 11:48:53,025 - INFO - TrainMain - *** Evaluation Finished ***
2025-04-26 11:48:53,025 - INFO - TrainMain - Saving final model/adapter to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:48:53,476 - INFO - TrainMain - Tokenizer saved to ./results/atis_multiclass_xlmr_lora
2025-04-26 11:48:53,476 - ERROR - TrainMain - Failed to save training arguments: 'TrainingConfig' object has no attribute 'to_dict'
2025-04-26 11:48:53,476 - INFO - TrainMain - Script finished successfully.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
