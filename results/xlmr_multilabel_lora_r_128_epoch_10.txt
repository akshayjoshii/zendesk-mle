2025-04-26 11:58:03,181 - INFO - Data Utils -   label_column_name: atis_labels
2025-04-26 11:58:03,181 - INFO - Data Utils -   label_delimiter: +
2025-04-26 11:58:03,181 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 11:58:03,188 - INFO - Data Utils - Loaded initial Pandas df with 4634 rows and 2 columns.
2025-04-26 11:58:03,188 - INFO - Data Utils - Unpacking multi-labels in column 'atis_labels' using delimiter '+'.
2025-04-26 11:58:03,195 - INFO - Data Utils - Unpacking complete. Initial rows: 4634, Final rows: 4657.
2025-04-26 11:58:03,200 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 11:58:03,200 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 11:58:03,204 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 11:58:03,207 - INFO - Data Utils - Resetting index after cleanup. New shape: (4657, 2)
2025-04-26 11:58:03,207 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (4657, 2)
2025-04-26 11:58:03,210 - INFO - DataProcessor - Preparing labels for task type: multilabel
2025-04-26 11:58:03,210 - INFO - DataProcessor - Grouping unpacked labels by text to prepare for multi-hot encoding...
2025-04-26 11:58:03,294 - INFO - DataProcessor - Calculated 17 unique individual labels for multilabel task.
2025-04-26 11:58:03,314 - INFO - DataProcessor - Reconstructed DataFrame shape for multilabel: (4634, 2)
2025-04-26 11:58:03,319 - INFO - DataProcessor - Splitting data into train/validation (0.8/0.2)
2025-04-26 11:58:03,320 - INFO - DataProcessor - Train size: 3707, Validation size: 927
2025-04-26 11:58:03,320 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 11:58:03,364 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3707/3707 [00:00<00:00, 22854.17 examples/s]
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 927/927 [00:00<00:00, 16312.99 examples/s]
2025-04-26 11:58:03,664 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 11:58:03,698 - INFO - TrainMain - Data processing complete. Number of labels: 17
2025-04-26 11:58:03,699 - INFO - TrainMain - Label mapping (id2label): {0: 'abbreviation', 1: 'aircraft', 2: 'airfare', 3: 'airline', 4: 'airport', 5: 'capacity', 6: 'cheapest', 7: 'city', 8: 'distance', 9: 'flight', 10: 'flight_no', 11: 'flight_time', 12: 'ground_fare', 13: 'ground_service', 14: 'meal', 15: 'quantity', 16: 'restriction'}
2025-04-26 11:58:03,700 - INFO - TrainMain - Label mappings saved to ./results/atis_multilabel_xlmr_lora
2025-04-26 11:58:03,700 - INFO - TrainMain - Loading model for training...
2025-04-26 11:58:03,700 - INFO - ModelLoader - Loading base model: xlm-roberta-base
2025-04-26 11:58:03,700 - INFO - ModelLoader - Configuring model for 'multi_label_classification' (num_labels=17).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-04-26 11:58:06,325 - INFO - ModelLoader - Freezing base model parameters.
2025-04-26 11:58:06,326 - INFO - ModelLoader - Applying PEFT method: lora
2025-04-26 11:58:06,326 - INFO - ModelLoader - lora_target_modules not specified, attempting auto-detection by PEFT library.
2025-04-26 11:58:06,326 - INFO - ModelLoader - LoRA Config: r=128, alpha=16, dropout=0.1, target_modules=auto
2025-04-26 11:58:06,638 - INFO - ModelLoader - PEFT model created successfully.
trainable params: 5,322,257 || all params: 283,378,978 || trainable%: 1.8781
2025-04-26 11:58:06,640 - INFO - TrainMain - Model loading complete.
2025-04-26 11:58:06,640 - INFO - TrainMain - Setting up Trainer...
2025-04-26 11:58:06,640 - INFO - TrainerSetup - Configuring HuggingFace Trainer...
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-04-26 11:58:06,785 - INFO - TrainerSetup - Adding EarlyStoppingCallback with patience=3 based on 'eval_f1_micro'.
/home/fe/gururaj/LRP_Experiment/zendesk-mle-master/coding_task/train/trainer.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2025-04-26 11:58:07,395] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
You are adding a <class 'transformers.trainer_callback.ProgressCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
TensorBoardCallback
PrinterCallback
ProgressCallback
EarlyStoppingCallback
2025-04-26 11:58:10,396 - INFO - TrainerSetup - Trainer configured.
2025-04-26 11:58:10,396 - INFO - TrainMain - Trainer setup complete.
2025-04-26 11:58:10,396 - INFO - TrainMain - *** Starting Training ***
  1%|█▋                                                                                                                                                        | 25/2320 [00:03<04:08,  9.25it/s{'loss': 0.6573, 'grad_norm': 1.160423994064331, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.10775862068965517}                                          | 25/2320 [00:03<04:08,  9.25it/s]
{'loss': 0.6573, 'grad_norm': 1.160423994064331, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.11}                                                                                          
{'loss': 0.6573, 'grad_norm': 1.160423994064331, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.11}                                                                                          
  2%|███▎                                                                                                                                                      | 50/2320 [00:05<04:06,  9.22it/s{'loss': 0.5814, 'grad_norm': 1.185530185699463, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.21551724137931033}                                          | 50/2320 [00:05<04:06,  9.21it/s]
{'loss': 0.5814, 'grad_norm': 1.185530185699463, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.22}                                                                                          
{'loss': 0.5814, 'grad_norm': 1.185530185699463, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.22}                                                                                          
  3%|████▉                                                                                                                                                     | 75/2320 [00:08<04:00,  9.32it/s{'loss': 0.4196, 'grad_norm': 0.6153164505958557, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.3232758620689655}                                         | 75/2320 [00:08<04:00,  9.32it/s]
{'loss': 0.4196, 'grad_norm': 0.6153164505958557, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.32}                                                                                        
{'loss': 0.4196, 'grad_norm': 0.6153164505958557, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.32}                                                                                        
  4%|██████▌                                                                                                                                                  | 100/2320 [00:11<03:59,  9.25it/s{'loss': 0.1919, 'grad_norm': 0.36828041076660156, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43103448275862066}                                       | 100/2320 [00:11<03:59,  9.25it/s]
{'loss': 0.1919, 'grad_norm': 0.36828041076660156, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43}                                                                                        
{'loss': 0.1919, 'grad_norm': 0.36828041076660156, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.43}                                                                                        
  5%|████████▏                                                                                                                                                | 125/2320 [00:14<03:48,  9.59it/s{'loss': 0.129, 'grad_norm': 0.26406458020210266, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.5387931034482759}                                          | 125/2320 [00:14<03:48,  9.59it/s]
{'loss': 0.129, 'grad_norm': 0.26406458020210266, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.54}                                                                                          
{'loss': 0.129, 'grad_norm': 0.26406458020210266, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.54}                                                                                          
  6%|█████████▉                                                                                                                                               | 150/2320 [00:16<03:56,  9.16it/s{'loss': 0.1189, 'grad_norm': 0.22471116483211517, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.646551724137931}                                         | 150/2320 [00:16<03:56,  9.16it/s]
{'loss': 0.1189, 'grad_norm': 0.22471116483211517, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.65}                                                                                        
{'loss': 0.1189, 'grad_norm': 0.22471116483211517, 'learning_rate': 9.954128440366974e-05, 'epoch': 0.65}                                                                                        
  8%|███████████▌                                                                                                                                             | 175/2320 [00:19<03:49,  9.36it/s{'loss': 0.1065, 'grad_norm': 0.21927005052566528, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.7543103448275862}                                        | 175/2320 [00:19<03:49,  9.36it/s]
{'loss': 0.1065, 'grad_norm': 0.21927005052566528, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.75}                                                                                        
{'loss': 0.1065, 'grad_norm': 0.21927005052566528, 'learning_rate': 9.839449541284404e-05, 'epoch': 0.75}                                                                                        
  9%|█████████████▏                                                                                                                                           | 200/2320 [00:22<03:53,  9.07it/s{'loss': 0.1068, 'grad_norm': 0.19120849668979645, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.8620689655172413}                                        | 200/2320 [00:22<03:53,  9.07it/s]
{'loss': 0.1068, 'grad_norm': 0.19120849668979645, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.86}                                                                                        
{'loss': 0.1068, 'grad_norm': 0.19120849668979645, 'learning_rate': 9.724770642201836e-05, 'epoch': 0.86}                                                                                        
 10%|██████████████▊                                                                                                                                          | 225/2320 [00:24<03:51,  9.06it/s{'loss': 0.1032, 'grad_norm': 0.23236669600009918, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.9698275862068966}                                        | 225/2320 [00:24<03:51,  9.06it/s]
{'loss': 0.1032, 'grad_norm': 0.23236669600009918, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.97}                                                                                        
{'loss': 0.1032, 'grad_norm': 0.23236669600009918, 'learning_rate': 9.610091743119267e-05, 'epoch': 0.97}                                                                                        
 10%|███████████████▎                                                                                                                                         | 232/2320 [00:25<03:49,  9.11it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.35it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.35it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.10152512043714523, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.789101202590144, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7342156726186214, 'eval_avg_precision': 0.789101202590144, 'eval_runtime': 0.8863, 'eval_samples_per_second': 1045.942, 'eval_steps_per_second': 32.721, 'epoch': 1.0}
{'eval_loss': 0.10152512043714523, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.789101202590144, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7342156726186214, 'eval_avg_precision': 0.789101202590144, 'eval_runtime': 0.8863, 'eval_samples_per_second': 1045.942, 'eval_steps_per_second': 32.721, 'epoch': 1.0}
{'eval_loss': 0.10152512043714523, 'eval_precision_micro': 0.7195253505933118, 'eval_recall_micro': 0.714898177920686, 'eval_f1_micro': 0.7172043010752689, 'eval_precision_macro': 0.042325020623135985, 'eval_recall_macro': 0.058823529411764705, 'eval_f1_macro': 0.049228725367185776, 'eval_precision_weighted': 0.5143873621069014, 'eval_recall_weighted': 0.714898177920686, 'eval_f1_weighted': 0.5982899431281024, 'eval_precision_samples': 0.7195253505933118, 'eval_recall_samples': 0.717727436174038, 'eval_f1_samples': 0.7182668104998201, 'eval_f1': 0.7172043010752689, 'eval_subset_accuracy': 0.7162891046386192, 'eval_hamming_loss': 0.03337775239545657, 'eval_jaccard_micro': 0.559094719195306, 'eval_jaccard_macro': 0.042325020623135985, 'eval_jaccard_weighted': 0.5143873621069014, 'eval_jaccard_samples': 0.717727436174038, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.789101202590144, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.7342156726186214, 'eval_avg_precision': 0.789101202590144, 'eval_runtime': 0.8863, 'eval_samples_per_second': 1045.942, 'eval_steps_per_second': 32.721, 'epoch': 1.0}
 11%|████████████████▍                                                                                                                                        | 250/2320 [00:29<03:47,  9.10it/s{'loss': 0.092, 'grad_norm': 0.2794743776321411, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.0775862068965518}                                          | 250/2320 [00:29<03:47,  9.10it/s]
{'loss': 0.092, 'grad_norm': 0.2794743776321411, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.08}                                                                                          
{'loss': 0.092, 'grad_norm': 0.2794743776321411, 'learning_rate': 9.495412844036697e-05, 'epoch': 1.08}                                                                                          
 12%|██████████████████▏                                                                                                                                      | 275/2320 [00:31<03:44,  9.10it/s{'loss': 0.1046, 'grad_norm': 0.19149772822856903, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.1853448275862069}                                        | 275/2320 [00:31<03:44,  9.10it/s]
{'loss': 0.1046, 'grad_norm': 0.19149772822856903, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.19}                                                                                        
{'loss': 0.1046, 'grad_norm': 0.19149772822856903, 'learning_rate': 9.380733944954129e-05, 'epoch': 1.19}                                                                                        
 13%|███████████████████▊                                                                                                                                     | 300/2320 [00:34<03:42,  9.09it/s{'loss': 0.103, 'grad_norm': 0.168669193983078, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.293103448275862}                                            | 300/2320 [00:34<03:42,  9.09it/s]
{'loss': 0.103, 'grad_norm': 0.168669193983078, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.29}                                                                                           
{'loss': 0.103, 'grad_norm': 0.168669193983078, 'learning_rate': 9.266055045871561e-05, 'epoch': 1.29}                                                                                           
 14%|█████████████████████▍                                                                                                                                   | 325/2320 [00:37<03:41,  8.99it/s{'loss': 0.0995, 'grad_norm': 0.1756293773651123, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4008620689655173}                                         | 325/2320 [00:37<03:41,  8.99it/s]
{'loss': 0.0995, 'grad_norm': 0.1756293773651123, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4}                                                                                          
{'loss': 0.0995, 'grad_norm': 0.1756293773651123, 'learning_rate': 9.151376146788991e-05, 'epoch': 1.4}                                                                                          
 15%|███████████████████████                                                                                                                                  | 350/2320 [00:40<03:37,  9.05it/s{'loss': 0.0906, 'grad_norm': 0.29948148131370544, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.5086206896551724}                                        | 350/2320 [00:40<03:37,  9.05it/s]
{'loss': 0.0906, 'grad_norm': 0.29948148131370544, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.51}                                                                                        
{'loss': 0.0906, 'grad_norm': 0.29948148131370544, 'learning_rate': 9.036697247706423e-05, 'epoch': 1.51}                                                                                        
 16%|████████████████████████▋                                                                                                                                | 375/2320 [00:42<03:34,  9.06it/s{'loss': 0.0879, 'grad_norm': 0.15851812064647675, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.6163793103448276}                                        | 375/2320 [00:42<03:34,  9.06it/s]
{'loss': 0.0879, 'grad_norm': 0.15851812064647675, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.62}                                                                                        
{'loss': 0.0879, 'grad_norm': 0.15851812064647675, 'learning_rate': 8.922018348623854e-05, 'epoch': 1.62}                                                                                        
 17%|██████████████████████████▍                                                                                                                              | 400/2320 [00:45<03:27,  9.24it/s{'loss': 0.0863, 'grad_norm': 0.31517139077186584, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.7241379310344827}                                        | 400/2320 [00:45<03:27,  9.24it/s]
{'loss': 0.0863, 'grad_norm': 0.31517139077186584, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.72}                                                                                        
{'loss': 0.0863, 'grad_norm': 0.31517139077186584, 'learning_rate': 8.807339449541285e-05, 'epoch': 1.72}                                                                                        
 18%|████████████████████████████                                                                                                                             | 425/2320 [00:48<03:29,  9.03it/s{'loss': 0.0793, 'grad_norm': 0.17256760597229004, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.831896551724138}                                         | 425/2320 [00:48<03:29,  9.02it/s]
{'loss': 0.0793, 'grad_norm': 0.17256760597229004, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.83}                                                                                        
{'loss': 0.0793, 'grad_norm': 0.17256760597229004, 'learning_rate': 8.692660550458716e-05, 'epoch': 1.83}                                                                                        
 19%|█████████████████████████████▋                                                                                                                           | 450/2320 [00:51<03:25,  9.09it/s{'loss': 0.0789, 'grad_norm': 0.18157215416431427, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.9396551724137931}                                        | 450/2320 [00:51<03:25,  9.09it/s]
{'loss': 0.0789, 'grad_norm': 0.18157215416431427, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.94}                                                                                        
{'loss': 0.0789, 'grad_norm': 0.18157215416431427, 'learning_rate': 8.577981651376146e-05, 'epoch': 1.94}                                                                                        
 20%|██████████████████████████████▌                                                                                                                          | 464/2320 [00:52<03:26,  9.00it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.46it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.46it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.07728221267461777, 'eval_precision_micro': 0.8943661971830986, 'eval_recall_micro': 0.6806002143622722, 'eval_f1_micro': 0.7729762629336578, 'eval_precision_macro': 0.05260977630488815, 'eval_recall_macro': 0.05600141105917629, 'eval_f1_macro': 0.05425263787432184, 'eval_precision_weighted': 0.6393807647600501, 'eval_recall_weighted': 0.6806002143622722, 'eval_f1_weighted': 0.6593469033836392, 'eval_precision_samples': 0.6850053937432579, 'eval_recall_samples': 0.6832074793239841, 'eval_f1_samples': 0.6837468536497662, 'eval_f1': 0.7729762629336578, 'eval_subset_accuracy': 0.6817691477885652, 'eval_hamming_loss': 0.023669014531378894, 'eval_jaccard_micro': 0.6299603174603174, 'eval_jaccard_macro': 0.0503408910734105, 'eval_jaccard_weighted': 0.6118063921558431, 'eval_jaccard_samples': 0.6832074793239841, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8425595214328135, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8189410686283475, 'eval_avg_precision': 0.8425595214328135, 'eval_runtime': 0.8776, 'eval_samples_per_second': 1056.27, 'eval_steps_per_second': 33.044, 'epoch': 2.0}
{'eval_loss': 0.07728221267461777, 'eval_precision_micro': 0.8943661971830986, 'eval_recall_micro': 0.6806002143622722, 'eval_f1_micro': 0.7729762629336578, 'eval_precision_macro': 0.05260977630488815, 'eval_recall_macro': 0.05600141105917629, 'eval_f1_macro': 0.05425263787432184, 'eval_precision_weighted': 0.6393807647600501, 'eval_recall_weighted': 0.6806002143622722, 'eval_f1_weighted': 0.6593469033836392, 'eval_precision_samples': 0.6850053937432579, 'eval_recall_samples': 0.6832074793239841, 'eval_f1_samples': 0.6837468536497662, 'eval_f1': 0.7729762629336578, 'eval_subset_accuracy': 0.6817691477885652, 'eval_hamming_loss': 0.023669014531378894, 'eval_jaccard_micro': 0.6299603174603174, 'eval_jaccard_macro': 0.0503408910734105, 'eval_jaccard_weighted': 0.6118063921558431, 'eval_jaccard_samples': 0.6832074793239841, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8425595214328135, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8189410686283475, 'eval_avg_precision': 0.8425595214328135, 'eval_runtime': 0.8776, 'eval_samples_per_second': 1056.27, 'eval_steps_per_second': 33.044, 'epoch': 2.0}
{'eval_loss': 0.07728221267461777, 'eval_precision_micro': 0.8943661971830986, 'eval_recall_micro': 0.6806002143622722, 'eval_f1_micro': 0.7729762629336578, 'eval_precision_macro': 0.05260977630488815, 'eval_recall_macro': 0.05600141105917629, 'eval_f1_macro': 0.05425263787432184, 'eval_precision_weighted': 0.6393807647600501, 'eval_recall_weighted': 0.6806002143622722, 'eval_f1_weighted': 0.6593469033836392, 'eval_precision_samples': 0.6850053937432579, 'eval_recall_samples': 0.6832074793239841, 'eval_f1_samples': 0.6837468536497662, 'eval_f1': 0.7729762629336578, 'eval_subset_accuracy': 0.6817691477885652, 'eval_hamming_loss': 0.023669014531378894, 'eval_jaccard_micro': 0.6299603174603174, 'eval_jaccard_macro': 0.0503408910734105, 'eval_jaccard_weighted': 0.6118063921558431, 'eval_jaccard_samples': 0.6832074793239841, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.8425595214328135, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.8189410686283475, 'eval_avg_precision': 0.8425595214328135, 'eval_runtime': 0.8776, 'eval_samples_per_second': 1056.27, 'eval_steps_per_second': 33.044, 'epoch': 2.0}
 20%|███████████████████████████████▎                                                                                                                         | 475/2320 [00:55<03:53,  7.89it/s{'loss': 0.0832, 'grad_norm': 0.15342406928539276, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.0474137931034484}                                        | 475/2320 [00:55<03:53,  7.89it/s]
{'loss': 0.0832, 'grad_norm': 0.15342406928539276, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.05}                                                                                        
{'loss': 0.0832, 'grad_norm': 0.15342406928539276, 'learning_rate': 8.463302752293578e-05, 'epoch': 2.05}                                                                                        
 22%|████████████████████████████████▉                                                                                                                        | 500/2320 [00:58<03:25,  8.86it/s{'loss': 0.0814, 'grad_norm': 0.29270514845848083, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.1551724137931036}                                         | 500/2320 [00:58<03:25,  8.86it/s]
{'loss': 0.0814, 'grad_norm': 0.29270514845848083, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.16}                                                                                         
{'loss': 0.0814, 'grad_norm': 0.29270514845848083, 'learning_rate': 8.34862385321101e-05, 'epoch': 2.16}                                                                                         
 23%|██████████████████████████████████▌                                                                                                                      | 525/2320 [01:00<03:19,  8.99it/s{'loss': 0.0655, 'grad_norm': 0.17478248476982117, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.2629310344827585}                                         | 525/2320 [01:00<03:19,  8.99it/s]
{'loss': 0.0655, 'grad_norm': 0.17478248476982117, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.26}                                                                                         
{'loss': 0.0655, 'grad_norm': 0.17478248476982117, 'learning_rate': 8.23394495412844e-05, 'epoch': 2.26}                                                                                         
 24%|████████████████████████████████████▎                                                                                                                    | 550/2320 [01:03<03:16,  9.00it/s{'loss': 0.0642, 'grad_norm': 0.15585985779762268, 'learning_rate': 8.119266055045872e-05, 'epoch': 2.3706896551724137}                                        | 550/2320 [01:03<03:16,  9.00it/s]
{'loss': 0.0642, 'grad_norm': 0.15585985779762268, 'learning_rate': 8.119266055045872e-05, 'epoch': 2.37}                                                                                        
{'loss': 0.0642, 'grad_norm': 0.15585985779762268, 'learning_rate': 8.119266055045872e-05, 'epoch': 2.37}                                                                                        
 25%|█████████████████████████████████████▉                                                                                                                   | 575/2320 [01:06<03:15,  8.94it/s{'loss': 0.0683, 'grad_norm': 0.2121107280254364, 'learning_rate': 8.004587155963303e-05, 'epoch': 2.478448275862069}                                          | 575/2320 [01:06<03:15,  8.94it/s]
{'loss': 0.0683, 'grad_norm': 0.2121107280254364, 'learning_rate': 8.004587155963303e-05, 'epoch': 2.48}                                                                                         
{'loss': 0.0683, 'grad_norm': 0.2121107280254364, 'learning_rate': 8.004587155963303e-05, 'epoch': 2.48}                                                                                         
 26%|███████████████████████████████████████▌                                                                                                                 | 600/2320 [01:09<03:06,  9.20it/s{'loss': 0.0616, 'grad_norm': 0.21840761601924896, 'learning_rate': 7.889908256880735e-05, 'epoch': 2.586206896551724}                                         | 600/2320 [01:09<03:06,  9.21it/s]
{'loss': 0.0616, 'grad_norm': 0.21840761601924896, 'learning_rate': 7.889908256880735e-05, 'epoch': 2.59}                                                                                        
{'loss': 0.0616, 'grad_norm': 0.21840761601924896, 'learning_rate': 7.889908256880735e-05, 'epoch': 2.59}                                                                                        
 27%|█████████████████████████████████████████▏                                                                                                               | 625/2320 [01:12<03:07,  9.02it/s{'loss': 0.0562, 'grad_norm': 0.1324414759874344, 'learning_rate': 7.775229357798165e-05, 'epoch': 2.6939655172413794}                                         | 625/2320 [01:12<03:07,  9.02it/s]
{'loss': 0.0562, 'grad_norm': 0.1324414759874344, 'learning_rate': 7.775229357798165e-05, 'epoch': 2.69}                                                                                         
{'loss': 0.0562, 'grad_norm': 0.1324414759874344, 'learning_rate': 7.775229357798165e-05, 'epoch': 2.69}                                                                                         
 28%|██████████████████████████████████████████▊                                                                                                              | 650/2320 [01:14<03:07,  8.92it/s{'loss': 0.0587, 'grad_norm': 0.10673216730356216, 'learning_rate': 7.660550458715597e-05, 'epoch': 2.8017241379310347}                                        | 650/2320 [01:14<03:07,  8.92it/s]
{'loss': 0.0587, 'grad_norm': 0.10673216730356216, 'learning_rate': 7.660550458715597e-05, 'epoch': 2.8}                                                                                         
{'loss': 0.0587, 'grad_norm': 0.10673216730356216, 'learning_rate': 7.660550458715597e-05, 'epoch': 2.8}                                                                                         
 29%|████████████████████████████████████████████▌                                                                                                            | 675/2320 [01:17<03:05,  8.89it/s{'loss': 0.0503, 'grad_norm': 0.1557665467262268, 'learning_rate': 7.545871559633027e-05, 'epoch': 2.9094827586206895}                                         | 675/2320 [01:17<03:05,  8.89it/s]
{'loss': 0.0503, 'grad_norm': 0.1557665467262268, 'learning_rate': 7.545871559633027e-05, 'epoch': 2.91}                                                                                         
{'loss': 0.0503, 'grad_norm': 0.1557665467262268, 'learning_rate': 7.545871559633027e-05, 'epoch': 2.91}                                                                                         
 30%|█████████████████████████████████████████████▉                                                                                                           | 696/2320 [01:19<02:58,  9.10it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.64it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.64it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.04814748093485832, 'eval_precision_micro': 0.930622009569378, 'eval_recall_micro': 0.8338692390139335, 'eval_f1_micro': 0.8795929903900509, 'eval_precision_macro': 0.16827830201314237, 'eval_recall_macro': 0.15580226864041116, 'eval_f1_macro': 0.16135044268036597, 'eval_precision_weighted': 0.8097644843832998, 'eval_recall_weighted': 0.8338692390139335, 'eval_f1_weighted': 0.8202063507529745, 'eval_precision_samples': 0.8392664509169363, 'eval_recall_samples': 0.8374685364976626, 'eval_f1_samples': 0.8380079108234447, 'eval_f1': 0.8795929903900509, 'eval_subset_accuracy': 0.8360302049622438, 'eval_hamming_loss': 0.013516086046068913, 'eval_jaccard_micro': 0.7850655903128153, 'eval_jaccard_macro': 0.14909359970403255, 'eval_jaccard_weighted': 0.7777777777777778, 'eval_jaccard_samples': 0.8374685364976626, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9264120635187872, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9163253448650032, 'eval_avg_precision': 0.9264120635187872, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.322, 'eval_steps_per_second': 33.233, 'epoch': 3.0}
{'eval_loss': 0.04814748093485832, 'eval_precision_micro': 0.930622009569378, 'eval_recall_micro': 0.8338692390139335, 'eval_f1_micro': 0.8795929903900509, 'eval_precision_macro': 0.16827830201314237, 'eval_recall_macro': 0.15580226864041116, 'eval_f1_macro': 0.16135044268036597, 'eval_precision_weighted': 0.8097644843832998, 'eval_recall_weighted': 0.8338692390139335, 'eval_f1_weighted': 0.8202063507529745, 'eval_precision_samples': 0.8392664509169363, 'eval_recall_samples': 0.8374685364976626, 'eval_f1_samples': 0.8380079108234447, 'eval_f1': 0.8795929903900509, 'eval_subset_accuracy': 0.8360302049622438, 'eval_hamming_loss': 0.013516086046068913, 'eval_jaccard_micro': 0.7850655903128153, 'eval_jaccard_macro': 0.14909359970403255, 'eval_jaccard_weighted': 0.7777777777777778, 'eval_jaccard_samples': 0.8374685364976626, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9264120635187872, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9163253448650032, 'eval_avg_precision': 0.9264120635187872, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.322, 'eval_steps_per_second': 33.233, 'epoch': 3.0}
{'eval_loss': 0.04814748093485832, 'eval_precision_micro': 0.930622009569378, 'eval_recall_micro': 0.8338692390139335, 'eval_f1_micro': 0.8795929903900509, 'eval_precision_macro': 0.16827830201314237, 'eval_recall_macro': 0.15580226864041116, 'eval_f1_macro': 0.16135044268036597, 'eval_precision_weighted': 0.8097644843832998, 'eval_recall_weighted': 0.8338692390139335, 'eval_f1_weighted': 0.8202063507529745, 'eval_precision_samples': 0.8392664509169363, 'eval_recall_samples': 0.8374685364976626, 'eval_f1_samples': 0.8380079108234447, 'eval_f1': 0.8795929903900509, 'eval_subset_accuracy': 0.8360302049622438, 'eval_hamming_loss': 0.013516086046068913, 'eval_jaccard_micro': 0.7850655903128153, 'eval_jaccard_macro': 0.14909359970403255, 'eval_jaccard_weighted': 0.7777777777777778, 'eval_jaccard_samples': 0.8374685364976626, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9264120635187872, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9163253448650032, 'eval_avg_precision': 0.9264120635187872, 'eval_runtime': 0.8726, 'eval_samples_per_second': 1062.322, 'eval_steps_per_second': 33.233, 'epoch': 3.0}
 30%|██████████████████████████████████████████████▏                                                                                                          | 700/2320 [01:21<06:38,  4.07it/s{'loss': 0.0445, 'grad_norm': 0.22074012458324432, 'learning_rate': 7.431192660550459e-05, 'epoch': 3.0172413793103448}                                        | 700/2320 [01:21<06:38,  4.07it/s]
{'loss': 0.0445, 'grad_norm': 0.22074012458324432, 'learning_rate': 7.431192660550459e-05, 'epoch': 3.02}                                                                                        
{'loss': 0.0445, 'grad_norm': 0.22074012458324432, 'learning_rate': 7.431192660550459e-05, 'epoch': 3.02}                                                                                        
 31%|███████████████████████████████████████████████▋                                                                                                         | 723/2320 [01:22<01:17, 20.55it/s{'loss': 0.052, 'grad_norm': 0.12235768884420395, 'learning_rate': 7.31651376146789e-05, 'epoch': 3.125}                                                       | 723/2320 [01:22<01:17, 20.55it/s]
{'loss': 0.052, 'grad_norm': 0.12235768884420395, 'learning_rate': 7.31651376146789e-05, 'epoch': 3.12}                                                                                          
{'loss': 0.052, 'grad_norm': 0.12235768884420395, 'learning_rate': 7.31651376146789e-05, 'epoch': 3.12}                                                                                          
 32%|█████████████████████████████████████████████████▍                                                                                                       | 750/2320 [01:23<01:07, 23.29it/s{'loss': 0.0456, 'grad_norm': 0.18351112306118011, 'learning_rate': 7.201834862385322e-05, 'epoch': 3.2327586206896552}                                        | 750/2320 [01:23<01:07, 23.29it/s]
{'loss': 0.0456, 'grad_norm': 0.18351112306118011, 'learning_rate': 7.201834862385322e-05, 'epoch': 3.23}                                                                                        
{'loss': 0.0456, 'grad_norm': 0.18351112306118011, 'learning_rate': 7.201834862385322e-05, 'epoch': 3.23}                                                                                        
 33%|███████████████████████████████████████████████████                                                                                                      | 774/2320 [01:24<01:04, 23.87it/s{'loss': 0.0357, 'grad_norm': 0.21625478565692902, 'learning_rate': 7.087155963302753e-05, 'epoch': 3.3405172413793105}                                        | 774/2320 [01:24<01:04, 23.87it/s]
{'loss': 0.0357, 'grad_norm': 0.21625478565692902, 'learning_rate': 7.087155963302753e-05, 'epoch': 3.34}                                                                                        
{'loss': 0.0357, 'grad_norm': 0.21625478565692902, 'learning_rate': 7.087155963302753e-05, 'epoch': 3.34}                                                                                        
 34%|████████████████████████████████████████████████████▋                                                                                                    | 798/2320 [01:26<01:04, 23.70it/s{'loss': 0.0412, 'grad_norm': 0.14080806076526642, 'learning_rate': 6.972477064220184e-05, 'epoch': 3.4482758620689653}                                        | 798/2320 [01:26<01:04, 23.70it/s]
{'loss': 0.0412, 'grad_norm': 0.14080806076526642, 'learning_rate': 6.972477064220184e-05, 'epoch': 3.45}                                                                                        
{'loss': 0.0412, 'grad_norm': 0.14080806076526642, 'learning_rate': 6.972477064220184e-05, 'epoch': 3.45}                                                                                        
 36%|██████████████████████████████████████████████████████▍                                                                                                  | 825/2320 [01:27<01:02, 23.79it/s{'loss': 0.0355, 'grad_norm': 0.12793849408626556, 'learning_rate': 6.857798165137616e-05, 'epoch': 3.5560344827586206}                                        | 825/2320 [01:27<01:02, 23.79it/s]
{'loss': 0.0355, 'grad_norm': 0.12793849408626556, 'learning_rate': 6.857798165137616e-05, 'epoch': 3.56}                                                                                        
{'loss': 0.0355, 'grad_norm': 0.12793849408626556, 'learning_rate': 6.857798165137616e-05, 'epoch': 3.56}                                                                                        
 37%|███████████████████████████████████████████████████████▉                                                                                                 | 849/2320 [01:28<01:02, 23.43it/s{'loss': 0.0397, 'grad_norm': 0.1836950182914734, 'learning_rate': 6.743119266055046e-05, 'epoch': 3.663793103448276}                                          | 849/2320 [01:28<01:02, 23.43it/s]
{'loss': 0.0397, 'grad_norm': 0.1836950182914734, 'learning_rate': 6.743119266055046e-05, 'epoch': 3.66}                                                                                         
{'loss': 0.0397, 'grad_norm': 0.1836950182914734, 'learning_rate': 6.743119266055046e-05, 'epoch': 3.66}                                                                                         
 38%|█████████████████████████████████████████████████████████▌                                                                                               | 873/2320 [01:29<01:01, 23.57it/s{'loss': 0.0483, 'grad_norm': 0.19098447263240814, 'learning_rate': 6.628440366972477e-05, 'epoch': 3.771551724137931}                                         | 873/2320 [01:29<01:01, 23.57it/s]
{'loss': 0.0483, 'grad_norm': 0.19098447263240814, 'learning_rate': 6.628440366972477e-05, 'epoch': 3.77}                                                                                        
{'loss': 0.0483, 'grad_norm': 0.19098447263240814, 'learning_rate': 6.628440366972477e-05, 'epoch': 3.77}                                                                                        
 39%|███████████████████████████████████████████████████████████▎                                                                                             | 900/2320 [01:30<01:00, 23.40it/s{'loss': 0.0303, 'grad_norm': 0.16385096311569214, 'learning_rate': 6.513761467889909e-05, 'epoch': 3.8793103448275863}                                        | 900/2320 [01:30<01:00, 23.40it/s]
{'loss': 0.0303, 'grad_norm': 0.16385096311569214, 'learning_rate': 6.513761467889909e-05, 'epoch': 3.88}                                                                                        
{'loss': 0.0303, 'grad_norm': 0.16385096311569214, 'learning_rate': 6.513761467889909e-05, 'epoch': 3.88}                                                                                        
 40%|████████████████████████████████████████████████████████████▉                                                                                            | 924/2320 [01:31<00:59, 23.30it/s{'loss': 0.0383, 'grad_norm': 0.11745931953191757, 'learning_rate': 6.39908256880734e-05, 'epoch': 3.987068965517241}                                          | 924/2320 [01:31<00:59, 23.30it/s]
{'loss': 0.0383, 'grad_norm': 0.11745931953191757, 'learning_rate': 6.39908256880734e-05, 'epoch': 3.99}                                                                                         
{'loss': 0.0383, 'grad_norm': 0.11745931953191757, 'learning_rate': 6.39908256880734e-05, 'epoch': 3.99}                                                                                         
 40%|█████████████████████████████████████████████████████████████▏                                                                                           | 927/2320 [01:31<01:00, 22.92it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 47.78it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 47.78it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.030286140739917755, 'eval_precision_micro': 0.9613636363636363, 'eval_recall_micro': 0.9067524115755627, 'eval_f1_micro': 0.9332597904026476, 'eval_precision_macro': 0.33198681897404486, 'eval_recall_macro': 0.29608122014720467, 'eval_f1_macro': 0.3111016187456061, 'eval_precision_weighted': 0.9126822700343007, 'eval_recall_weighted': 0.9067524115755627, 'eval_f1_weighted': 0.908252834969152, 'eval_precision_samples': 0.9120819848975189, 'eval_recall_samples': 0.9102840704782451, 'eval_f1_samples': 0.9106436533621001, 'eval_f1': 0.9332597904026476, 'eval_subset_accuracy': 0.9072276159654801, 'eval_hamming_loss': 0.007678152167015674, 'eval_jaccard_micro': 0.8748707342295761, 'eval_jaccard_macro': 0.28156960287197624, 'eval_jaccard_weighted': 0.8738920284313688, 'eval_jaccard_samples': 0.909744696152463, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9616138769426272, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9449829857661978, 'eval_avg_precision': 0.9616138769426272, 'eval_runtime': 0.7109, 'eval_samples_per_second': 1304.002, 'eval_steps_per_second': 40.794, 'epoch': 4.0}
{'eval_loss': 0.030286140739917755, 'eval_precision_micro': 0.9613636363636363, 'eval_recall_micro': 0.9067524115755627, 'eval_f1_micro': 0.9332597904026476, 'eval_precision_macro': 0.33198681897404486, 'eval_recall_macro': 0.29608122014720467, 'eval_f1_macro': 0.3111016187456061, 'eval_precision_weighted': 0.9126822700343007, 'eval_recall_weighted': 0.9067524115755627, 'eval_f1_weighted': 0.908252834969152, 'eval_precision_samples': 0.9120819848975189, 'eval_recall_samples': 0.9102840704782451, 'eval_f1_samples': 0.9106436533621001, 'eval_f1': 0.9332597904026476, 'eval_subset_accuracy': 0.9072276159654801, 'eval_hamming_loss': 0.007678152167015674, 'eval_jaccard_micro': 0.8748707342295761, 'eval_jaccard_macro': 0.28156960287197624, 'eval_jaccard_weighted': 0.8738920284313688, 'eval_jaccard_samples': 0.909744696152463, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9616138769426272, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9449829857661978, 'eval_avg_precision': 0.9616138769426272, 'eval_runtime': 0.7109, 'eval_samples_per_second': 1304.002, 'eval_steps_per_second': 40.794, 'epoch': 4.0}
{'eval_loss': 0.030286140739917755, 'eval_precision_micro': 0.9613636363636363, 'eval_recall_micro': 0.9067524115755627, 'eval_f1_micro': 0.9332597904026476, 'eval_precision_macro': 0.33198681897404486, 'eval_recall_macro': 0.29608122014720467, 'eval_f1_macro': 0.3111016187456061, 'eval_precision_weighted': 0.9126822700343007, 'eval_recall_weighted': 0.9067524115755627, 'eval_f1_weighted': 0.908252834969152, 'eval_precision_samples': 0.9120819848975189, 'eval_recall_samples': 0.9102840704782451, 'eval_f1_samples': 0.9106436533621001, 'eval_f1': 0.9332597904026476, 'eval_subset_accuracy': 0.9072276159654801, 'eval_hamming_loss': 0.007678152167015674, 'eval_jaccard_micro': 0.8748707342295761, 'eval_jaccard_macro': 0.28156960287197624, 'eval_jaccard_weighted': 0.8738920284313688, 'eval_jaccard_samples': 0.909744696152463, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9616138769426272, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9449829857661978, 'eval_avg_precision': 0.9616138769426272, 'eval_runtime': 0.7109, 'eval_samples_per_second': 1304.002, 'eval_steps_per_second': 40.794, 'epoch': 4.0}
 41%|██████████████████████████████████████████████████████████████▌                                                                                          | 949/2320 [01:33<01:12, 18.87it/s{'loss': 0.0342, 'grad_norm': 0.17165625095367432, 'learning_rate': 6.284403669724771e-05, 'epoch': 4.094827586206897}                                         | 949/2320 [01:33<01:12, 18.87it/s]
{'loss': 0.0342, 'grad_norm': 0.17165625095367432, 'learning_rate': 6.284403669724771e-05, 'epoch': 4.09}                                                                                        
{'loss': 0.0342, 'grad_norm': 0.17165625095367432, 'learning_rate': 6.284403669724771e-05, 'epoch': 4.09}                                                                                        
 42%|████████████████████████████████████████████████████████████████▏                                                                                        | 973/2320 [01:34<00:58, 23.04it/s{'loss': 0.034, 'grad_norm': 0.06481951475143433, 'learning_rate': 6.169724770642203e-05, 'epoch': 4.202586206896552}                                          | 973/2320 [01:34<00:58, 23.04it/s]
{'loss': 0.034, 'grad_norm': 0.06481951475143433, 'learning_rate': 6.169724770642203e-05, 'epoch': 4.2}                                                                                          
{'loss': 0.034, 'grad_norm': 0.06481951475143433, 'learning_rate': 6.169724770642203e-05, 'epoch': 4.2}                                                                                          
 43%|█████████████████████████████████████████████████████████████████▌                                                                                      | 1000/2320 [01:36<00:56, 23.30it/s{'loss': 0.0291, 'grad_norm': 0.0301351398229599, 'learning_rate': 6.055045871559634e-05, 'epoch': 4.310344827586207}                                         | 1000/2320 [01:36<00:56, 23.30it/s]
{'loss': 0.0291, 'grad_norm': 0.0301351398229599, 'learning_rate': 6.055045871559634e-05, 'epoch': 4.31}                                                                                         
{'loss': 0.0291, 'grad_norm': 0.0301351398229599, 'learning_rate': 6.055045871559634e-05, 'epoch': 4.31}                                                                                         
 44%|███████████████████████████████████████████████████████████████████                                                                                     | 1024/2320 [01:37<00:54, 23.64it/s{'loss': 0.0308, 'grad_norm': 0.20918914675712585, 'learning_rate': 5.940366972477065e-05, 'epoch': 4.418103448275862}                                        | 1024/2320 [01:37<00:54, 23.64it/s]
{'loss': 0.0308, 'grad_norm': 0.20918914675712585, 'learning_rate': 5.940366972477065e-05, 'epoch': 4.42}                                                                                        
{'loss': 0.0308, 'grad_norm': 0.20918914675712585, 'learning_rate': 5.940366972477065e-05, 'epoch': 4.42}                                                                                        
 45%|████████████████████████████████████████████████████████████████████▋                                                                                   | 1048/2320 [01:38<00:53, 23.83it/s{'loss': 0.0356, 'grad_norm': 0.14303994178771973, 'learning_rate': 5.8256880733944955e-05, 'epoch': 4.525862068965517}                                       | 1048/2320 [01:38<00:53, 23.83it/s]
{'loss': 0.0356, 'grad_norm': 0.14303994178771973, 'learning_rate': 5.8256880733944955e-05, 'epoch': 4.53}                                                                                       
{'loss': 0.0356, 'grad_norm': 0.14303994178771973, 'learning_rate': 5.8256880733944955e-05, 'epoch': 4.53}                                                                                       
 46%|██████████████████████████████████████████████████████████████████████▍                                                                                 | 1075/2320 [01:39<00:52, 23.56it/s{'loss': 0.0266, 'grad_norm': 0.11687213182449341, 'learning_rate': 5.7110091743119266e-05, 'epoch': 4.633620689655173}                                       | 1075/2320 [01:39<00:52, 23.56it/s]
{'loss': 0.0266, 'grad_norm': 0.11687213182449341, 'learning_rate': 5.7110091743119266e-05, 'epoch': 4.63}                                                                                       
{'loss': 0.0266, 'grad_norm': 0.11687213182449341, 'learning_rate': 5.7110091743119266e-05, 'epoch': 4.63}                                                                                       
 47%|████████████████████████████████████████████████████████████████████████                                                                                | 1099/2320 [01:40<00:51, 23.50it/s{'loss': 0.0269, 'grad_norm': 0.1256481558084488, 'learning_rate': 5.596330275229358e-05, 'epoch': 4.741379310344827}                                         | 1099/2320 [01:40<00:51, 23.50it/s]
{'loss': 0.0269, 'grad_norm': 0.1256481558084488, 'learning_rate': 5.596330275229358e-05, 'epoch': 4.74}                                                                                         
{'loss': 0.0269, 'grad_norm': 0.1256481558084488, 'learning_rate': 5.596330275229358e-05, 'epoch': 4.74}                                                                                         
 48%|█████████████████████████████████████████████████████████████████████████▌                                                                              | 1123/2320 [01:41<00:50, 23.83it/s{'loss': 0.0301, 'grad_norm': 0.1259380429983139, 'learning_rate': 5.481651376146789e-05, 'epoch': 4.849137931034483}                                         | 1123/2320 [01:41<00:50, 23.83it/s]
{'loss': 0.0301, 'grad_norm': 0.1259380429983139, 'learning_rate': 5.481651376146789e-05, 'epoch': 4.85}                                                                                         
{'loss': 0.0301, 'grad_norm': 0.1259380429983139, 'learning_rate': 5.481651376146789e-05, 'epoch': 4.85}                                                                                         
 50%|███████████████████████████████████████████████████████████████████████████▎                                                                            | 1150/2320 [01:42<00:50, 23.37it/s{'loss': 0.0258, 'grad_norm': 0.2353096753358841, 'learning_rate': 5.36697247706422e-05, 'epoch': 4.956896551724138}                                          | 1150/2320 [01:42<00:50, 23.37it/s]
{'loss': 0.0258, 'grad_norm': 0.2353096753358841, 'learning_rate': 5.36697247706422e-05, 'epoch': 4.96}                                                                                          
{'loss': 0.0258, 'grad_norm': 0.2353096753358841, 'learning_rate': 5.36697247706422e-05, 'epoch': 4.96}                                                                                          
 50%|███████████████████████████████████████████████████████████████████████████▉                                                                            | 1159/2320 [01:42<00:50, 23.12it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 47.75it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 47.75it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.022086847573518753, 'eval_precision_micro': 0.9785067873303167, 'eval_recall_micro': 0.9271168274383709, 'eval_f1_micro': 0.9521188772702256, 'eval_precision_macro': 0.33738235052415205, 'eval_recall_macro': 0.32786528916025215, 'eval_f1_macro': 0.33240334501006596, 'eval_precision_weighted': 0.9292770329736351, 'eval_recall_weighted': 0.9271168274383709, 'eval_f1_weighted': 0.9280644060654379, 'eval_precision_samples': 0.9309600862998921, 'eval_recall_samples': 0.9297015462064004, 'eval_f1_samples': 0.9297015462064004, 'eval_f1': 0.9521188772702256, 'eval_subset_accuracy': 0.9255663430420712, 'eval_hamming_loss': 0.005520654863887303, 'eval_jaccard_micro': 0.9086134453781513, 'eval_jaccard_macro': 0.31476272386334975, 'eval_jaccard_weighted': 0.908254093776036, 'eval_jaccard_samples': 0.9286227975548363, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9792972563471641, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9724268028355076, 'eval_avg_precision': 0.9792972563471641, 'eval_runtime': 0.7122, 'eval_samples_per_second': 1301.604, 'eval_steps_per_second': 40.719, 'epoch': 5.0}
{'eval_loss': 0.022086847573518753, 'eval_precision_micro': 0.9785067873303167, 'eval_recall_micro': 0.9271168274383709, 'eval_f1_micro': 0.9521188772702256, 'eval_precision_macro': 0.33738235052415205, 'eval_recall_macro': 0.32786528916025215, 'eval_f1_macro': 0.33240334501006596, 'eval_precision_weighted': 0.9292770329736351, 'eval_recall_weighted': 0.9271168274383709, 'eval_f1_weighted': 0.9280644060654379, 'eval_precision_samples': 0.9309600862998921, 'eval_recall_samples': 0.9297015462064004, 'eval_f1_samples': 0.9297015462064004, 'eval_f1': 0.9521188772702256, 'eval_subset_accuracy': 0.9255663430420712, 'eval_hamming_loss': 0.005520654863887303, 'eval_jaccard_micro': 0.9086134453781513, 'eval_jaccard_macro': 0.31476272386334975, 'eval_jaccard_weighted': 0.908254093776036, 'eval_jaccard_samples': 0.9286227975548363, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9792972563471641, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9724268028355076, 'eval_avg_precision': 0.9792972563471641, 'eval_runtime': 0.7122, 'eval_samples_per_second': 1301.604, 'eval_steps_per_second': 40.719, 'epoch': 5.0}
{'eval_loss': 0.022086847573518753, 'eval_precision_micro': 0.9785067873303167, 'eval_recall_micro': 0.9271168274383709, 'eval_f1_micro': 0.9521188772702256, 'eval_precision_macro': 0.33738235052415205, 'eval_recall_macro': 0.32786528916025215, 'eval_f1_macro': 0.33240334501006596, 'eval_precision_weighted': 0.9292770329736351, 'eval_recall_weighted': 0.9271168274383709, 'eval_f1_weighted': 0.9280644060654379, 'eval_precision_samples': 0.9309600862998921, 'eval_recall_samples': 0.9297015462064004, 'eval_f1_samples': 0.9297015462064004, 'eval_f1': 0.9521188772702256, 'eval_subset_accuracy': 0.9255663430420712, 'eval_hamming_loss': 0.005520654863887303, 'eval_jaccard_micro': 0.9086134453781513, 'eval_jaccard_macro': 0.31476272386334975, 'eval_jaccard_weighted': 0.908254093776036, 'eval_jaccard_samples': 0.9286227975548363, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9792972563471641, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9724268028355076, 'eval_avg_precision': 0.9792972563471641, 'eval_runtime': 0.7122, 'eval_samples_per_second': 1301.604, 'eval_steps_per_second': 40.719, 'epoch': 5.0}
 51%|████████████████████████████████████████████████████████████████████████████▉                                                                           | 1175/2320 [01:44<01:17, 14.73it/s{'loss': 0.0233, 'grad_norm': 0.09667550027370453, 'learning_rate': 5.252293577981652e-05, 'epoch': 5.064655172413793}                                        | 1175/2320 [01:44<01:17, 14.74it/s]
{'loss': 0.0233, 'grad_norm': 0.09667550027370453, 'learning_rate': 5.252293577981652e-05, 'epoch': 5.06}                                                                                        
{'loss': 0.0233, 'grad_norm': 0.09667550027370453, 'learning_rate': 5.252293577981652e-05, 'epoch': 5.06}                                                                                        
 52%|██████████████████████████████████████████████████████████████████████████████▌                                                                         | 1199/2320 [01:45<00:49, 22.46it/s{'loss': 0.0234, 'grad_norm': 0.07912161201238632, 'learning_rate': 5.137614678899083e-05, 'epoch': 5.172413793103448}                                        | 1199/2320 [01:45<00:49, 22.46it/s]
{'loss': 0.0234, 'grad_norm': 0.07912161201238632, 'learning_rate': 5.137614678899083e-05, 'epoch': 5.17}                                                                                        
{'loss': 0.0234, 'grad_norm': 0.07912161201238632, 'learning_rate': 5.137614678899083e-05, 'epoch': 5.17}                                                                                        
 53%|████████████████████████████████████████████████████████████████████████████████▏                                                                       | 1223/2320 [01:46<00:47, 22.92it/s{'loss': 0.0254, 'grad_norm': 0.14975053071975708, 'learning_rate': 5.022935779816514e-05, 'epoch': 5.280172413793103}                                        | 1223/2320 [01:46<00:47, 22.92it/s]
{'loss': 0.0254, 'grad_norm': 0.14975053071975708, 'learning_rate': 5.022935779816514e-05, 'epoch': 5.28}                                                                                        
{'loss': 0.0254, 'grad_norm': 0.14975053071975708, 'learning_rate': 5.022935779816514e-05, 'epoch': 5.28}                                                                                        
 54%|█████████████████████████████████████████████████████████████████████████████████▉                                                                      | 1250/2320 [01:48<00:45, 23.77it/s{'loss': 0.0265, 'grad_norm': 0.19025923311710358, 'learning_rate': 4.9082568807339454e-05, 'epoch': 5.387931034482759}                                       | 1250/2320 [01:48<00:45, 23.77it/s]
{'loss': 0.0265, 'grad_norm': 0.19025923311710358, 'learning_rate': 4.9082568807339454e-05, 'epoch': 5.39}                                                                                       
{'loss': 0.0265, 'grad_norm': 0.19025923311710358, 'learning_rate': 4.9082568807339454e-05, 'epoch': 5.39}                                                                                       
 55%|███████████████████████████████████████████████████████████████████████████████████▍                                                                    | 1274/2320 [01:49<00:44, 23.57it/s{'loss': 0.026, 'grad_norm': 0.20561043918132782, 'learning_rate': 4.7935779816513766e-05, 'epoch': 5.495689655172414}                                        | 1274/2320 [01:49<00:44, 23.57it/s]
{'loss': 0.026, 'grad_norm': 0.20561043918132782, 'learning_rate': 4.7935779816513766e-05, 'epoch': 5.5}                                                                                         
{'loss': 0.026, 'grad_norm': 0.20561043918132782, 'learning_rate': 4.7935779816513766e-05, 'epoch': 5.5}                                                                                         
 56%|█████████████████████████████████████████████████████████████████████████████████████                                                                   | 1298/2320 [01:50<00:42, 23.86it/s{'loss': 0.0183, 'grad_norm': 0.05122111737728119, 'learning_rate': 4.678899082568808e-05, 'epoch': 5.603448275862069}                                        | 1298/2320 [01:50<00:42, 23.85it/s]
{'loss': 0.0183, 'grad_norm': 0.05122111737728119, 'learning_rate': 4.678899082568808e-05, 'epoch': 5.6}                                                                                         
{'loss': 0.0183, 'grad_norm': 0.05122111737728119, 'learning_rate': 4.678899082568808e-05, 'epoch': 5.6}                                                                                         
 57%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 1325/2320 [01:51<00:42, 23.56it/s{'loss': 0.0197, 'grad_norm': 0.03152436017990112, 'learning_rate': 4.564220183486239e-05, 'epoch': 5.711206896551724}                                        | 1325/2320 [01:51<00:42, 23.56it/s]
{'loss': 0.0197, 'grad_norm': 0.03152436017990112, 'learning_rate': 4.564220183486239e-05, 'epoch': 5.71}                                                                                        
{'loss': 0.0197, 'grad_norm': 0.03152436017990112, 'learning_rate': 4.564220183486239e-05, 'epoch': 5.71}                                                                                        
 58%|████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 1350/2320 [01:53<01:45,  9.18it/s{'loss': 0.0272, 'grad_norm': 0.2298377901315689, 'learning_rate': 4.44954128440367e-05, 'epoch': 5.818965517241379}                                          | 1350/2320 [01:53<01:45,  9.18it/s]
{'loss': 0.0272, 'grad_norm': 0.2298377901315689, 'learning_rate': 4.44954128440367e-05, 'epoch': 5.82}                                                                                          
{'loss': 0.0272, 'grad_norm': 0.2298377901315689, 'learning_rate': 4.44954128440367e-05, 'epoch': 5.82}                                                                                          
 59%|██████████████████████████████████████████████████████████████████████████████████████████                                                              | 1375/2320 [01:56<01:42,  9.20it/s{'loss': 0.0222, 'grad_norm': 0.08756297081708908, 'learning_rate': 4.334862385321101e-05, 'epoch': 5.926724137931035}                                        | 1375/2320 [01:56<01:42,  9.20it/s]
{'loss': 0.0222, 'grad_norm': 0.08756297081708908, 'learning_rate': 4.334862385321101e-05, 'epoch': 5.93}                                                                                        
{'loss': 0.0222, 'grad_norm': 0.08756297081708908, 'learning_rate': 4.334862385321101e-05, 'epoch': 5.93}                                                                                        
 60%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 1392/2320 [01:58<01:39,  9.35it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.26it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.26it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.018500298261642456, 'eval_precision_micro': 0.9797297297297297, 'eval_recall_micro': 0.932475884244373, 'eval_f1_micro': 0.9555189456342668, 'eval_precision_macro': 0.39711711823772466, 'eval_recall_macro': 0.35208910703324287, 'eval_f1_macro': 0.36133089484815906, 'eval_precision_weighted': 0.938996070025009, 'eval_recall_weighted': 0.932475884244373, 'eval_f1_weighted': 0.9337883954659522, 'eval_precision_samples': 0.9374325782092773, 'eval_recall_samples': 0.9356346637900035, 'eval_f1_samples': 0.935814455231931, 'eval_f1': 0.9555189456342668, 'eval_subset_accuracy': 0.9309600862998921, 'eval_hamming_loss': 0.005139920045688178, 'eval_jaccard_micro': 0.9148264984227129, 'eval_jaccard_macro': 0.33890834555260746, 'eval_jaccard_weighted': 0.9146024355554002, 'eval_jaccard_samples': 0.9345559151384393, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9854669614390394, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9794455690984883, 'eval_avg_precision': 0.9854669614390394, 'eval_runtime': 0.8852, 'eval_samples_per_second': 1047.172, 'eval_steps_per_second': 32.759, 'epoch': 6.0}
{'eval_loss': 0.018500298261642456, 'eval_precision_micro': 0.9797297297297297, 'eval_recall_micro': 0.932475884244373, 'eval_f1_micro': 0.9555189456342668, 'eval_precision_macro': 0.39711711823772466, 'eval_recall_macro': 0.35208910703324287, 'eval_f1_macro': 0.36133089484815906, 'eval_precision_weighted': 0.938996070025009, 'eval_recall_weighted': 0.932475884244373, 'eval_f1_weighted': 0.9337883954659522, 'eval_precision_samples': 0.9374325782092773, 'eval_recall_samples': 0.9356346637900035, 'eval_f1_samples': 0.935814455231931, 'eval_f1': 0.9555189456342668, 'eval_subset_accuracy': 0.9309600862998921, 'eval_hamming_loss': 0.005139920045688178, 'eval_jaccard_micro': 0.9148264984227129, 'eval_jaccard_macro': 0.33890834555260746, 'eval_jaccard_weighted': 0.9146024355554002, 'eval_jaccard_samples': 0.9345559151384393, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9854669614390394, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9794455690984883, 'eval_avg_precision': 0.9854669614390394, 'eval_runtime': 0.8852, 'eval_samples_per_second': 1047.172, 'eval_steps_per_second': 32.759, 'epoch': 6.0}
{'eval_loss': 0.018500298261642456, 'eval_precision_micro': 0.9797297297297297, 'eval_recall_micro': 0.932475884244373, 'eval_f1_micro': 0.9555189456342668, 'eval_precision_macro': 0.39711711823772466, 'eval_recall_macro': 0.35208910703324287, 'eval_f1_macro': 0.36133089484815906, 'eval_precision_weighted': 0.938996070025009, 'eval_recall_weighted': 0.932475884244373, 'eval_f1_weighted': 0.9337883954659522, 'eval_precision_samples': 0.9374325782092773, 'eval_recall_samples': 0.9356346637900035, 'eval_f1_samples': 0.935814455231931, 'eval_f1': 0.9555189456342668, 'eval_subset_accuracy': 0.9309600862998921, 'eval_hamming_loss': 0.005139920045688178, 'eval_jaccard_micro': 0.9148264984227129, 'eval_jaccard_macro': 0.33890834555260746, 'eval_jaccard_weighted': 0.9146024355554002, 'eval_jaccard_samples': 0.9345559151384393, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9854669614390394, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9794455690984883, 'eval_avg_precision': 0.9854669614390394, 'eval_runtime': 0.8852, 'eval_samples_per_second': 1047.172, 'eval_steps_per_second': 32.759, 'epoch': 6.0}
 60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 1400/2320 [02:00<02:26,  6.28it/s{'loss': 0.0218, 'grad_norm': 0.16264708340168, 'learning_rate': 4.2201834862385324e-05, 'epoch': 6.0344827586206895}                                         | 1400/2320 [02:00<02:26,  6.28it/s]
{'loss': 0.0218, 'grad_norm': 0.16264708340168, 'learning_rate': 4.2201834862385324e-05, 'epoch': 6.03}                                                                                          
{'loss': 0.0218, 'grad_norm': 0.16264708340168, 'learning_rate': 4.2201834862385324e-05, 'epoch': 6.03}                                                                                          
 61%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 1425/2320 [02:03<01:36,  9.23it/s{'loss': 0.0233, 'grad_norm': 0.1252925992012024, 'learning_rate': 4.1055045871559636e-05, 'epoch': 6.142241379310345}                                        | 1425/2320 [02:03<01:36,  9.23it/s]
{'loss': 0.0233, 'grad_norm': 0.1252925992012024, 'learning_rate': 4.1055045871559636e-05, 'epoch': 6.14}                                                                                        
{'loss': 0.0233, 'grad_norm': 0.1252925992012024, 'learning_rate': 4.1055045871559636e-05, 'epoch': 6.14}                                                                                        
 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 1450/2320 [02:06<01:29,  9.73it/s{'loss': 0.0228, 'grad_norm': 0.2167772650718689, 'learning_rate': 3.990825688073395e-05, 'epoch': 6.25}                                                      | 1450/2320 [02:06<01:29,  9.73it/s]
{'loss': 0.0228, 'grad_norm': 0.2167772650718689, 'learning_rate': 3.990825688073395e-05, 'epoch': 6.25}                                                                                         
{'loss': 0.0228, 'grad_norm': 0.2167772650718689, 'learning_rate': 3.990825688073395e-05, 'epoch': 6.25}                                                                                         
 64%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 1475/2320 [02:08<01:33,  9.02it/s{'loss': 0.0237, 'grad_norm': 0.16030330955982208, 'learning_rate': 3.876146788990826e-05, 'epoch': 6.357758620689655}                                        | 1475/2320 [02:08<01:33,  9.02it/s]
{'loss': 0.0237, 'grad_norm': 0.16030330955982208, 'learning_rate': 3.876146788990826e-05, 'epoch': 6.36}                                                                                        
{'loss': 0.0237, 'grad_norm': 0.16030330955982208, 'learning_rate': 3.876146788990826e-05, 'epoch': 6.36}                                                                                        
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 1500/2320 [02:11<01:28,  9.24it/s{'loss': 0.0173, 'grad_norm': 0.09598328173160553, 'learning_rate': 3.761467889908257e-05, 'epoch': 6.4655172413793105}                                       | 1500/2320 [02:11<01:28,  9.24it/s]
{'loss': 0.0173, 'grad_norm': 0.09598328173160553, 'learning_rate': 3.761467889908257e-05, 'epoch': 6.47}                                                                                        
{'loss': 0.0173, 'grad_norm': 0.09598328173160553, 'learning_rate': 3.761467889908257e-05, 'epoch': 6.47}                                                                                        
 66%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                    | 1525/2320 [02:14<01:27,  9.10it/s{'loss': 0.0208, 'grad_norm': 0.1415066421031952, 'learning_rate': 3.646788990825688e-05, 'epoch': 6.573275862068965}                                         | 1525/2320 [02:14<01:27,  9.10it/s]
{'loss': 0.0208, 'grad_norm': 0.1415066421031952, 'learning_rate': 3.646788990825688e-05, 'epoch': 6.57}                                                                                         
{'loss': 0.0208, 'grad_norm': 0.1415066421031952, 'learning_rate': 3.646788990825688e-05, 'epoch': 6.57}                                                                                         
 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 1550/2320 [02:17<01:23,  9.27it/s{'loss': 0.0208, 'grad_norm': 0.09891604632139206, 'learning_rate': 3.5321100917431193e-05, 'epoch': 6.681034482758621}                                       | 1550/2320 [02:17<01:23,  9.27it/s]
{'loss': 0.0208, 'grad_norm': 0.09891604632139206, 'learning_rate': 3.5321100917431193e-05, 'epoch': 6.68}                                                                                       
{'loss': 0.0208, 'grad_norm': 0.09891604632139206, 'learning_rate': 3.5321100917431193e-05, 'epoch': 6.68}                                                                                       
 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 1575/2320 [02:19<01:20,  9.23it/s{'loss': 0.0206, 'grad_norm': 0.1681896448135376, 'learning_rate': 3.4174311926605505e-05, 'epoch': 6.788793103448276}                                        | 1575/2320 [02:19<01:20,  9.23it/s]
{'loss': 0.0206, 'grad_norm': 0.1681896448135376, 'learning_rate': 3.4174311926605505e-05, 'epoch': 6.79}                                                                                        
{'loss': 0.0206, 'grad_norm': 0.1681896448135376, 'learning_rate': 3.4174311926605505e-05, 'epoch': 6.79}                                                                                        
 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 1600/2320 [02:22<01:16,  9.40it/s{'loss': 0.0138, 'grad_norm': 0.09227962046861649, 'learning_rate': 3.302752293577982e-05, 'epoch': 6.896551724137931}                                        | 1600/2320 [02:22<01:16,  9.40it/s]
{'loss': 0.0138, 'grad_norm': 0.09227962046861649, 'learning_rate': 3.302752293577982e-05, 'epoch': 6.9}                                                                                         
{'loss': 0.0138, 'grad_norm': 0.09227962046861649, 'learning_rate': 3.302752293577982e-05, 'epoch': 6.9}                                                                                         
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1624/2320 [02:25<01:14,  9.35it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.27it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.27it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.016249842941761017, 'eval_precision_micro': 0.9810690423162584, 'eval_recall_micro': 0.9442658092175777, 'eval_f1_micro': 0.9623156744948116, 'eval_precision_macro': 0.4504425602543626, 'eval_recall_macro': 0.39912010292983796, 'eval_f1_macro': 0.4138680698097533, 'eval_precision_weighted': 0.9571061388613038, 'eval_recall_weighted': 0.9442658092175777, 'eval_f1_weighted': 0.9474638484114902, 'eval_precision_samples': 0.9487594390507011, 'eval_recall_samples': 0.9469615246314275, 'eval_f1_samples': 0.9473211075152823, 'eval_f1': 0.9623156744948116, 'eval_subset_accuracy': 0.9439050701186623, 'eval_hamming_loss': 0.00437845040928993, 'eval_jaccard_micro': 0.9273684210526316, 'eval_jaccard_macro': 0.38353807739645374, 'eval_jaccard_weighted': 0.9273018228965996, 'eval_jaccard_samples': 0.9464221503056454, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9887082960969382, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9822587815276235, 'eval_avg_precision': 0.9887082960969382, 'eval_runtime': 0.8833, 'eval_samples_per_second': 1049.464, 'eval_steps_per_second': 32.831, 'epoch': 7.0}
{'eval_loss': 0.016249842941761017, 'eval_precision_micro': 0.9810690423162584, 'eval_recall_micro': 0.9442658092175777, 'eval_f1_micro': 0.9623156744948116, 'eval_precision_macro': 0.4504425602543626, 'eval_recall_macro': 0.39912010292983796, 'eval_f1_macro': 0.4138680698097533, 'eval_precision_weighted': 0.9571061388613038, 'eval_recall_weighted': 0.9442658092175777, 'eval_f1_weighted': 0.9474638484114902, 'eval_precision_samples': 0.9487594390507011, 'eval_recall_samples': 0.9469615246314275, 'eval_f1_samples': 0.9473211075152823, 'eval_f1': 0.9623156744948116, 'eval_subset_accuracy': 0.9439050701186623, 'eval_hamming_loss': 0.00437845040928993, 'eval_jaccard_micro': 0.9273684210526316, 'eval_jaccard_macro': 0.38353807739645374, 'eval_jaccard_weighted': 0.9273018228965996, 'eval_jaccard_samples': 0.9464221503056454, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9887082960969382, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9822587815276235, 'eval_avg_precision': 0.9887082960969382, 'eval_runtime': 0.8833, 'eval_samples_per_second': 1049.464, 'eval_steps_per_second': 32.831, 'epoch': 7.0}
{'eval_loss': 0.016249842941761017, 'eval_precision_micro': 0.9810690423162584, 'eval_recall_micro': 0.9442658092175777, 'eval_f1_micro': 0.9623156744948116, 'eval_precision_macro': 0.4504425602543626, 'eval_recall_macro': 0.39912010292983796, 'eval_f1_macro': 0.4138680698097533, 'eval_precision_weighted': 0.9571061388613038, 'eval_recall_weighted': 0.9442658092175777, 'eval_f1_weighted': 0.9474638484114902, 'eval_precision_samples': 0.9487594390507011, 'eval_recall_samples': 0.9469615246314275, 'eval_f1_samples': 0.9473211075152823, 'eval_f1': 0.9623156744948116, 'eval_subset_accuracy': 0.9439050701186623, 'eval_hamming_loss': 0.00437845040928993, 'eval_jaccard_micro': 0.9273684210526316, 'eval_jaccard_macro': 0.38353807739645374, 'eval_jaccard_weighted': 0.9273018228965996, 'eval_jaccard_samples': 0.9464221503056454, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9887082960969382, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9822587815276235, 'eval_avg_precision': 0.9887082960969382, 'eval_runtime': 0.8833, 'eval_samples_per_second': 1049.464, 'eval_steps_per_second': 32.831, 'epoch': 7.0}
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 1625/2320 [02:26<05:50,  1.98it/s{'loss': 0.0141, 'grad_norm': 0.017443975433707237, 'learning_rate': 3.188073394495413e-05, 'epoch': 7.004310344827586}                                       | 1625/2320 [02:26<05:50,  1.98it/s]
{'loss': 0.0141, 'grad_norm': 0.017443975433707237, 'learning_rate': 3.188073394495413e-05, 'epoch': 7.0}                                                                                        
{'loss': 0.0141, 'grad_norm': 0.017443975433707237, 'learning_rate': 3.188073394495413e-05, 'epoch': 7.0}                                                                                        
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 1650/2320 [02:29<01:12,  9.26it/s{'loss': 0.0249, 'grad_norm': 0.15794962644577026, 'learning_rate': 3.073394495412844e-05, 'epoch': 7.112068965517241}                                        | 1650/2320 [02:29<01:12,  9.26it/s]
{'loss': 0.0249, 'grad_norm': 0.15794962644577026, 'learning_rate': 3.073394495412844e-05, 'epoch': 7.11}                                                                                        
{'loss': 0.0249, 'grad_norm': 0.15794962644577026, 'learning_rate': 3.073394495412844e-05, 'epoch': 7.11}                                                                                        
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 1675/2320 [02:31<01:08,  9.35it/s{'loss': 0.0149, 'grad_norm': 0.08304981142282486, 'learning_rate': 2.9587155963302755e-05, 'epoch': 7.219827586206897}                                       | 1675/2320 [02:31<01:08,  9.35it/s]
{'loss': 0.0149, 'grad_norm': 0.08304981142282486, 'learning_rate': 2.9587155963302755e-05, 'epoch': 7.22}                                                                                       
{'loss': 0.0149, 'grad_norm': 0.08304981142282486, 'learning_rate': 2.9587155963302755e-05, 'epoch': 7.22}                                                                                       
 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 1700/2320 [02:34<01:08,  9.01it/s{'loss': 0.0192, 'grad_norm': 0.020713672041893005, 'learning_rate': 2.8440366972477066e-05, 'epoch': 7.327586206896552}                                      | 1700/2320 [02:34<01:08,  9.01it/s]
{'loss': 0.0192, 'grad_norm': 0.020713672041893005, 'learning_rate': 2.8440366972477066e-05, 'epoch': 7.33}                                                                                      
{'loss': 0.0192, 'grad_norm': 0.020713672041893005, 'learning_rate': 2.8440366972477066e-05, 'epoch': 7.33}                                                                                      
 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 1725/2320 [02:37<01:05,  9.06it/s{'loss': 0.0149, 'grad_norm': 0.06642761081457138, 'learning_rate': 2.7293577981651375e-05, 'epoch': 7.435344827586207}                                       | 1725/2320 [02:37<01:05,  9.06it/s]
{'loss': 0.0149, 'grad_norm': 0.06642761081457138, 'learning_rate': 2.7293577981651375e-05, 'epoch': 7.44}                                                                                       
{'loss': 0.0149, 'grad_norm': 0.06642761081457138, 'learning_rate': 2.7293577981651375e-05, 'epoch': 7.44}                                                                                       
 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 1750/2320 [02:40<01:01,  9.22it/s{'loss': 0.0186, 'grad_norm': 0.06819099932909012, 'learning_rate': 2.614678899082569e-05, 'epoch': 7.543103448275862}██▋                                     | 1750/2320 [02:40<01:01,  9.22it/s]
{'loss': 0.0186, 'grad_norm': 0.06819099932909012, 'learning_rate': 2.614678899082569e-05, 'epoch': 7.54}                                                                                        
{'loss': 0.0186, 'grad_norm': 0.06819099932909012, 'learning_rate': 2.614678899082569e-05, 'epoch': 7.54}                                                                                        
 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 1775/2320 [02:42<01:00,  9.03it/s{'loss': 0.0223, 'grad_norm': 0.14112713932991028, 'learning_rate': 2.5e-05, 'epoch': 7.650862068965517}██████████████████▎                                   | 1775/2320 [02:42<01:00,  9.03it/s]
{'loss': 0.0223, 'grad_norm': 0.14112713932991028, 'learning_rate': 2.5e-05, 'epoch': 7.65}                                                                                                      
{'loss': 0.0223, 'grad_norm': 0.14112713932991028, 'learning_rate': 2.5e-05, 'epoch': 7.65}                                                                                                      
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 1800/2320 [02:45<00:56,  9.24it/s{'loss': 0.0142, 'grad_norm': 0.03922874107956886, 'learning_rate': 2.3853211009174313e-05, 'epoch': 7.758620689655173}████▉                                  | 1800/2320 [02:45<00:56,  9.24it/s]
{'loss': 0.0142, 'grad_norm': 0.03922874107956886, 'learning_rate': 2.3853211009174313e-05, 'epoch': 7.76}                                                                                       
{'loss': 0.0142, 'grad_norm': 0.03922874107956886, 'learning_rate': 2.3853211009174313e-05, 'epoch': 7.76}                                                                                       
 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                | 1825/2320 [02:48<00:51,  9.54it/s{'loss': 0.0159, 'grad_norm': 0.016964968293905258, 'learning_rate': 2.2706422018348624e-05, 'epoch': 7.866379310344827}█████▌                                | 1825/2320 [02:48<00:51,  9.54it/s]
{'loss': 0.0159, 'grad_norm': 0.016964968293905258, 'learning_rate': 2.2706422018348624e-05, 'epoch': 7.87}                                                                                      
{'loss': 0.0159, 'grad_norm': 0.016964968293905258, 'learning_rate': 2.2706422018348624e-05, 'epoch': 7.87}                                                                                      
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 1850/2320 [02:51<00:51,  9.08it/s{'loss': 0.0182, 'grad_norm': 0.019656455144286156, 'learning_rate': 2.155963302752294e-05, 'epoch': 7.974137931034483}████████▏                              | 1850/2320 [02:51<00:51,  9.08it/s]
{'loss': 0.0182, 'grad_norm': 0.019656455144286156, 'learning_rate': 2.155963302752294e-05, 'epoch': 7.97}                                                                                       
{'loss': 0.0182, 'grad_norm': 0.019656455144286156, 'learning_rate': 2.155963302752294e-05, 'epoch': 7.97}                                                                                       
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 1856/2320 [02:51<00:50,  9.16it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.22it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.22it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.01484639011323452, 'eval_precision_micro': 0.977997799779978, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9652551574375678, 'eval_precision_macro': 0.44232911781004036, 'eval_recall_macro': 0.4220701529502595, 'eval_f1_macro': 0.43037321013552665, 'eval_precision_weighted': 0.9541550708580667, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9529333689133878, 'eval_precision_samples': 0.9568500539374326, 'eval_recall_samples': 0.9555915138439409, 'eval_f1_samples': 0.9555915138439409, 'eval_f1': 0.9652551574375678, 'eval_subset_accuracy': 0.9514563106796117, 'eval_hamming_loss': 0.004061171394123993, 'eval_jaccard_micro': 0.9328436516264428, 'eval_jaccard_macro': 0.40090064887324034, 'eval_jaccard_weighted': 0.9334621239857519, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9911966300975014, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9859661631540457, 'eval_avg_precision': 0.9911966300975014, 'eval_runtime': 0.8837, 'eval_samples_per_second': 1048.995, 'eval_steps_per_second': 32.816, 'epoch': 8.0}
{'eval_loss': 0.01484639011323452, 'eval_precision_micro': 0.977997799779978, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9652551574375678, 'eval_precision_macro': 0.44232911781004036, 'eval_recall_macro': 0.4220701529502595, 'eval_f1_macro': 0.43037321013552665, 'eval_precision_weighted': 0.9541550708580667, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9529333689133878, 'eval_precision_samples': 0.9568500539374326, 'eval_recall_samples': 0.9555915138439409, 'eval_f1_samples': 0.9555915138439409, 'eval_f1': 0.9652551574375678, 'eval_subset_accuracy': 0.9514563106796117, 'eval_hamming_loss': 0.004061171394123993, 'eval_jaccard_micro': 0.9328436516264428, 'eval_jaccard_macro': 0.40090064887324034, 'eval_jaccard_weighted': 0.9334621239857519, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9911966300975014, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9859661631540457, 'eval_avg_precision': 0.9911966300975014, 'eval_runtime': 0.8837, 'eval_samples_per_second': 1048.995, 'eval_steps_per_second': 32.816, 'epoch': 8.0}
{'eval_loss': 0.01484639011323452, 'eval_precision_micro': 0.977997799779978, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.9652551574375678, 'eval_precision_macro': 0.44232911781004036, 'eval_recall_macro': 0.4220701529502595, 'eval_f1_macro': 0.43037321013552665, 'eval_precision_weighted': 0.9541550708580667, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9529333689133878, 'eval_precision_samples': 0.9568500539374326, 'eval_recall_samples': 0.9555915138439409, 'eval_f1_samples': 0.9555915138439409, 'eval_f1': 0.9652551574375678, 'eval_subset_accuracy': 0.9514563106796117, 'eval_hamming_loss': 0.004061171394123993, 'eval_jaccard_micro': 0.9328436516264428, 'eval_jaccard_macro': 0.40090064887324034, 'eval_jaccard_weighted': 0.9334621239857519, 'eval_jaccard_samples': 0.9545127651923767, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9911966300975014, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9859661631540457, 'eval_avg_precision': 0.9911966300975014, 'eval_runtime': 0.8837, 'eval_samples_per_second': 1048.995, 'eval_steps_per_second': 32.816, 'epoch': 8.0}
 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 1875/2320 [02:55<00:49,  9.03it/s{'loss': 0.0104, 'grad_norm': 0.08162084966897964, 'learning_rate': 2.0412844036697248e-05, 'epoch': 8.081896551724139}█████████▊                             | 1875/2320 [02:55<00:49,  9.03it/s]
{'loss': 0.0104, 'grad_norm': 0.08162084966897964, 'learning_rate': 2.0412844036697248e-05, 'epoch': 8.08}                                                                                       
{'loss': 0.0104, 'grad_norm': 0.08162084966897964, 'learning_rate': 2.0412844036697248e-05, 'epoch': 8.08}                                                                                       
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 1900/2320 [02:57<00:45,  9.22it/s{'loss': 0.0183, 'grad_norm': 0.1716592013835907, 'learning_rate': 1.9266055045871563e-05, 'epoch': 8.189655172413794}████████████▍                           | 1900/2320 [02:57<00:45,  9.22it/s]
{'loss': 0.0183, 'grad_norm': 0.1716592013835907, 'learning_rate': 1.9266055045871563e-05, 'epoch': 8.19}                                                                                        
{'loss': 0.0183, 'grad_norm': 0.1716592013835907, 'learning_rate': 1.9266055045871563e-05, 'epoch': 8.19}                                                                                        
 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 1925/2320 [03:00<00:43,  9.08it/s{'loss': 0.0145, 'grad_norm': 0.06283333152532578, 'learning_rate': 1.811926605504587e-05, 'epoch': 8.297413793103448}██████████████                          | 1925/2320 [03:00<00:43,  9.08it/s]
{'loss': 0.0145, 'grad_norm': 0.06283333152532578, 'learning_rate': 1.811926605504587e-05, 'epoch': 8.3}                                                                                         
{'loss': 0.0145, 'grad_norm': 0.06283333152532578, 'learning_rate': 1.811926605504587e-05, 'epoch': 8.3}                                                                                         
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 1950/2320 [03:03<00:39,  9.35it/s{'loss': 0.0176, 'grad_norm': 0.10124209523200989, 'learning_rate': 1.6972477064220186e-05, 'epoch': 8.405172413793103}██████████████▊                        | 1950/2320 [03:03<00:39,  9.35it/s]
{'loss': 0.0176, 'grad_norm': 0.10124209523200989, 'learning_rate': 1.6972477064220186e-05, 'epoch': 8.41}                                                                                       
{'loss': 0.0176, 'grad_norm': 0.10124209523200989, 'learning_rate': 1.6972477064220186e-05, 'epoch': 8.41}                                                                                       
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 1975/2320 [03:05<00:37,  9.16it/s{'loss': 0.0187, 'grad_norm': 0.03640352934598923, 'learning_rate': 1.5825688073394497e-05, 'epoch': 8.512931034482758}████████████████▍                      | 1975/2320 [03:05<00:37,  9.16it/s]
{'loss': 0.0187, 'grad_norm': 0.03640352934598923, 'learning_rate': 1.5825688073394497e-05, 'epoch': 8.51}                                                                                       
{'loss': 0.0187, 'grad_norm': 0.03640352934598923, 'learning_rate': 1.5825688073394497e-05, 'epoch': 8.51}                                                                                       
 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 2000/2320 [03:08<00:35,  9.02it/s{'loss': 0.0153, 'grad_norm': 0.04405440762639046, 'learning_rate': 1.4678899082568809e-05, 'epoch': 8.620689655172415}██████████████████                     | 2000/2320 [03:08<00:35,  9.02it/s]
{'loss': 0.0153, 'grad_norm': 0.04405440762639046, 'learning_rate': 1.4678899082568809e-05, 'epoch': 8.62}                                                                                       
{'loss': 0.0153, 'grad_norm': 0.04405440762639046, 'learning_rate': 1.4678899082568809e-05, 'epoch': 8.62}                                                                                       
 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 2025/2320 [03:11<00:31,  9.24it/s{'loss': 0.0182, 'grad_norm': 0.10790074616670609, 'learning_rate': 1.3532110091743119e-05, 'epoch': 8.72844827586207}████████████████████▋                   | 2025/2320 [03:11<00:31,  9.24it/s]
{'loss': 0.0182, 'grad_norm': 0.10790074616670609, 'learning_rate': 1.3532110091743119e-05, 'epoch': 8.73}                                                                                       
{'loss': 0.0182, 'grad_norm': 0.10790074616670609, 'learning_rate': 1.3532110091743119e-05, 'epoch': 8.73}                                                                                       
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 2050/2320 [03:14<00:29,  9.08it/s{'loss': 0.0152, 'grad_norm': 0.00929331872612238, 'learning_rate': 1.2385321100917432e-05, 'epoch': 8.836206896551724}█████████████████████▎                 | 2050/2320 [03:14<00:29,  9.08it/s]
{'loss': 0.0152, 'grad_norm': 0.00929331872612238, 'learning_rate': 1.2385321100917432e-05, 'epoch': 8.84}                                                                                       
{'loss': 0.0152, 'grad_norm': 0.00929331872612238, 'learning_rate': 1.2385321100917432e-05, 'epoch': 8.84}                                                                                       
 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 2075/2320 [03:16<00:26,  9.15it/s{'loss': 0.0183, 'grad_norm': 0.141557514667511, 'learning_rate': 1.1238532110091744e-05, 'epoch': 8.943965517241379}████████████████████████▉                | 2075/2320 [03:16<00:26,  9.15it/s]
{'loss': 0.0183, 'grad_norm': 0.141557514667511, 'learning_rate': 1.1238532110091744e-05, 'epoch': 8.94}                                                                                         
{'loss': 0.0183, 'grad_norm': 0.141557514667511, 'learning_rate': 1.1238532110091744e-05, 'epoch': 8.94}                                                                                         
 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 2088/2320 [03:18<00:22, 10.19it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.█████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.38it/s]
  _warn_prf(average, modifier, msg_start, len(result))███████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 39.38it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.01420549862086773, 'eval_precision_micro': 0.9812568908489526, 'eval_recall_micro': 0.9539121114683816, 'eval_f1_micro': 0.9673913043478262, 'eval_precision_macro': 0.4475669420460037, 'eval_recall_macro': 0.42830064307042237, 'eval_f1_macro': 0.43717106120910587, 'eval_precision_weighted': 0.9572467631088848, 'eval_recall_weighted': 0.9539121114683816, 'eval_f1_weighted': 0.9553168425290843, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.9566702624955051, 'eval_f1_samples': 0.95702984537936, 'eval_f1': 0.9673913043478262, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003807348181991243, 'eval_jaccard_micro': 0.9368421052631579, 'eval_jaccard_macro': 0.41189941141176084, 'eval_jaccard_weighted': 0.9374136178750652, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914429712704356, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9864354827513206, 'eval_avg_precision': 0.9914429712704356, 'eval_runtime': 0.8704, 'eval_samples_per_second': 1065.051, 'eval_steps_per_second': 33.319, 'epoch': 9.0}
{'eval_loss': 0.01420549862086773, 'eval_precision_micro': 0.9812568908489526, 'eval_recall_micro': 0.9539121114683816, 'eval_f1_micro': 0.9673913043478262, 'eval_precision_macro': 0.4475669420460037, 'eval_recall_macro': 0.42830064307042237, 'eval_f1_macro': 0.43717106120910587, 'eval_precision_weighted': 0.9572467631088848, 'eval_recall_weighted': 0.9539121114683816, 'eval_f1_weighted': 0.9553168425290843, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.9566702624955051, 'eval_f1_samples': 0.95702984537936, 'eval_f1': 0.9673913043478262, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003807348181991243, 'eval_jaccard_micro': 0.9368421052631579, 'eval_jaccard_macro': 0.41189941141176084, 'eval_jaccard_weighted': 0.9374136178750652, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914429712704356, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9864354827513206, 'eval_avg_precision': 0.9914429712704356, 'eval_runtime': 0.8704, 'eval_samples_per_second': 1065.051, 'eval_steps_per_second': 33.319, 'epoch': 9.0}
{'eval_loss': 0.01420549862086773, 'eval_precision_micro': 0.9812568908489526, 'eval_recall_micro': 0.9539121114683816, 'eval_f1_micro': 0.9673913043478262, 'eval_precision_macro': 0.4475669420460037, 'eval_recall_macro': 0.42830064307042237, 'eval_f1_macro': 0.43717106120910587, 'eval_precision_weighted': 0.9572467631088848, 'eval_recall_weighted': 0.9539121114683816, 'eval_f1_weighted': 0.9553168425290843, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.9566702624955051, 'eval_f1_samples': 0.95702984537936, 'eval_f1': 0.9673913043478262, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003807348181991243, 'eval_jaccard_micro': 0.9368421052631579, 'eval_jaccard_macro': 0.41189941141176084, 'eval_jaccard_weighted': 0.9374136178750652, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914429712704356, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9864354827513206, 'eval_avg_precision': 0.9914429712704356, 'eval_runtime': 0.8704, 'eval_samples_per_second': 1065.051, 'eval_steps_per_second': 33.319, 'epoch': 9.0}
 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 2100/2320 [03:20<00:27,  7.97it/s{'loss': 0.017, 'grad_norm': 0.1423182636499405, 'learning_rate': 1.0091743119266055e-05, 'epoch': 9.051724137931034}██████████████████████████▌              | 2100/2320 [03:20<00:27,  7.97it/s]
{'loss': 0.017, 'grad_norm': 0.1423182636499405, 'learning_rate': 1.0091743119266055e-05, 'epoch': 9.05}                                                                                         
{'loss': 0.017, 'grad_norm': 0.1423182636499405, 'learning_rate': 1.0091743119266055e-05, 'epoch': 9.05}                                                                                         
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 2125/2320 [03:23<00:21,  9.18it/s{'loss': 0.0166, 'grad_norm': 0.21366114914417267, 'learning_rate': 8.944954128440369e-06, 'epoch': 9.15948275862069}████████████████████████████▏            | 2125/2320 [03:23<00:21,  9.18it/s]
{'loss': 0.0166, 'grad_norm': 0.21366114914417267, 'learning_rate': 8.944954128440369e-06, 'epoch': 9.16}                                                                                        
{'loss': 0.0166, 'grad_norm': 0.21366114914417267, 'learning_rate': 8.944954128440369e-06, 'epoch': 9.16}                                                                                        
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 2150/2320 [03:26<00:18,  9.04it/s{'loss': 0.0148, 'grad_norm': 0.14736568927764893, 'learning_rate': 7.79816513761468e-06, 'epoch': 9.267241379310345}█████████████████████████████▊           | 2150/2320 [03:26<00:18,  9.04it/s]
{'loss': 0.0148, 'grad_norm': 0.14736568927764893, 'learning_rate': 7.79816513761468e-06, 'epoch': 9.27}                                                                                         
{'loss': 0.0148, 'grad_norm': 0.14736568927764893, 'learning_rate': 7.79816513761468e-06, 'epoch': 9.27}                                                                                         
 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 2175/2320 [03:28<00:15,  9.12it/s{'loss': 0.0138, 'grad_norm': 0.1550186425447464, 'learning_rate': 6.651376146788992e-06, 'epoch': 9.375}███████████████████████████████████████████▌         | 2175/2320 [03:28<00:15,  9.11it/s]
{'loss': 0.0138, 'grad_norm': 0.1550186425447464, 'learning_rate': 6.651376146788992e-06, 'epoch': 9.38}                                                                                         
{'loss': 0.0138, 'grad_norm': 0.1550186425447464, 'learning_rate': 6.651376146788992e-06, 'epoch': 9.38}                                                                                         
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 2199/2320 [03:31<00:12,  9.37it/s{'loss': 0.0192, 'grad_norm': 0.17199601233005524, 'learning_rate': 5.504587155963303e-06, 'epoch': 9.482758620689655}████████████████████████████████        | 2199/2320 [03:31<00:12,  9.37it/s]
{'loss': 0.0192, 'grad_norm': 0.17199601233005524, 'learning_rate': 5.504587155963303e-06, 'epoch': 9.48}                                                                                        
{'loss': 0.0192, 'grad_norm': 0.17199601233005524, 'learning_rate': 5.504587155963303e-06, 'epoch': 9.48}                                                                                        
 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2225/2320 [03:34<00:10,  9.34it/s{'loss': 0.0194, 'grad_norm': 0.08990390598773956, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59051724137931}██████████████████████████████████▊      | 2225/2320 [03:34<00:10,  9.34it/s]
{'loss': 0.0194, 'grad_norm': 0.08990390598773956, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59}                                                                                        
{'loss': 0.0194, 'grad_norm': 0.08990390598773956, 'learning_rate': 4.403669724770643e-06, 'epoch': 9.59}                                                                                        
 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 2249/2320 [03:37<00:07,  9.50it/s{'loss': 0.0136, 'grad_norm': 0.034373484551906586, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.698275862068966}█████████████████████████████████▎    | 2249/2320 [03:37<00:07,  9.50it/s]
{'loss': 0.0136, 'grad_norm': 0.034373484551906586, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.7}                                                                                       
{'loss': 0.0136, 'grad_norm': 0.034373484551906586, 'learning_rate': 3.2568807339449546e-06, 'epoch': 9.7}                                                                                       
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 2275/2320 [03:39<00:04,  9.29it/s{'loss': 0.0174, 'grad_norm': 0.15047502517700195, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.806034482758621}█████████████████████████████████████   | 2275/2320 [03:39<00:04,  9.29it/s]
{'loss': 0.0174, 'grad_norm': 0.15047502517700195, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.81}                                                                                        
{'loss': 0.0174, 'grad_norm': 0.15047502517700195, 'learning_rate': 2.110091743119266e-06, 'epoch': 9.81}                                                                                        
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 2300/2320 [03:42<00:02,  9.15it/s{'loss': 0.0168, 'grad_norm': 0.07068727165460587, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.913793103448276}███████████████████████████████████████▋ | 2300/2320 [03:42<00:02,  9.15it/s]
{'loss': 0.0168, 'grad_norm': 0.07068727165460587, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.91}                                                                                         
{'loss': 0.0168, 'grad_norm': 0.07068727165460587, 'learning_rate': 9.63302752293578e-07, 'epoch': 9.91}                                                                                         
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2319/2320 [03:44<00:00,  9.37it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.███████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.77it/s]
  _warn_prf(average, modifier, msg_start, len(result))█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 28/29 [00:00<00:00, 38.77it/s]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.014158723875880241, 'eval_precision_micro': 0.980154355016538, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.966304347826087, 'eval_precision_macro': 0.4474462916889661, 'eval_recall_macro': 0.4276542306593041, 'eval_f1_macro': 0.4367956707836623, 'eval_precision_weighted': 0.9561335687516435, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9542368121241234, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.956130888169723, 'eval_f1_samples': 0.9566702624955051, 'eval_f1': 0.966304347826087, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003934259788057618, 'eval_jaccard_micro': 0.9348054679284963, 'eval_jaccard_macro': 0.4112012012012012, 'eval_jaccard_weighted': 0.9353589495175777, 'eval_jaccard_samples': 0.9555915138439409, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914645716507892, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9863129960758414, 'eval_avg_precision': 0.9914645716507892, 'eval_runtime': 0.8675, 'eval_samples_per_second': 1068.608, 'eval_steps_per_second': 33.43, 'epoch': 10.0}
{'eval_loss': 0.014158723875880241, 'eval_precision_micro': 0.980154355016538, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.966304347826087, 'eval_precision_macro': 0.4474462916889661, 'eval_recall_macro': 0.4276542306593041, 'eval_f1_macro': 0.4367956707836623, 'eval_precision_weighted': 0.9561335687516435, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9542368121241234, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.956130888169723, 'eval_f1_samples': 0.9566702624955051, 'eval_f1': 0.966304347826087, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003934259788057618, 'eval_jaccard_micro': 0.9348054679284963, 'eval_jaccard_macro': 0.4112012012012012, 'eval_jaccard_weighted': 0.9353589495175777, 'eval_jaccard_samples': 0.9555915138439409, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914645716507892, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9863129960758414, 'eval_avg_precision': 0.9914645716507892, 'eval_runtime': 0.8675, 'eval_samples_per_second': 1068.608, 'eval_steps_per_second': 33.43, 'epoch': 10.0}
{'eval_loss': 0.014158723875880241, 'eval_precision_micro': 0.980154355016538, 'eval_recall_micro': 0.9528403001071811, 'eval_f1_micro': 0.966304347826087, 'eval_precision_macro': 0.4474462916889661, 'eval_recall_macro': 0.4276542306593041, 'eval_f1_macro': 0.4367956707836623, 'eval_precision_weighted': 0.9561335687516435, 'eval_recall_weighted': 0.9528403001071811, 'eval_f1_weighted': 0.9542368121241234, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.956130888169723, 'eval_f1_samples': 0.9566702624955051, 'eval_f1': 0.966304347826087, 'eval_subset_accuracy': 0.9525350593311759, 'eval_hamming_loss': 0.003934259788057618, 'eval_jaccard_micro': 0.9348054679284963, 'eval_jaccard_macro': 0.4112012012012012, 'eval_jaccard_weighted': 0.9353589495175777, 'eval_jaccard_samples': 0.9555915138439409, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914645716507892, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9863129960758414, 'eval_avg_precision': 0.9914645716507892, 'eval_runtime': 0.8675, 'eval_samples_per_second': 1068.608, 'eval_steps_per_second': 33.43, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:46<00:00,  9.37it/s{'train_runtime': 226.7139, 'train_samples_per_second': 163.51, 'train_steps_per_second': 10.233, 'train_loss': 0.05811093810560374, 'epoch': 10.0}███████████| 2320/2320 [03:46<00:00,  9.37it/s]
{'train_runtime': 226.7139, 'train_samples_per_second': 163.51, 'train_steps_per_second': 10.233, 'train_loss': 0.05811093810560374, 'epoch': 10.0}                                              
{'train_runtime': 226.7139, 'train_samples_per_second': 163.51, 'train_steps_per_second': 10.233, 'train_loss': 0.05811093810560374, 'epoch': 10.0}                                              
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:46<00:00, 10.23it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2320/2320 [03:46<00:00, 10.23it/s]
2025-04-26 12:01:57,369 - INFO - TrainMain - *** Training Finished ***
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0581
  train_runtime            = 0:03:46.71
  train_samples_per_second =     163.51
  train_steps_per_second   =     10.233
2025-04-26 12:01:57,375 - INFO - TrainMain - Training metrics: {'train_runtime': 226.7139, 'train_samples_per_second': 163.51, 'train_steps_per_second': 10.233, 'train_loss': 0.05811093810560374, 'epoch': 10.0}
2025-04-26 12:01:57,376 - INFO - TrainMain - *** Starting Final Evaluation on Validation Set ***
 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 25/29 [00:00<00:00, 39.60it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.01420549862086773, 'eval_precision_micro': 0.9812568908489526, 'eval_recall_micro': 0.9539121114683816, 'eval_f1_micro': 0.9673913043478262, 'eval_precision_macro': 0.4475669420460037, 'eval_recall_macro': 0.42830064307042237, 'eval_f1_macro': 0.43717106120910587, 'eval_precision_weighted': 0.9572467631088848, 'eval_recall_weighted': 0.9539121114683816, 'eval_f1_weighted': 0.9553168425290843, 'eval_precision_samples': 0.9584681769147788, 'eval_recall_samples': 0.9566702624955051, 'eval_f1_samples': 0.95702984537936, 'eval_f1': 0.9673913043478262, 'eval_subset_accuracy': 0.95361380798274, 'eval_hamming_loss': 0.003807348181991243, 'eval_jaccard_micro': 0.9368421052631579, 'eval_jaccard_macro': 0.41189941141176084, 'eval_jaccard_weighted': 0.9374136178750652, 'eval_jaccard_samples': 0.956130888169723, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9914429712704356, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9864354827513206, 'eval_avg_precision': 0.9914429712704356, 'eval_runtime': 0.8548, 'eval_samples_per_second': 1084.515, 'eval_steps_per_second': 33.928, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 35.07it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 35.07it/s]
***** eval metrics *****
  eval_epoch                       =       10.0
  eval_eval_avg_precision          =     0.9914
  eval_eval_avg_precision_macro    =        nan
  eval_eval_avg_precision_micro    =     0.9914
  eval_eval_avg_precision_weighted =     0.9864
  eval_eval_f1                     =     0.9674
  eval_eval_f1_macro               =     0.4372
  eval_eval_f1_micro               =     0.9674
  eval_eval_f1_samples             =      0.957
  eval_eval_f1_weighted            =     0.9553
  eval_eval_hamming_loss           =     0.0038
  eval_eval_jaccard_macro          =     0.4119
  eval_eval_jaccard_micro          =     0.9368
  eval_eval_jaccard_samples        =     0.9561
  eval_eval_jaccard_weighted       =     0.9374
  eval_eval_loss                   =     0.0142
  eval_eval_precision_macro        =     0.4476
  eval_eval_precision_micro        =     0.9813
  eval_eval_precision_samples      =     0.9585
  eval_eval_precision_weighted     =     0.9572
  eval_eval_recall_macro           =     0.4283
  eval_eval_recall_micro           =     0.9539
  eval_eval_recall_samples         =     0.9567
  eval_eval_recall_weighted        =     0.9539
  eval_eval_roc_auc                =        nan
  eval_eval_roc_auc_macro          =        nan
  eval_eval_roc_auc_micro          =        nan
  eval_eval_roc_auc_weighted       =        nan
  eval_eval_runtime                = 0:00:00.85
  eval_eval_samples_per_second     =   1084.515
  eval_eval_steps_per_second       =     33.928
  eval_eval_subset_accuracy        =     0.9536
2025-04-26 12:01:58,242 - INFO - TrainMain - Final validation metrics: {'eval_eval_loss': 0.01420549862086773, 'eval_eval_precision_micro': 0.9812568908489526, 'eval_eval_recall_micro': 0.9539121114683816, 'eval_eval_f1_micro': 0.9673913043478262, 'eval_eval_precision_macro': 0.4475669420460037, 'eval_eval_recall_macro': 0.42830064307042237, 'eval_eval_f1_macro': 0.43717106120910587, 'eval_eval_precision_weighted': 0.9572467631088848, 'eval_eval_recall_weighted': 0.9539121114683816, 'eval_eval_f1_weighted': 0.9553168425290843, 'eval_eval_precision_samples': 0.9584681769147788, 'eval_eval_recall_samples': 0.9566702624955051, 'eval_eval_f1_samples': 0.95702984537936, 'eval_eval_f1': 0.9673913043478262, 'eval_eval_subset_accuracy': 0.95361380798274, 'eval_eval_hamming_loss': 0.003807348181991243, 'eval_eval_jaccard_micro': 0.9368421052631579, 'eval_eval_jaccard_macro': 0.41189941141176084, 'eval_eval_jaccard_weighted': 0.9374136178750652, 'eval_eval_jaccard_samples': 0.956130888169723, 'eval_eval_roc_auc_micro': nan, 'eval_eval_roc_auc_macro': nan, 'eval_eval_roc_auc_weighted': nan, 'eval_eval_roc_auc': nan, 'eval_eval_avg_precision_micro': 0.9914429712704356, 'eval_eval_avg_precision_macro': nan, 'eval_eval_avg_precision_weighted': 0.9864354827513206, 'eval_eval_avg_precision': 0.9914429712704356, 'eval_eval_runtime': 0.8548, 'eval_eval_samples_per_second': 1084.515, 'eval_eval_steps_per_second': 33.928, 'eval_epoch': 10.0}
2025-04-26 12:01:58,242 - INFO - TrainMain - *** Starting Test Set Evaluation ***
2025-04-26 12:01:58,242 - INFO - TrainMain - Loading test dataset from: ./coding_task/data/atis/test.tsv
2025-04-26 12:01:58,242 - INFO - DataProcessor - DataProcessor initialized with precomputed label mappings.
2025-04-26 12:01:58,242 - INFO - DataProcessor - Loading data from: ./coding_task/data/atis/test.tsv using Pandas
2025-04-26 12:01:58,242 - INFO - Data Utils - File extension: .tsv
2025-04-26 12:01:58,242 - INFO - Data Utils - CustomTextDataset initialized:
2025-04-26 12:01:58,243 - INFO - Data Utils -   file_path: ./coding_task/data/atis/test.tsv
2025-04-26 12:01:58,243 - INFO - Data Utils -   file_type: tsv
2025-04-26 12:01:58,243 - INFO - Data Utils -   column_names: ['atis_text', 'atis_labels']
2025-04-26 12:01:58,243 - INFO - Data Utils -   use_dask: False
2025-04-26 12:01:58,243 - INFO - Data Utils -   unpack_multi_labels: True
2025-04-26 12:01:58,243 - INFO - Data Utils -   label_column_name: atis_labels
2025-04-26 12:01:58,243 - INFO - Data Utils -   label_delimiter: +
2025-04-26 12:01:58,243 - INFO - Data Utils - Loading data using Pandas (chunksize: None)...
2025-04-26 12:01:58,248 - INFO - Data Utils - Loaded initial Pandas df with 850 rows and 2 columns.
2025-04-26 12:01:58,248 - INFO - Data Utils - Unpacking multi-labels in column 'atis_labels' using delimiter '+'.
2025-04-26 12:01:58,254 - INFO - Data Utils - Unpacking complete. Initial rows: 850, Final rows: 865.
2025-04-26 12:01:58,260 - INFO - DataProcessor - Applying text cleanup...
2025-04-26 12:01:58,260 - INFO - Data Utils - Applying cleanup function 'basic_text_cleanup' to columns: ['atis_text']
2025-04-26 12:01:58,262 - INFO - Data Utils - Dropping rows with any NaN values.
2025-04-26 12:01:58,263 - INFO - Data Utils - Resetting index after cleanup. New shape: (865, 2)
2025-04-26 12:01:58,264 - INFO - DataProcessor - Loaded and preprocessed Pandas DataFrame shape: (865, 2)
2025-04-26 12:01:58,268 - INFO - DataProcessor - Preparing labels for task type: multilabel
2025-04-26 12:01:58,268 - INFO - DataProcessor - Grouping unpacked labels by text to prepare for multi-hot encoding...
2025-04-26 12:01:58,290 - INFO - DataProcessor - Using precomputed 17 labels for multilabel task.
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:860: UserWarning: unknown class(es) ['day_name'] will be ignored
  warnings.warn('unknown class(es) {0} will be ignored'
2025-04-26 12:01:58,296 - INFO - DataProcessor - Reconstructed DataFrame shape for multilabel: (850, 2)
2025-04-26 12:01:58,302 - INFO - DataProcessor - Using full dataset for training (validation_split_ratio=0).
2025-04-26 12:01:58,302 - INFO - DataProcessor - Converting DataFrame(s) to Hugging Face DatasetDict...
2025-04-26 12:01:58,315 - INFO - DataProcessor - Tokenizing datasets...
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 850/850 [00:00<00:00, 19168.55 examples/s]
2025-04-26 12:01:58,398 - INFO - DataProcessor - Dataset processing complete.
2025-04-26 12:01:58,400 - INFO - TrainMain - *** Starting Test Set Evaluation ***
 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 25/27 [00:00<00:00, 40.54it/s/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
/home/fe/gururaj/LRP_Experiment/env-pointnet/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in divide
  recall = tps / tps[-1]
{'eval_loss': 0.023558707907795906, 'eval_precision_micro': 0.9604449938195303, 'eval_recall_micro': 0.9003476245654692, 'eval_f1_micro': 0.929425837320574, 'eval_precision_macro': 0.44265302796923, 'eval_recall_macro': 0.46389762430974424, 'eval_f1_macro': 0.4154917451147581, 'eval_precision_weighted': 0.9161096026677366, 'eval_recall_weighted': 0.9003476245654692, 'eval_f1_weighted': 0.8973956667267579, 'eval_precision_samples': 0.9094117647058824, 'eval_recall_samples': 0.9035294117647059, 'eval_f1_samples': 0.9054901960784313, 'eval_f1': 0.929425837320574, 'eval_subset_accuracy': 0.8976470588235295, 'eval_hamming_loss': 0.008166089965397924, 'eval_jaccard_micro': 0.8681564245810056, 'eval_jaccard_macro': 0.37893404591162977, 'eval_jaccard_weighted': 0.8766217152385666, 'eval_jaccard_samples': 0.9035294117647059, 'eval_roc_auc_micro': nan, 'eval_roc_auc_macro': nan, 'eval_roc_auc_weighted': nan, 'eval_roc_auc': nan, 'eval_avg_precision_micro': 0.9794100854917647, 'eval_avg_precision_macro': nan, 'eval_avg_precision_weighted': 0.9887790907617204, 'eval_avg_precision': 0.9794100854917647, 'eval_runtime': 0.7709, 'eval_samples_per_second': 1102.585, 'eval_steps_per_second': 35.023, 'epoch': 10.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 36.44it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 36.44it/s]
***** test metrics *****
  test_epoch                       =       10.0
  test_eval_avg_precision          =     0.9794
  test_eval_avg_precision_macro    =        nan
  test_eval_avg_precision_micro    =     0.9794
  test_eval_avg_precision_weighted =     0.9888
  test_eval_f1                     =     0.9294
  test_eval_f1_macro               =     0.4155
  test_eval_f1_micro               =     0.9294
  test_eval_f1_samples             =     0.9055
  test_eval_f1_weighted            =     0.8974
  test_eval_hamming_loss           =     0.0082
  test_eval_jaccard_macro          =     0.3789
  test_eval_jaccard_micro          =     0.8682
  test_eval_jaccard_samples        =     0.9035
  test_eval_jaccard_weighted       =     0.8766
  test_eval_loss                   =     0.0236
  test_eval_precision_macro        =     0.4427
  test_eval_precision_micro        =     0.9604
  test_eval_precision_samples      =     0.9094
  test_eval_precision_weighted     =     0.9161
  test_eval_recall_macro           =     0.4639
  test_eval_recall_micro           =     0.9003
  test_eval_recall_samples         =     0.9035
  test_eval_recall_weighted        =     0.9003
  test_eval_roc_auc                =        nan
  test_eval_roc_auc_macro          =        nan
  test_eval_roc_auc_micro          =        nan
  test_eval_roc_auc_weighted       =        nan
  test_eval_runtime                = 0:00:00.77
  test_eval_samples_per_second     =   1102.585
  test_eval_steps_per_second       =     35.023
  test_eval_subset_accuracy        =     0.8976
2025-04-26 12:01:59,182 - INFO - TrainMain - Test set evaluation metrics: {'test_eval_loss': 0.023558707907795906, 'test_eval_precision_micro': 0.9604449938195303, 'test_eval_recall_micro': 0.9003476245654692, 'test_eval_f1_micro': 0.929425837320574, 'test_eval_precision_macro': 0.44265302796923, 'test_eval_recall_macro': 0.46389762430974424, 'test_eval_f1_macro': 0.4154917451147581, 'test_eval_precision_weighted': 0.9161096026677366, 'test_eval_recall_weighted': 0.9003476245654692, 'test_eval_f1_weighted': 0.8973956667267579, 'test_eval_precision_samples': 0.9094117647058824, 'test_eval_recall_samples': 0.9035294117647059, 'test_eval_f1_samples': 0.9054901960784313, 'test_eval_f1': 0.929425837320574, 'test_eval_subset_accuracy': 0.8976470588235295, 'test_eval_hamming_loss': 0.008166089965397924, 'test_eval_jaccard_micro': 0.8681564245810056, 'test_eval_jaccard_macro': 0.37893404591162977, 'test_eval_jaccard_weighted': 0.8766217152385666, 'test_eval_jaccard_samples': 0.9035294117647059, 'test_eval_roc_auc_micro': nan, 'test_eval_roc_auc_macro': nan, 'test_eval_roc_auc_weighted': nan, 'test_eval_roc_auc': nan, 'test_eval_avg_precision_micro': 0.9794100854917647, 'test_eval_avg_precision_macro': nan, 'test_eval_avg_precision_weighted': 0.9887790907617204, 'test_eval_avg_precision': 0.9794100854917647, 'test_eval_runtime': 0.7709, 'test_eval_samples_per_second': 1102.585, 'test_eval_steps_per_second': 35.023, 'test_epoch': 10.0}
2025-04-26 12:01:59,182 - INFO - TrainMain - *** Evaluation Finished ***
2025-04-26 12:01:59,182 - INFO - TrainMain - Saving final model/adapter to ./results/atis_multilabel_xlmr_lora
2025-04-26 12:01:59,643 - INFO - TrainMain - Tokenizer saved to ./results/atis_multilabel_xlmr_lora
2025-04-26 12:01:59,643 - ERROR - TrainMain - Failed to save training arguments: 'TrainingConfig' object has no attribute 'to_dict'
2025-04-26 12:01:59,643 - INFO - TrainMain - Script finished successfully.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
